WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.660
Well, let's see. The first part has 500 times 10 parameters, which is 5,000.

00:00:06.660 --> 00:00:11.025
The second part has 10 times 1,000, which is 10,000.

00:00:11.025 --> 00:00:14.365
So, together, they have 15,000 parameters.

00:00:14.365 --> 00:00:16.899
This is much better than 500,000.

00:00:16.899 --> 00:00:21.039
This is called Latent Dirichlet Allocation or LDA for short.

00:00:21.039 --> 00:00:25.599
An LDA is an example of matrix factorization. We'll see how.

00:00:25.600 --> 00:00:28.270
The idea is the following: we go from this Bag of

00:00:28.269 --> 00:00:31.570
Words model in the left to the LDA model in the right.

00:00:31.570 --> 00:00:36.185
The Bag of Words model in the left basically says our probability of,

00:00:36.185 --> 00:00:41.710
say, the word tax being generated by the second document is the label of this arrow.

00:00:41.710 --> 00:00:43.924
On the LDA model in the right,

00:00:43.924 --> 00:00:48.989
that probability is calculated by these arrows by multiplying the P of t given z

00:00:48.990 --> 00:00:54.385
on the top by the corresponding P of z given d on the bottom and adding them.

00:00:54.384 --> 00:00:58.844
This formula reminds us a bit of matrix multiplication in the following way.

00:00:58.844 --> 00:01:02.729
We can put all the probabilities in the left model on a big matrix,

00:01:02.729 --> 00:01:05.329
then the idea is to write this big bag of

00:01:05.329 --> 00:01:08.465
words matrix as a product of a tall skinny matrix

00:01:08.465 --> 00:01:14.495
indexed by documents and topics with a wide flat matrix indexed by topics and terms.

00:01:14.495 --> 00:01:20.450
In this case, the entry corresponding to say the second document and the term "tax" in

00:01:20.450 --> 00:01:23.900
the Bag of Words matrix will be equal to the inner product

00:01:23.900 --> 00:01:27.790
of the corresponding row and column in the matrices on the right.

00:01:27.790 --> 00:01:30.750
And as before, if the matrices are big,

00:01:30.750 --> 00:01:32.984
say if we have 500 documents,

00:01:32.984 --> 00:01:35.319
10 topics and 1,000 terms,

00:01:35.319 --> 00:01:38.750
the Bag of Words matrix has 500,000 entries,

00:01:38.750 --> 00:01:43.840
whereas the two matrices in the topic model combined have 15,000 entries.

00:01:43.840 --> 00:01:45.640
But aside from being much simpler,

00:01:45.640 --> 00:01:48.680
the LDA model has a huge advantage that it gives

00:01:48.680 --> 00:01:52.560
us a bunch of topics that we can divide the documents on.

00:01:52.560 --> 00:01:54.530
In here we're calling them science,

00:01:54.530 --> 00:01:56.734
politics, and sports, but in real life,

00:01:56.734 --> 00:02:00.349
the algorithm will just throw some topics and it'll be up to us to look at

00:02:00.349 --> 00:02:05.140
the associated words and decide what is the common topic of all these words.

00:02:05.140 --> 00:02:07.040
We'll keep them as science, politics,

00:02:07.040 --> 00:02:08.474
and sports for clarity,

00:02:08.474 --> 00:02:10.060
but think of them as topic 1,

00:02:10.060 --> 00:02:11.530
topic 2, and topic 3.

