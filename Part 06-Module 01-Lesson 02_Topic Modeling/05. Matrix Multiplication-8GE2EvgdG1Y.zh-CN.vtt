WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:06.660
我们来看看 第一部分有 500*10 个参数 即 5,000 个

00:00:06.660 --> 00:00:11.025
第二部分有 10*1,000 个参数 即 0,000 个

00:00:11.025 --> 00:00:14.365
一共有 15,000 个

00:00:14.365 --> 00:00:16.899
比 500,000 少多了

00:00:16.899 --> 00:00:21.039
称之为潜在狄利克雷分布 简称 LDA

00:00:21.039 --> 00:00:25.599
LDA 是一种矩阵分解 我们将了解如何计算

00:00:25.600 --> 00:00:28.270
原理是从左侧的词袋模型

00:00:28.269 --> 00:00:31.570
变成右侧的 LDA 模型

00:00:31.570 --> 00:00:36.185
左侧的词袋模型表示

00:00:36.185 --> 00:00:41.710
单词 tax 由第二个文档生成的概率是这个箭头标签

00:00:41.710 --> 00:00:43.924
在右侧的 LDA 模型中

00:00:43.924 --> 00:00:48.989
这个概率可以通过这些箭头计算

00:00:48.990 --> 00:00:54.385
用顶部的 P(t|z) 乘以底部对应的 P(z|d) 然后求和

00:00:54.384 --> 00:00:58.844
这个公式可以采用矩阵乘法

00:00:58.844 --> 00:01:02.729
我们可以将左侧模型中的概率写成大的矩阵

00:01:02.729 --> 00:01:05.329
然后将这个大的词袋矩阵写成

00:01:05.329 --> 00:01:08.465
这个用文档和主题作为索引的高瘦矩阵

00:01:08.465 --> 00:01:14.495
与这个用主题和术语作为索引的宽扁模型的积

00:01:14.495 --> 00:01:20.450
在此示例中 词袋矩阵中第二个文档和术语“tax”

00:01:20.450 --> 00:01:23.900
对应的条目等于

00:01:23.900 --> 00:01:27.790
右侧矩阵的相应行和列的内积

00:01:27.790 --> 00:01:30.750
和之前一样 如果这些矩阵很大

00:01:30.750 --> 00:01:32.984
例如有 500 个文档

00:01:32.984 --> 00:01:35.319
10 个主题和 1,000 个术语

00:01:35.319 --> 00:01:38.750
词袋矩阵有 500,000 个条目

00:01:38.750 --> 00:01:43.840
而主题模型中的两个矩阵相结合后有 15,000 个条目

00:01:43.840 --> 00:01:45.640
除了更简化之外

00:01:45.640 --> 00:01:48.680
LDA 模型还有一个巨大的优势

00:01:48.680 --> 00:01:52.560
它具有大量主题 使我们能够根据这些主题拆分文档

00:01:52.560 --> 00:01:54.530
在这里 我们称这些主题为科学 政治和体育

00:01:54.530 --> 00:01:56.734
但在现实生活中

00:01:56.734 --> 00:02:00.349
算法将直接采用一些主题

00:02:00.349 --> 00:02:05.140
并且由我们来查看相关单词 并决定所有这些单词的共同主题是什么

00:02:05.140 --> 00:02:07.040
澄清下 这些主题

00:02:07.040 --> 00:02:08.474
为科学 政治和体育

00:02:08.474 --> 00:02:10.060
但是看做主题 1

00:02:10.060 --> 00:02:11.530
主题 2 和主题 3

