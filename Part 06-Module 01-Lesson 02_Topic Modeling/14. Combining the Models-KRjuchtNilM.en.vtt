WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.359
So, now let's put this all together and study how to get

00:00:03.359 --> 00:00:08.484
these two matrices in the LDA model based on their respective dirichlet distributions.

00:00:08.484 --> 00:00:10.779
The rough idea is as we just saw,

00:00:10.779 --> 00:00:14.914
the entries from the first matrix come from picking points in the distribution alpha.

00:00:14.914 --> 00:00:19.174
The interest from the second matrix come from from picking points in distribution beta.

00:00:19.175 --> 00:00:21.810
The idea is to find the best locations of

00:00:21.809 --> 00:00:25.765
these points to get the best factorization of the matrix.

00:00:25.765 --> 00:00:30.245
The best locations of these points will give us precisely the topics we want.

00:00:30.245 --> 00:00:32.134
Let's generate some documents,

00:00:32.134 --> 00:00:34.644
in order to compare them with the originals.

00:00:34.645 --> 00:00:38.745
We started with dirichlet distribution for topics alpha.

00:00:38.744 --> 00:00:43.224
From here, we draw some points corresponding to all the documents.

00:00:43.225 --> 00:00:45.520
Let's start by drawing one of them.

00:00:45.520 --> 00:00:49.180
This point will give some values for each of the three topics,

00:00:49.179 --> 00:00:52.299
which will generate a multivariate distribution theta.

00:00:52.299 --> 00:00:53.949
This is the mixture of topics,

00:00:53.950 --> 00:00:55.910
corresponding to document one.

00:00:55.909 --> 00:00:59.924
Now, let's generate some words for document one as follows.

00:00:59.924 --> 00:01:04.039
From theta, we draw some topics. How many topics?

00:01:04.040 --> 00:01:05.950
This is a bit out of the scope,

00:01:05.950 --> 00:01:09.385
but the idea is that we'll have a Poisson variable to tell us how many.

00:01:09.385 --> 00:01:12.125
Just think of yet another parameter in this model.

00:01:12.125 --> 00:01:14.754
So, we draw some topics based on the probability is

00:01:14.754 --> 00:01:17.295
given by this distribution, which means,

00:01:17.295 --> 00:01:20.030
we'll draw science with a 0.7 probability,

00:01:20.030 --> 00:01:23.829
politics with 0.2, and sports with 0.1.

00:01:23.829 --> 00:01:26.700
Now, we'll associate words to these topics.

00:01:26.700 --> 00:01:30.615
How? Using the words dirichlet distribution beta.

00:01:30.614 --> 00:01:34.109
In this distribution, we locate the topic somewhere,

00:01:34.109 --> 00:01:36.099
and from each of these dots,

00:01:36.099 --> 00:01:38.229
we obtain a distribution of the words,

00:01:38.230 --> 00:01:40.365
are generated by each of the topics.

00:01:40.364 --> 00:01:42.944
For example, topic one science,

00:01:42.944 --> 00:01:46.479
generates the word space with 0.4 probability.

00:01:46.480 --> 00:01:50.454
Climate with 0.4, vote with 0.1,

00:01:50.454 --> 00:01:53.064
and rule with 0.1 probability.

00:01:53.064 --> 00:01:55.509
These distributions are called phi.

00:01:55.510 --> 00:01:57.954
Now, for each of the topics we've chosen,

00:01:57.954 --> 00:02:03.280
we'll pick a word associated to it using the multivariate distribution phi.

00:02:03.280 --> 00:02:06.765
For example, for the first topic we have science.

00:02:06.765 --> 00:02:10.604
We look at the science row in the phi distribution and pick a word out there.

00:02:10.604 --> 00:02:12.164
For example, space.

00:02:12.164 --> 00:02:15.590
So, space is the first word in document one.

00:02:15.590 --> 00:02:19.099
We do this for every one of these topics and

00:02:19.099 --> 00:02:22.164
then generate words from our first generated document.

00:02:22.164 --> 00:02:25.204
Let's call it "fake document one".

00:02:25.205 --> 00:02:26.940
We do this process again,

00:02:26.939 --> 00:02:29.289
drawing another point from the alpha distribution,

00:02:29.289 --> 00:02:31.349
getting another multivariable distribution theta,

00:02:31.349 --> 00:02:33.639
which generates new topics which from beta

00:02:33.639 --> 00:02:37.250
generate new words and that's "fake document two".

00:02:37.250 --> 00:02:40.629
We go on and on, generating many documents.

00:02:40.629 --> 00:02:43.194
Now, it's time to compare them with originals.

00:02:43.194 --> 00:02:47.019
What we'll do is we want to use maximum likelihood to figure out the arrangements

00:02:47.020 --> 00:02:51.230
of points which will give us the real articles with the highest probability.

00:02:51.229 --> 00:02:53.219
In summary, here's what we're doing.

00:02:53.219 --> 00:02:56.914
We have the two dirichlet distributions, alpha and beta.

00:02:56.914 --> 00:02:58.769
From the distribution alpha,

00:02:58.770 --> 00:03:01.814
we pick some documents and from distribution beta,

00:03:01.814 --> 00:03:03.465
we pick some topics.

00:03:03.465 --> 00:03:07.474
We use these two combined to create some fake articles.

00:03:07.474 --> 00:03:10.525
Then we compare them to the real articles.

00:03:10.525 --> 00:03:13.110
The probability of obtaining the real articles,

00:03:13.110 --> 00:03:15.090
of course is really small,

00:03:15.090 --> 00:03:17.219
but there must be some arrangement of points in

00:03:17.219 --> 00:03:21.104
the above distributions that maximizes this probability.

00:03:21.104 --> 00:03:26.334
Our goal is to find this arrangement of points and that will give us the topics.

00:03:26.335 --> 00:03:30.090
In the same way that we train many algorithms in machine learning,

00:03:30.090 --> 00:03:32.270
there will be an error that will tell us how

00:03:32.270 --> 00:03:35.310
far we are from generating the real articles.

00:03:35.310 --> 00:03:39.784
The error will back propagate all the way to the distributions,

00:03:39.784 --> 00:03:42.379
giving us a gradient that will tell us where to

00:03:42.379 --> 00:03:45.460
move the points in order to reduce this error.

00:03:45.460 --> 00:03:50.435
So, we move the points as indicated and now we've obtained a slightly better model.

00:03:50.435 --> 00:03:54.914
Doing this repeatedly, will give us a good enough arrangement on the points.

00:03:54.914 --> 00:03:59.525
Naturally, a good arrangement of the points will give us some topics.

00:03:59.525 --> 00:04:01.960
The dirichlet distribution alpha,

00:04:01.960 --> 00:04:03.805
will tell us what articles are associated to

00:04:03.805 --> 00:04:06.425
these topics and the dirichlet distribution beta,

00:04:06.425 --> 00:04:09.635
will tell us what words are associated to these topics.

00:04:09.634 --> 00:04:12.634
We can go a bit further and actually backpropagate the error,

00:04:12.634 --> 00:04:14.269
all the way to alpha and beta,

00:04:14.270 --> 00:04:18.345
obtaining not only better point arrangements but actually better distributions,

00:04:18.345 --> 00:04:19.925
alpha and beta prime.

00:04:19.925 --> 00:04:23.259
That's it. That's how latent dirichlet analysis works.

00:04:23.259 --> 00:04:27.009
If you want to understand the diagram and the paper, here it is.

00:04:27.009 --> 00:04:29.894
All we need to know, what the alpha is the topics distribution.

00:04:29.894 --> 00:04:33.245
Beta is the words distribution, theta and phi,

00:04:33.245 --> 00:04:36.454
are the multivariate distributions drawn from them,

00:04:36.454 --> 00:04:39.389
Z are our topics and W,

00:04:39.389 --> 00:04:43.000
are the documents obtained by combining these two.

