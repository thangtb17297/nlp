WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.359
So, the idea for building our LDA model will be to factor are

00:00:03.359 --> 00:00:06.869
Bag of Words matrix on the left into two matrices,

00:00:06.870 --> 00:00:11.375
one indexing documents by topic and the other indexing topics by word.

00:00:11.375 --> 00:00:15.140
In this video, I'll be more specific about what these matrices mean.

00:00:15.140 --> 00:00:17.710
Here's how we calculate our Bag of Words matrix.

00:00:17.710 --> 00:00:19.195
Let's say we have a document,

00:00:19.195 --> 00:00:22.940
Document two which has the words space three times,

00:00:22.940 --> 00:00:26.645
climate and rule once each and no other words.

00:00:26.644 --> 00:00:28.879
Remember that we process this document and

00:00:28.879 --> 00:00:31.599
extracted the important words so things like is,

00:00:31.600 --> 00:00:33.829
the, and are not counted.

00:00:33.829 --> 00:00:36.219
We write these numbers in the corresponding row.

00:00:36.219 --> 00:00:38.344
Now to find the probabilities,

00:00:38.344 --> 00:00:42.115
we just divide by the row sum and we get three-fifths,

00:00:42.115 --> 00:00:45.465
one-fifth, one-fifth, and zeros for the rest.

00:00:45.465 --> 00:00:47.530
That's our Bag of Words matrix.

00:00:47.530 --> 00:00:50.820
Our document topic matrix is as follows;

00:00:50.820 --> 00:00:53.715
Let's say we have a document say Document three,

00:00:53.715 --> 00:00:56.280
and let's say we have a way to figure out that Document three is

00:00:56.280 --> 00:00:59.295
mostly about science and a bit about sports and politics.

00:00:59.295 --> 00:01:01.780
Let's say it's 70 percent about science,

00:01:01.780 --> 00:01:03.879
10 percent about politics,

00:01:03.878 --> 00:01:06.144
and 20 percent about sports.

00:01:06.144 --> 00:01:08.159
So, we just record these numbers in

00:01:08.159 --> 00:01:11.090
the corresponding row and that's how we obtain this matrix.

00:01:11.090 --> 00:01:13.770
The topic term matrix is similar.

00:01:13.769 --> 00:01:16.409
Here we have a topic, say politics,

00:01:16.409 --> 00:01:21.254
and let's say we can figure out the probabilities that words are generated by this topic.

00:01:21.254 --> 00:01:23.759
We take all this probabilities we should add to

00:01:23.760 --> 00:01:26.825
one and put them in the corresponding row.

00:01:26.825 --> 00:01:31.439
As we saw, the product of these two matrices is the Bag of Word matrix.

00:01:31.439 --> 00:01:35.064
Well, this is not exact but the idea is to get really close.

00:01:35.064 --> 00:01:39.864
If we can find two matrices whose product is very close to the Bag of Word matrix,

00:01:39.864 --> 00:01:41.809
then we've created a topic model.

00:01:41.810 --> 00:01:46.135
But I still haven't told you how to calculate the entries in these two matrices.

00:01:46.135 --> 00:01:50.670
Well, one way is using the traditional matrix factorization algorithm.

00:01:50.670 --> 00:01:54.159
This is out of the context of this course but in the instructor comments,

00:01:54.159 --> 00:01:57.924
we'll add some resources and keys you want to learn it more in detail.

00:01:57.924 --> 00:02:00.715
However, these matrices are very special.

00:02:00.715 --> 00:02:03.230
For ones, the rows add up to one.

00:02:03.230 --> 00:02:04.710
Also if you think about it,

00:02:04.709 --> 00:02:08.715
there's a lot of structure coming from a set of documents, topics and words.

00:02:08.715 --> 00:02:13.310
So, what we'll do is something a bit more elaborate than matrix multiplication.

00:02:13.310 --> 00:02:16.009
The basic idea is the following: the entries in

00:02:16.009 --> 00:02:20.715
the two topic modeling matrices come from some special distributions.

00:02:20.715 --> 00:02:26.689
So, we'll embrace this fact and work with these distributions to find these two matrices.

00:02:26.689 --> 00:02:29.129
We'll see this in the next few videos.

