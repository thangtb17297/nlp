WEBVTT
Kind: captions
Language: zh-CN

00:00:00.020 --> 00:00:06.343
我们进一步观察之前介绍的语音用户界面 (VUI) 的基本流程 复习一下

00:00:06.344 --> 00:00:08.399
其中包括三个通用部分

00:00:08.398 --> 00:00:12.133
语音到文本 文本输入到合乎逻辑的文本输出

00:00:12.134 --> 00:00:14.775
最后是文本到语音

00:00:14.775 --> 00:00:17.010
首先是语音到文本

00:00:17.010 --> 00:00:18.914
这是语音识别

00:00:18.914 --> 00:00:22.394
过去语音识别对机器来说很难

00:00:22.393 --> 00:00:26.564
但对人来说很容易 这是人工智能的重大目标

00:00:26.565 --> 00:00:29.214
当人们对着麦克风讲话

00:00:29.213 --> 00:00:32.548
声振动会转化成音频信号

00:00:32.548 --> 00:00:34.950
以某个频率对信号进行采样

00:00:34.950 --> 00:00:38.969
这些样本会转化为成分频率的向量

00:00:38.969 --> 00:00:41.133
这里显示的是频谱图

00:00:41.133 --> 00:00:45.140
这些向量代表了数据集中的声音特征

00:00:45.140 --> 00:00:49.950
所以这一步可以看作特征提取

00:00:49.950 --> 00:00:53.189
语音识别的下一步是解码

00:00:53.189 --> 00:00:56.969
或把一系列向量识别成词语或语句

00:00:56.969 --> 00:00:58.353
为了做到这一点

00:00:58.353 --> 00:01:04.530
我们需要擅长处理声音模式时序数据的概率模型

00:01:04.530 --> 00:01:06.525
这是声学模型

00:01:06.525 --> 00:01:09.709
利用声学模型解码向量

00:01:09.709 --> 00:01:13.730
可以让我们更好地猜测词语是什么

00:01:13.730 --> 00:01:15.390
不过这可能还不够

00:01:15.390 --> 00:01:19.079
一些词语的序列与其他非常相似

00:01:19.079 --> 00:01:24.884
例如 根据短语 “hello world (你好世界)” 的发音

00:01:24.885 --> 00:01:28.935
声学模型可能不确定这个词语是

00:01:28.935 --> 00:01:33.969
“hello world (你好世界)” 还是 “how a word (词语多么)” 或者其他内容

00:01:33.968 --> 00:01:38.634
现在我们知道最优可能是第一个选择 “hello world (你好世界)”

00:01:38.635 --> 00:01:40.334
但是我们为什么知道呢？

00:01:40.334 --> 00:01:44.393
因为我们头脑中有个语言模型

00:01:44.393 --> 00:01:50.569
这个模型受过多年经验的训练 也是我们需要添加到解码器中的东西

00:01:50.569 --> 00:01:54.829
同样也需要口音模型

00:01:54.828 --> 00:01:59.529
如果利用大量代表性示例训练这些模型

00:01:59.530 --> 00:02:03.731
我们产生正确文本的概率越来越高

00:02:03.731 --> 00:02:06.899
这是要训练的许多模型

00:02:06.899 --> 00:02:10.900
一个强大的系统需要声学模型 语言模型和口音模型

00:02:10.900 --> 00:02:16.384
我们还没有详细了解整个 VUI 流程

00:02:16.383 --> 00:02:19.948
我们要在随后课程中了解语音识别模型更多内容

00:02:19.949 --> 00:02:22.944
这里是预览

00:02:22.943 --> 00:02:28.663
记得我说过 我们需要擅长处理时序数据的概率模型

00:02:28.663 --> 00:02:32.864
回想你已经在这个课程中学习的两个模型

00:02:32.865 --> 00:02:40.009
之前我们创建了隐马尔可夫模型 解码一系列手势

00:02:40.008 --> 00:02:42.049
在我们深度学习课程中

00:02:42.050 --> 00:02:47.740
我们使用循环神经网络训练时序数据

00:02:47.740 --> 00:02:51.659
这两个模型都可以成功运用在语音识别中

00:02:51.658 --> 00:02:56.954
我们会在详细学习语音识别时探讨更多

00:02:56.955 --> 00:02:59.185
对于流程

00:02:59.185 --> 00:03:02.115
一旦我们以文本形式得到语音内容

00:03:02.115 --> 00:03:07.935
现在要考虑语音应用程序的思考部分 推理逻辑

00:03:07.935 --> 00:03:09.509
如果我问你

00:03:09.508 --> 00:03:12.598
人类 可能会问天气怎么样？

00:03:12.598 --> 00:03:15.134
你可能回答各种各种 “我不知道”

00:03:15.134 --> 00:03:19.905
“外面很冷”  “温度计显示 90 度” 等等

00:03:19.905 --> 00:03:21.658
为了进行回答

00:03:21.658 --> 00:03:24.900
你首先要理解我的提问

00:03:24.900 --> 00:03:29.460
然后处理请求 形成响应

00:03:29.460 --> 00:03:33.090
这很容易 因为你是人类

00:03:33.090 --> 00:03:39.324
不过计算机理解我们的需求和我们说话的含义是很困难的

00:03:39.324 --> 00:03:44.879
自然语言处理 (NLP) 领域应运而生

00:03:44.878 --> 00:03:47.128
为了充分实现自然语言处理

00:03:47.128 --> 00:03:49.438
必须处理大量语言数据集

00:03:49.438 --> 00:03:53.840
因此这是需要克服的巨大挑战

00:03:53.840 --> 00:03:55.875
不过我们来观察一个较小的问题

00:03:55.875 --> 00:04:00.884
例如从 VUI 设备中得到天气预报

00:04:00.883 --> 00:04:03.448
想象一个应用程序中包含天气信息

00:04:03.449 --> 00:04:07.245
可以响应某个文本请求

00:04:07.245 --> 00:04:10.072
我们并不是解析所有词语

00:04:10.072 --> 00:04:13.408
而是采取捷径 只映射

00:04:13.408 --> 00:04:18.719
最常用的天气请求短语 得到天气过程

00:04:18.720 --> 00:04:24.814
在这个例子中 应用程序实际上大部分时间可以理解请求

00:04:24.814 --> 00:04:30.100
如果没有提前映射某个请求作为潜在选项 这就不会有效果

00:04:30.100 --> 00:04:36.300
这对有限的应用非常有限 可以随着时间不断改进

00:04:36.300 --> 00:04:38.305
一旦我们得到文本响应

00:04:38.305 --> 00:04:44.435
VUI 传送途径的剩余任务是把文本转化为语音

00:04:44.435 --> 00:04:47.860
这是语音合成 或叫做文本到语音 (TTS)

00:04:47.860 --> 00:04:53.800
这里仍然是如何说出词语 用于训练模型

00:04:53.800 --> 00:04:58.355
提供口语词汇最常见的发音成分

00:04:58.355 --> 00:05:02.769
当我们从单调的机器人声音

00:05:02.769 --> 00:05:06.115
到包含音调变化和热情的人类丰富声音

00:05:06.115 --> 00:05:11.199
任务的难度各有不同

00:05:11.199 --> 00:05:14.019
目前为止人们使用深度学习技巧

00:05:14.019 --> 00:05:18.468
生成了一些现实中发音机器的声音

