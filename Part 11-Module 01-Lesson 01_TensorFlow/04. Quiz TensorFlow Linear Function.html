<!-- udacimak v1.3.0 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Quiz: TensorFlow Linear Function</title>
  <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../assets/css/plyr.css">
  <link rel="stylesheet" href="../assets/css/katex.min.css">
  <link rel="stylesheet" href="../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <link rel="shortcut icon" type="image/png" href="../assets/img/udacimak.png" />
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>TensorFlow</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="01. Intro.html">01. Intro</a>
    </li>
    <li class="">
      <a href="02. Installing TensorFlow.html">02. Installing TensorFlow</a>
    </li>
    <li class="">
      <a href="03. Hello, Tensor World!.html">03. Hello, Tensor World!</a>
    </li>
    <li class="">
      <a href="04. Quiz TensorFlow Linear Function.html">04. Quiz: TensorFlow Linear Function</a>
    </li>
    <li class="">
      <a href="05. Quiz TensorFlow Softmax.html">05. Quiz: TensorFlow Softmax</a>
    </li>
    <li class="">
      <a href="06. Quiz TensorFlow Cross Entropy.html">06. Quiz: TensorFlow Cross Entropy</a>
    </li>
    <li class="">
      <a href="07. Quiz Mini-batch.html">07. Quiz: Mini-batch</a>
    </li>
    <li class="">
      <a href="08. Epochs.html">08. Epochs</a>
    </li>
    <li class="">
      <a href="09. Pre-Lab NotMNIST in TensorFlow.html">09. Pre-Lab: NotMNIST in TensorFlow</a>
    </li>
    <li class="">
      <a href="10. Lab NotMNIST in TensorFlow.html">10. Lab: NotMNIST in TensorFlow</a>
    </li>
    <li class="">
      <a href="11. Two-layer Neural Network.html">11. Two-layer Neural Network</a>
    </li>
    <li class="">
      <a href="12. Quiz TensorFlow ReLUs.html">12. Quiz: TensorFlow ReLUs</a>
    </li>
    <li class="">
      <a href="13. Deep Neural Network in TensorFlow.html">13. Deep Neural Network in TensorFlow</a>
    </li>
    <li class="">
      <a href="14. Save and Restore TensorFlow Models.html">14. Save and Restore TensorFlow Models</a>
    </li>
    <li class="">
      <a href="15. Finetuning.html">15. Finetuning</a>
    </li>
    <li class="">
      <a href="16. Quiz TensorFlow Dropout.html">16. Quiz: TensorFlow Dropout</a>
    </li>
    <li class="">
      <a href="17. Outro.html">17. Outro</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">04. Quiz: TensorFlow Linear Function</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <div>
  <h1 id="linear-functions-in-tensorflow">Linear functions in TensorFlow</h1>
<p>The most common operation in neural networks is calculating the linear combination of inputs, weights, and biases. As a reminder, we can write the output of the linear operation as</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/linear-equation.gif" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>Here, <span class="mathquill ud-math">\mathbf{W}</span> is a matrix of the weights connecting two layers. The output <span class="mathquill ud-math">\mathbf{y}</span>, the input <span class="mathquill ud-math">\mathbf{x}</span>, and the biases <span class="mathquill ud-math">\mathbf{b}</span>  are all vectors.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="weights-and-bias-in-tensorflow">Weights and Bias in TensorFlow</h2>
<p>The goal of training a neural network is to modify weights and biases to best predict the labels.  In order to use weights and bias, you'll need a Tensor that can be modified.  This leaves out <a href="https://www.tensorflow.org/api_docs/python/tf/placeholder" target="_blank"><code>tf.placeholder()</code></a> and <a href="https://www.tensorflow.org/api_docs/python/tf/constant" target="_blank"><code>tf.constant()</code></a>, since those Tensors can't be modified.  This is where <a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank"><code>tf.Variable</code></a> class comes in.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h3 id="tfvariable">tf.Variable()</h3>
<pre><code class="python language-python">x = tf.Variable(5)</code></pre>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank"><code>tf.Variable</code></a> class creates a tensor with an initial value that can be modified, much like a normal Python variable.  This tensor stores its state in the session, so you must initialize the state of the tensor manually.  You'll use the <a href="https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer" target="_blank"><code>tf.global_variables_initializer()</code></a> function to initialize the state of all the Variable tensors.</p>
<h5 id="initialization">Initialization</h5>
<pre><code>init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)</code></pre>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer" target="_blank"><code>tf.global_variables_initializer()</code></a> call returns an operation that will initialize all TensorFlow variables from the graph.  You call the operation using a session to initialize all the variables as shown above.  Using the <a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank"><code>tf.Variable</code></a> class allows us to change the weights and bias, but an initial value needs to be chosen.</p>
<p>Initializing the weights with random numbers from a normal distribution is good practice.  Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You'll learn more about this in the next lesson, when you study gradient descent.</p>
<p>Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights.  You'll use the <a href="https://www.tensorflow.org/api_docs/python/tf/truncated_normal" target="_blank"><code>tf.truncated_normal()</code></a> function to generate random numbers from a normal distribution.</p>
<h3 id="tftruncated_normal">tf.truncated_normal()</h3>
<pre><code class="python language-python">n_features = 120
n_labels = 5
weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))</code></pre>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/truncated_normal" target="_blank"><code>tf.truncated_normal()</code></a> function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.  </p>
<p>Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias.  Let's use the simplest solution, setting the bias to 0.</p>
<h3 id="tfzeros">tf.zeros()</h3>
<pre><code class="python language-python">n_labels = 5
bias = tf.Variable(tf.zeros(n_labels))</code></pre>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/zeros" target="_blank"><code>tf.zeros()</code></a> function returns a tensor with all zeros.</p>
<h2 id="linear-classifier-quiz">Linear Classifier Quiz</h2>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/mnist-012.png" alt="A subset of the MNIST dataset" class="img img-fluid">
    <figcaption class="figure-caption">
      <p>A subset of the MNIST dataset</p>
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>You'll be classifying the handwritten numbers <code>0</code>, <code>1</code>, and <code>2</code> from the MNIST dataset using TensorFlow.  The above is a small sample of the data you'll be training on.  Notice how some of the <code>1</code>s are written with a <a href="https://en.wikipedia.org/wiki/Serif" target="_blank">serif</a> at the top and at different angles.  The similarities and differences will play a part in shaping the weights of the model.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/weights-0-1-2.png" alt="Left: Weights for labeling 0. Middle: Weights for labeling 1. Right: Weights for labeling 2." class="img img-fluid">
    <figcaption class="figure-caption">
      <p>Left: Weights for labeling 0. Middle: Weights for labeling 1. Right: Weights for labeling 2.</p>
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>The images above are trained weights for each label (<code>0</code>, <code>1</code>, and <code>2</code>).  The weights display the unique properties of each digit they have found.  Complete this quiz to train your own weights using the MNIST dataset.</p>
<h3 id="instructions">Instructions</h3>
<ol>
<li>Open quiz.py.<ol>
<li>Implement <code>get_weights</code> to return a <a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank"><code>tf.Variable</code></a> of weights</li>
<li>Implement <code>get_biases</code> to return a <a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank"><code>tf.Variable</code></a> of biases</li>
<li>Implement <code>xW + b</code> in the <code>linear</code> function</li></ol></li>
<li>Open sandbox.py<ol>
<li>Initialize all weights</li></ol></li>
</ol>
<p>Since <code>xW</code> in <code>xW + b</code> is matrix multiplication, you have to use the <a href="https://www.tensorflow.org/api_docs/python/tf/matmul" target="_blank"><code>tf.matmul()</code></a> function instead of <a href="https://www.tensorflow.org/api_docs/python/tf/multiply" target="_blank"><code>tf.multiply()</code></a>.  Don't forget that order matters in matrix multiplication, so <code>tf.matmul(a,b)</code> is not the same as <code>tf.matmul(b,a)</code>.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>

  <h4>Start Quiz:</h4>
  <div>
  <div class="nav nav-tabs nav-fill" role="tablist" id="question-tabs">
    <a href="#266693-sandbox-py" class="nav-item nav-link  active show" id="tab-266693-sandbox-py" data-toggle="tab" role="tab"
      aria-controls="266693-sandbox-py" aria-selected="true">sandbox.py</a>
    <a href="#266693-quiz-py" class="nav-item nav-link " id="tab-266693-quiz-py" data-toggle="tab" role="tab"
      aria-controls="266693-quiz-py" aria-selected="false">quiz.py</a>
    <a href="#266693-quiz_solution-py" class="nav-item nav-link " id="tab-266693-quiz_solution-py" data-toggle="tab" role="tab"
      aria-controls="266693-quiz_solution-py" aria-selected="false">quiz_solution.py</a>
    <a href="#266693-sandbox_solution-py" class="nav-item nav-link " id="tab-266693-sandbox_solution-py" data-toggle="tab" role="tab"
      aria-controls="266693-sandbox_solution-py" aria-selected="false">sandbox_solution.py</a>
  </div>

  <div class="tab-content" style="padding: 20px 0;" id="question-tab-contents">
    <div class="tab-pane  active show" id="266693-sandbox-py" aria-labelledby="tab-266693-sandbox-py" role="tabpanel">
      <pre><code></code># Solution is available in the other &quot;sandbox_solution.py&quot; tab
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
from quiz import get_weights, get_biases, linear


def mnist_features_labels(n_labels):
    &quot;&quot;&quot;
    Gets the first &lt;n&gt; labels from the MNIST dataset
    :param n_labels: Number of labels to use
    :return: Tuple of feature list and label list
    &quot;&quot;&quot;
    mnist_features &#x3D; []
    mnist_labels &#x3D; []

    mnist &#x3D; input_data.read_data_sets(&#x27;/datasets/ud730/mnist&#x27;, one_hot&#x3D;True)

    # In order to make quizzes run faster, we&#x27;re only looking at 10000 images
    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):

        # Add features and labels if it&#x27;s for the first &lt;n&gt;th labels
        if mnist_label[:n_labels].any():
            mnist_features.append(mnist_feature)
            mnist_labels.append(mnist_label[:n_labels])

    return mnist_features, mnist_labels


# Number of features (28*28 image is 784 features)
n_features &#x3D; 784
# Number of labels
n_labels &#x3D; 3

# Features and Labels
features &#x3D; tf.placeholder(tf.float32)
labels &#x3D; tf.placeholder(tf.float32)

# Weights and Biases
w &#x3D; get_weights(n_features, n_labels)
b &#x3D; get_biases(n_labels)

# Linear Function xW + b
logits &#x3D; linear(features, w, b)

# Training data
train_features, train_labels &#x3D; mnist_features_labels(n_labels)

with tf.Session() as session:
    # TODO: Initialize session variables
    
    # Softmax
    prediction &#x3D; tf.nn.softmax(logits)

    # Cross entropy
    # This quantifies how far off the predictions were.
    # You&#x27;ll learn more about this in future lessons.
    cross_entropy &#x3D; -tf.reduce_sum(labels * tf.log(prediction), reduction_indices&#x3D;1)

    # Training loss
    # You&#x27;ll learn more about this in future lessons.
    loss &#x3D; tf.reduce_mean(cross_entropy)

    # Rate at which the weights are changed
    # You&#x27;ll learn more about this in future lessons.
    learning_rate &#x3D; 0.08

    # Gradient Descent
    # This is the method used to train the model
    # You&#x27;ll learn more about this in future lessons.
    optimizer &#x3D; tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)

    # Run optimizer and get loss
    _, l &#x3D; session.run(
        [optimizer, loss],
        feed_dict&#x3D;{features: train_features, labels: train_labels})

# Print loss
print(&#x27;Loss: {}&#x27;.format(l))
</code></pre>
    </div>
    <div class="tab-pane " id="266693-quiz-py" aria-labelledby="tab-266693-quiz-py" role="tabpanel">
      <pre><code></code># Solution is available in the other &quot;quiz_solution.py&quot; tab
import tensorflow as tf

def get_weights(n_features, n_labels):
    &quot;&quot;&quot;
    Return TensorFlow weights
    :param n_features: Number of features
    :param n_labels: Number of labels
    :return: TensorFlow weights
    &quot;&quot;&quot;
    # TODO: Return weights
    pass


def get_biases(n_labels):
    &quot;&quot;&quot;
    Return TensorFlow bias
    :param n_labels: Number of labels
    :return: TensorFlow bias
    &quot;&quot;&quot;
    # TODO: Return biases
    pass


def linear(input, w, b):
    &quot;&quot;&quot;
    Return linear function in TensorFlow
    :param input: TensorFlow input
    :param w: TensorFlow weights
    :param b: TensorFlow biases
    :return: TensorFlow linear function
    &quot;&quot;&quot;
    # TODO: Linear Function (xW + b)
    pass</code></pre>
    </div>
    <div class="tab-pane " id="266693-quiz_solution-py" aria-labelledby="tab-266693-quiz_solution-py" role="tabpanel">
      <pre><code></code># Quiz Solution
# Note: You can&#x27;t run code in this tab
import tensorflow as tf

def get_weights(n_features, n_labels):
    &quot;&quot;&quot;
    Return TensorFlow weights
    :param n_features: Number of features
    :param n_labels: Number of labels
    :return: TensorFlow weights
    &quot;&quot;&quot;
    # TODO: Return weights
    return tf.Variable(tf.truncated_normal((n_features, n_labels)))


def get_biases(n_labels):
    &quot;&quot;&quot;
    Return TensorFlow bias
    :param n_labels: Number of labels
    :return: TensorFlow bias
    &quot;&quot;&quot;
    # TODO: Return biases
    return tf.Variable(tf.zeros(n_labels))


def linear(input, w, b):
    &quot;&quot;&quot;
    Return linear function in TensorFlow
    :param input: TensorFlow input
    :param w: TensorFlow weights
    :param b: TensorFlow biases
    :return: TensorFlow linear function
    &quot;&quot;&quot;
    # TODO: Linear Function (xW + b)
    return tf.add(tf.matmul(input, w), b)</code></pre>
    </div>
    <div class="tab-pane " id="266693-sandbox_solution-py" aria-labelledby="tab-266693-sandbox_solution-py" role="tabpanel">
      <pre><code></code>import tensorflow as tf
# Sandbox Solution
# Note: You can&#x27;t run code in this tab
from tensorflow.examples.tutorials.mnist import input_data
from quiz import get_weights, get_biases, linear


def mnist_features_labels(n_labels):
    &quot;&quot;&quot;
    Gets the first &lt;n&gt; labels from the MNIST dataset
    :param n_labels: Number of labels to use
    :return: Tuple of feature list and label list
    &quot;&quot;&quot;
    mnist_features &#x3D; []
    mnist_labels &#x3D; []

    mnist &#x3D; input_data.read_data_sets(&#x27;/datasets/ud730/mnist&#x27;, one_hot&#x3D;True)

    # In order to make quizzes run faster, we&#x27;re only looking at 10000 images
    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):

        # Add features and labels if it&#x27;s for the first &lt;n&gt;th labels
        if mnist_label[:n_labels].any():
            mnist_features.append(mnist_feature)
            mnist_labels.append(mnist_label[:n_labels])

    return mnist_features, mnist_labels


# Number of features (28*28 image is 784 features)
n_features &#x3D; 784
# Number of labels
n_labels &#x3D; 3

# Features and Labels
features &#x3D; tf.placeholder(tf.float32)
labels &#x3D; tf.placeholder(tf.float32)

# Weights and Biases
w &#x3D; get_weights(n_features, n_labels)
b &#x3D; get_biases(n_labels)

# Linear Function xW + b
logits &#x3D; linear(features, w, b)

# Training data
train_features, train_labels &#x3D; mnist_features_labels(n_labels)

with tf.Session() as session:
    session.run(tf.global_variables_initializer())

    # Softmax
    prediction &#x3D; tf.nn.softmax(logits)

    # Cross entropy
    # This quantifies how far off the predictions were.
    # You&#x27;ll learn more about this in future lessons.
    cross_entropy &#x3D; -tf.reduce_sum(labels * tf.log(prediction), reduction_indices&#x3D;1)

    # Training loss
    # You&#x27;ll learn more about this in future lessons.
    loss &#x3D; tf.reduce_mean(cross_entropy)

    # Rate at which the weights are changed
    # You&#x27;ll learn more about this in future lessons.
    learning_rate &#x3D; 0.08

    # Gradient Descent
    # This is the method used to train the model
    # You&#x27;ll learn more about this in future lessons.
    optimizer &#x3D; tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)

    # Run optimizer and get loss
    _, l &#x3D; session.run(
        [optimizer, loss],
        feed_dict&#x3D;{features: train_features, labels: train_labels})

# Print loss
print(&#x27;Loss: {}&#x27;.format(l))
</code></pre>
    </div>
  </div>
</div>



</div>


</div>
<div class="divider"></div>
          </div>

          <div class="col-12">
            <p class="text-right">
              <a href="05. Quiz TensorFlow Softmax.html" class="btn btn-outline-primary mt-4" role="button">Next Concept</a>
            </p>
          </div>
        </div>
      </main>

      <footer class="footer">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <p class="text-center">
                <a href="https://github.com/udacimak/udacimak#readme" target="_blank">udacimak v1.3.0</a>
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </div>


  <script src="../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../assets/js/bootstrap.min.js"></script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('04. Quiz: TensorFlow Linear Function')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
</body>

</html>
