<!-- udacimak v1.3.0 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Quiz: Mini-batch</title>
  <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../assets/css/plyr.css">
  <link rel="stylesheet" href="../assets/css/katex.min.css">
  <link rel="stylesheet" href="../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <link rel="shortcut icon" type="image/png" href="../assets/img/udacimak.png" />
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>TensorFlow</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="01. Intro.html">01. Intro</a>
    </li>
    <li class="">
      <a href="02. Installing TensorFlow.html">02. Installing TensorFlow</a>
    </li>
    <li class="">
      <a href="03. Hello, Tensor World!.html">03. Hello, Tensor World!</a>
    </li>
    <li class="">
      <a href="04. Quiz TensorFlow Linear Function.html">04. Quiz: TensorFlow Linear Function</a>
    </li>
    <li class="">
      <a href="05. Quiz TensorFlow Softmax.html">05. Quiz: TensorFlow Softmax</a>
    </li>
    <li class="">
      <a href="06. Quiz TensorFlow Cross Entropy.html">06. Quiz: TensorFlow Cross Entropy</a>
    </li>
    <li class="">
      <a href="07. Quiz Mini-batch.html">07. Quiz: Mini-batch</a>
    </li>
    <li class="">
      <a href="08. Epochs.html">08. Epochs</a>
    </li>
    <li class="">
      <a href="09. Pre-Lab NotMNIST in TensorFlow.html">09. Pre-Lab: NotMNIST in TensorFlow</a>
    </li>
    <li class="">
      <a href="10. Lab NotMNIST in TensorFlow.html">10. Lab: NotMNIST in TensorFlow</a>
    </li>
    <li class="">
      <a href="11. Two-layer Neural Network.html">11. Two-layer Neural Network</a>
    </li>
    <li class="">
      <a href="12. Quiz TensorFlow ReLUs.html">12. Quiz: TensorFlow ReLUs</a>
    </li>
    <li class="">
      <a href="13. Deep Neural Network in TensorFlow.html">13. Deep Neural Network in TensorFlow</a>
    </li>
    <li class="">
      <a href="14. Save and Restore TensorFlow Models.html">14. Save and Restore TensorFlow Models</a>
    </li>
    <li class="">
      <a href="15. Finetuning.html">15. Finetuning</a>
    </li>
    <li class="">
      <a href="16. Quiz TensorFlow Dropout.html">16. Quiz: TensorFlow Dropout</a>
    </li>
    <li class="">
      <a href="17. Outro.html">17. Outro</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">07. Quiz: Mini-batch</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="mini-batching">Mini-batching</h2>
<p>In this section, you'll go over what mini-batching is and how to apply it in TensorFlow. </p>
<p>Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time.  This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.</p>
<p>Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples.  However, this is a small price to pay in order to be able to run the model at all.</p>
<p>It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch.</p>
<p>Let's look at the MNIST dataset with weights and a bias to see if your machine can handle it.</p>
<pre><code class="python language-python">from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf

n_input = 784  # MNIST data input (img shape: 28*28)
n_classes = 10  # MNIST total classes (0-9 digits)

# Import MNIST data
mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)

# The features are already scaled and the data is shuffled
train_features = mnist.train.images
test_features = mnist.test.images

train_labels = mnist.train.labels.astype(np.float32)
test_labels = mnist.test.labels.astype(np.float32)

# Weights &amp; bias
weights = tf.Variable(tf.random_normal([n_input, n_classes]))
bias = tf.Variable(tf.random_normal([n_classes]))</code></pre>
<h3 id="question-1">Question 1</h3>
<p>Calculate the memory size of <code>train_features</code>, <code>train_labels</code>, <code>weights</code>, and <code>bias</code> in bytes.  Ignore memory for overhead, just calculate the memory required for the stored data.</p>
<p>You may have to look up how much memory a float32 requires, using <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format" target="_blank">this link</a>.</p>
<p><em>train_features  Shape: (55000, 784) Type: float32</em></p>
<p><em>train_labels Shape: (55000, 10) Type: float32</em></p>
<p><em>weights Shape: (784, 10) Type: float32</em></p>
<p><em>bias Shape: (10,) Type: float32</em></p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <div>
    <p><strong>QUESTION:</strong> <p>How many bytes of memory does <code>train_features</code> need?</p></p>
    <div class="" form-group">
      <label for="answer"><strong>ANSWER:</strong></label>
      <textarea class="form-control" id="answer"></textarea>
    </div>
  </div>

  <details>
    <summary><strong>SOLUTION:</strong></summary>
    <p><i>NOTE: The solutions are expressed in RegEx pattern. Udacity uses these patterns to check the given answer</i></p>

  </details>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <div>
    <p><strong>QUESTION:</strong> <p>How many bytes of memory does <code>train_labels</code> need?</p></p>
    <div class="" form-group">
      <label for="answer"><strong>ANSWER:</strong></label>
      <textarea class="form-control" id="answer"></textarea>
    </div>
  </div>

  <details>
    <summary><strong>SOLUTION:</strong></summary>
    <p><i>NOTE: The solutions are expressed in RegEx pattern. Udacity uses these patterns to check the given answer</i></p>

  </details>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <div>
    <p><strong>QUESTION:</strong> <p>How many bytes of memory does <code>weights</code> need?</p></p>
    <div class="" form-group">
      <label for="answer"><strong>ANSWER:</strong></label>
      <textarea class="form-control" id="answer"></textarea>
    </div>
  </div>

  <details>
    <summary><strong>SOLUTION:</strong></summary>
    <p><i>NOTE: The solutions are expressed in RegEx pattern. Udacity uses these patterns to check the given answer</i></p>

  </details>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <div>
    <p><strong>QUESTION:</strong> <p>How many bytes of memory does <code>bias</code> need?</p></p>
    <div class="" form-group">
      <label for="answer"><strong>ANSWER:</strong></label>
      <textarea class="form-control" id="answer"></textarea>
    </div>
  </div>

  <details>
    <summary><strong>SOLUTION:</strong></summary>
    <p><i>NOTE: The solutions are expressed in RegEx pattern. Udacity uses these patterns to check the given answer</i></p>

  </details>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>The total memory space required for the inputs, weights and bias is around 174 megabytes, which isn't that much memory.  You could train this whole dataset on most CPUs and GPUs.  </p>
<p>But larger datasets that you'll use in the future measured in gigabytes or more. It's possible to purchase more memory, but it's expensive. A Titan X GPU with 12 GB of memory costs over $1,000. </p>
<p>Instead, in order to run large models on your machine, you'll learn how to use mini-batching.</p>
<p>Let's look at how you implement mini-batching in TensorFlow.</p>
<h2 id="tensorflow-mini-batching">TensorFlow Mini-batching</h2>
<p>In order to use mini-batching, you must first divide your data into batches. </p>
<p>Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size.  For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)</p>
<p>In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's <a href="https://www.tensorflow.org/api_docs/python/tf/placeholder" target="_blank"><code>tf.placeholder()</code></a> function to receive the varying batch sizes.</p>
<p>Continuing the example, if each sample had <code>n_input = 784</code> features and <code>n_classes = 10</code> possible labels, the dimensions for <code>features</code> would be <code>[None, n_input]</code> and <code>labels</code> would be <code>[None, n_classes]</code>.</p>
<pre><code class="python language-python"># Features and Labels
features = tf.placeholder(tf.float32, [None, n_input])
labels = tf.placeholder(tf.float32, [None, n_classes])</code></pre>
<p>What does <code>None</code> do here?</p>
<p>The <code>None</code> dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.</p>
<p>Going back to our earlier example, this setup allows you to feed <code>features</code> and <code>labels</code> into the model as either the batches of 128 samples or the single batch of 104 samples.</p>
<h3 id="question-2">Question 2</h3>
<p>Use the parameters below, how many batches are there, and what is the last batch size?</p>
<p><em>features is (50000, 400)</em></p>
<p><em>labels is (50000, 10)</em></p>
<p><em>batch_size is 128</em></p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <div>
    <p><strong>QUESTION:</strong> <p>How many batches are there?</p></p>
    <div class="" form-group">
      <label for="answer"><strong>ANSWER:</strong></label>
      <textarea class="form-control" id="answer"></textarea>
    </div>
  </div>

  <details>
    <summary><strong>SOLUTION:</strong></summary>
    <p><i>NOTE: The solutions are expressed in RegEx pattern. Udacity uses these patterns to check the given answer</i></p>

  </details>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <div>
    <p><strong>QUESTION:</strong> <p>What is the last batch size?</p></p>
    <div class="" form-group">
      <label for="answer"><strong>ANSWER:</strong></label>
      <textarea class="form-control" id="answer"></textarea>
    </div>
  </div>

  <details>
    <summary><strong>SOLUTION:</strong></summary>
    <p><i>NOTE: The solutions are expressed in RegEx pattern. Udacity uses these patterns to check the given answer</i></p>

  </details>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>Now that you know the basics, let's learn how to implement mini-batching.</p>
<h3 id="question-3">Question 3</h3>
<p>Implement the <code>batches</code> function to batch <code>features</code> and <code>labels</code>.  The function should return each batch with a maximum size of <code>batch_size</code>.  To help you with the quiz, look at the following example output of a working <code>batches</code> function.</p>
<pre><code class="python language-python"># 4 Samples of features
example_features = [
    ['F11','F12','F13','F14'],
    ['F21','F22','F23','F24'],
    ['F31','F32','F33','F34'],
    ['F41','F42','F43','F44']]
# 4 Samples of labels
example_labels = [
    ['L11','L12'],
    ['L21','L22'],
    ['L31','L32'],
    ['L41','L42']]

example_batches = batches(3, example_features, example_labels)</code></pre>
<p>The <code>example_batches</code> variable would be the following:</p>
<pre><code class="python language-python">[
    # 2 batches:
    #   First is a batch of size 3.
    #   Second is a batch of size 1
    [
        # First Batch is size 3
        [
            # 3 samples of features.
            # There are 4 features per sample.
            ['F11', 'F12', 'F13', 'F14'],
            ['F21', 'F22', 'F23', 'F24'],
            ['F31', 'F32', 'F33', 'F34']
        ], [
            # 3 samples of labels.
            # There are 2 labels per sample.
            ['L11', 'L12'],
            ['L21', 'L22'],
            ['L31', 'L32']
        ]
    ], [
        # Second Batch is size 1.
        # Since batch size is 3, there is only one sample left from the 4 samples.
        [
            # 1 sample of features.
            ['F41', 'F42', 'F43', 'F44']
        ], [
            # 1 sample of labels.
            ['L41', 'L42']
        ]
    ]
]</code></pre>
<p>Implement the <code>batches</code> function in the "quiz.py" file below.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>

  <h4>Start Quiz:</h4>
  <div>
  <div class="nav nav-tabs nav-fill" role="tablist" id="question-tabs">
    <a href="#220372-sandbox-py" class="nav-item nav-link  active show" id="tab-220372-sandbox-py" data-toggle="tab" role="tab"
      aria-controls="220372-sandbox-py" aria-selected="true">sandbox.py</a>
    <a href="#220372-quiz-py" class="nav-item nav-link " id="tab-220372-quiz-py" data-toggle="tab" role="tab"
      aria-controls="220372-quiz-py" aria-selected="false">quiz.py</a>
    <a href="#220372-quiz_solution-py" class="nav-item nav-link " id="tab-220372-quiz_solution-py" data-toggle="tab" role="tab"
      aria-controls="220372-quiz_solution-py" aria-selected="false">quiz_solution.py</a>
  </div>

  <div class="tab-content" style="padding: 20px 0;" id="question-tab-contents">
    <div class="tab-pane  active show" id="220372-sandbox-py" aria-labelledby="tab-220372-sandbox-py" role="tabpanel">
      <pre><code></code>from quiz import batches
from pprint import pprint

# 4 Samples of features
example_features &#x3D; [
    [&#x27;F11&#x27;,&#x27;F12&#x27;,&#x27;F13&#x27;,&#x27;F14&#x27;],
    [&#x27;F21&#x27;,&#x27;F22&#x27;,&#x27;F23&#x27;,&#x27;F24&#x27;],
    [&#x27;F31&#x27;,&#x27;F32&#x27;,&#x27;F33&#x27;,&#x27;F34&#x27;],
    [&#x27;F41&#x27;,&#x27;F42&#x27;,&#x27;F43&#x27;,&#x27;F44&#x27;]]
# 4 Samples of labels
example_labels &#x3D; [
    [&#x27;L11&#x27;,&#x27;L12&#x27;],
    [&#x27;L21&#x27;,&#x27;L22&#x27;],
    [&#x27;L31&#x27;,&#x27;L32&#x27;],
    [&#x27;L41&#x27;,&#x27;L42&#x27;]]

# PPrint prints data structures like 2d arrays, so they are easier to read
pprint(batches(3, example_features, example_labels))
</code></pre>
    </div>
    <div class="tab-pane " id="220372-quiz-py" aria-labelledby="tab-220372-quiz-py" role="tabpanel">
      <pre><code></code>import math
def batches(batch_size, features, labels):
    &quot;&quot;&quot;
    Create batches of features and labels
    :param batch_size: The batch size
    :param features: List of features
    :param labels: List of labels
    :return: Batches of (Features, Labels)
    &quot;&quot;&quot;
    assert len(features) &#x3D;&#x3D; len(labels)
    # TODO: Implement batching
    pass
</code></pre>
    </div>
    <div class="tab-pane " id="220372-quiz_solution-py" aria-labelledby="tab-220372-quiz_solution-py" role="tabpanel">
      <pre><code></code>import math
def batches(batch_size, features, labels):
    &quot;&quot;&quot;
    Create batches of features and labels
    :param batch_size: The batch size
    :param features: List of features
    :param labels: List of labels
    :return: Batches of (Features, Labels)
    &quot;&quot;&quot;
    assert len(features) &#x3D;&#x3D; len(labels)
    # TODO: Implement batching
    output_batches &#x3D; []
    
    sample_size &#x3D; len(features)
    for start_i in range(0, sample_size, batch_size):
        end_i &#x3D; start_i + batch_size
        batch &#x3D; [features[start_i:end_i], labels[start_i:end_i]]
        output_batches.append(batch)
        
    return output_batches
</code></pre>
    </div>
  </div>
</div>



</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>Let's use mini-batching to feed batches of MNIST features and labels into a linear model.</p>
<p>Set the batch size and run the optimizer over all the batches with the <code>batches</code> function.  The recommended batch size is 128.  If you have memory restrictions, feel free to make it smaller.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>

  <h4>Start Quiz:</h4>
  <div>
  <div class="nav nav-tabs nav-fill" role="tablist" id="question-tabs">
    <a href="#220450-quiz-py" class="nav-item nav-link  active show" id="tab-220450-quiz-py" data-toggle="tab" role="tab"
      aria-controls="220450-quiz-py" aria-selected="true">quiz.py</a>
    <a href="#220450-helper-py" class="nav-item nav-link " id="tab-220450-helper-py" data-toggle="tab" role="tab"
      aria-controls="220450-helper-py" aria-selected="false">helper.py</a>
    <a href="#220450-quiz_solution-py" class="nav-item nav-link " id="tab-220450-quiz_solution-py" data-toggle="tab" role="tab"
      aria-controls="220450-quiz_solution-py" aria-selected="false">quiz_solution.py</a>
  </div>

  <div class="tab-content" style="padding: 20px 0;" id="question-tab-contents">
    <div class="tab-pane  active show" id="220450-quiz-py" aria-labelledby="tab-220450-quiz-py" role="tabpanel">
      <pre><code></code>from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf
import numpy as np
from helper import batches

learning_rate &#x3D; 0.001
n_input &#x3D; 784  # MNIST data input (img shape: 28*28)
n_classes &#x3D; 10  # MNIST total classes (0-9 digits)

# Import MNIST data
mnist &#x3D; input_data.read_data_sets(&#x27;/datasets/ud730/mnist&#x27;, one_hot&#x3D;True)

# The features are already scaled and the data is shuffled
train_features &#x3D; mnist.train.images
test_features &#x3D; mnist.test.images

train_labels &#x3D; mnist.train.labels.astype(np.float32)
test_labels &#x3D; mnist.test.labels.astype(np.float32)

# Features and Labels
features &#x3D; tf.placeholder(tf.float32, [None, n_input])
labels &#x3D; tf.placeholder(tf.float32, [None, n_classes])

# Weights &amp; bias
weights &#x3D; tf.Variable(tf.random_normal([n_input, n_classes]))
bias &#x3D; tf.Variable(tf.random_normal([n_classes]))

# Logits - xW + b
logits &#x3D; tf.add(tf.matmul(features, weights), bias)

# Define loss and optimizer
cost &#x3D; tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits&#x3D;logits, labels&#x3D;labels))
optimizer &#x3D; tf.train.GradientDescentOptimizer(learning_rate&#x3D;learning_rate).minimize(cost)

# Calculate accuracy
correct_prediction &#x3D; tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))
accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, tf.float32))


# TODO: Set batch size
batch_size &#x3D; None
assert batch_size is not None, &#x27;You must set the batch size&#x27;

init &#x3D; tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    
    # TODO: Train optimizer on all batches
    # for batch_features, batch_labels in ______
    sess.run(optimizer, feed_dict&#x3D;{features: batch_features, labels: batch_labels})

    # Calculate accuracy for test dataset
    test_accuracy &#x3D; sess.run(
        accuracy,
        feed_dict&#x3D;{features: test_features, labels: test_labels})

print(&#x27;Test Accuracy: {}&#x27;.format(test_accuracy))
</code></pre>
    </div>
    <div class="tab-pane " id="220450-helper-py" aria-labelledby="tab-220450-helper-py" role="tabpanel">
      <pre><code></code>import math
def batches(batch_size, features, labels):
    &quot;&quot;&quot;
    Create batches of features and labels
    :param batch_size: The batch size
    :param features: List of features
    :param labels: List of labels
    :return: Batches of (Features, Labels)
    &quot;&quot;&quot;
    assert len(features) &#x3D;&#x3D; len(labels)
    outout_batches &#x3D; []
    
    sample_size &#x3D; len(features)
    for start_i in range(0, sample_size, batch_size):
        end_i &#x3D; start_i + batch_size
        batch &#x3D; [features[start_i:end_i], labels[start_i:end_i]]
        outout_batches.append(batch)
        
    return outout_batches
</code></pre>
    </div>
    <div class="tab-pane " id="220450-quiz_solution-py" aria-labelledby="tab-220450-quiz_solution-py" role="tabpanel">
      <pre><code></code>from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf
import numpy as np
from helper import batches

learning_rate &#x3D; 0.001
n_input &#x3D; 784  # MNIST data input (img shape: 28*28)
n_classes &#x3D; 10  # MNIST total classes (0-9 digits)

# Import MNIST data
mnist &#x3D; input_data.read_data_sets(&#x27;/datasets/ud730/mnist&#x27;, one_hot&#x3D;True)

# The features are already scaled and the data is shuffled
train_features &#x3D; mnist.train.images
test_features &#x3D; mnist.test.images

train_labels &#x3D; mnist.train.labels.astype(np.float32)
test_labels &#x3D; mnist.test.labels.astype(np.float32)

# Features and Labels
features &#x3D; tf.placeholder(tf.float32, [None, n_input])
labels &#x3D; tf.placeholder(tf.float32, [None, n_classes])

# Weights &amp; bias
weights &#x3D; tf.Variable(tf.random_normal([n_input, n_classes]))
bias &#x3D; tf.Variable(tf.random_normal([n_classes]))

# Logits - xW + b
logits &#x3D; tf.add(tf.matmul(features, weights), bias)

# Define loss and optimizer
cost &#x3D; tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits&#x3D;logits, labels&#x3D;labels))
optimizer &#x3D; tf.train.GradientDescentOptimizer(learning_rate&#x3D;learning_rate).minimize(cost)

# Calculate accuracy
correct_prediction &#x3D; tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))
accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, tf.float32))


# TODO: Set batch size
batch_size &#x3D; 128
assert batch_size is not None, &#x27;You must set the batch size&#x27;

init &#x3D; tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    
    # TODO: Train optimizer on all batches
    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):
        sess.run(optimizer, feed_dict&#x3D;{features: batch_features, labels: batch_labels})

    # Calculate accuracy for test dataset
    test_accuracy &#x3D; sess.run(
        accuracy,
        feed_dict&#x3D;{features: test_features, labels: test_labels})

print(&#x27;Test Accuracy: {}&#x27;.format(test_accuracy))
</code></pre>
    </div>
  </div>
</div>



</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>The accuracy is low, but you probably know that you could train on the dataset more than once.  You can train a model using the dataset multiple times.  You'll go over this subject in the next section where we talk about "epochs".</p>
</div>

</div>
<div class="divider"></div>
          </div>

          <div class="col-12">
            <p class="text-right">
              <a href="08. Epochs.html" class="btn btn-outline-primary mt-4" role="button">Next Concept</a>
            </p>
          </div>
        </div>
      </main>

      <footer class="footer">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <p class="text-center">
                <a href="https://github.com/udacimak/udacimak#readme" target="_blank">udacimak v1.3.0</a>
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </div>


  <script src="../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../assets/js/bootstrap.min.js"></script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('07. Quiz: Mini-batch')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
</body>

</html>
