WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.819
Now that we have
an unrolled example of input

00:00:02.819 --> 00:00:06.150
and output of the network,
let's go another level deeper,

00:00:06.150 --> 00:00:09.220
and look at some of the
parameters of the model.

00:00:09.220 --> 00:00:10.950
So at this point
of the course you

00:00:10.949 --> 00:00:14.009
know that you can't just feed
words directly to the network.

00:00:14.009 --> 00:00:17.190
We need to turn the
words into vectors first.

00:00:17.190 --> 00:00:20.010
This is the same concept
we saw in word2vec

00:00:20.010 --> 00:00:23.010
and the sentiment
analysis RNN lessons.

00:00:23.010 --> 00:00:25.980
Our first set of
parameters are the ways

00:00:25.980 --> 00:00:27.839
that we multiply
the input vectors

00:00:27.839 --> 00:00:30.570
by every time we
take an element.

00:00:30.570 --> 00:00:32.140
Now here's the thing.

00:00:32.140 --> 00:00:34.240
So you don't have to
worry about this as much.

00:00:34.240 --> 00:00:37.320
So Tensorflow actually shields
you from a lot of this,

00:00:37.320 --> 00:00:40.926
and then takes care of a lot of
these different parts for you.

00:00:40.926 --> 00:00:42.759
So don't worry about
implementation for now.

00:00:42.759 --> 00:00:44.879
Just make sure
you're comfortable

00:00:44.880 --> 00:00:46.080
with the general concept.

00:00:46.079 --> 00:00:47.870
And then we'll get to
implementation later.

00:00:47.871 --> 00:00:52.800
But this is one of the things
that Tensorflow handles for us.

00:00:52.799 --> 00:00:55.599
But it's good nonetheless
to understand.

00:00:55.600 --> 00:01:03.539
So we have U, which is the
input matrix of weights

00:01:03.539 --> 00:01:05.280
that we multiply
the inputs with.

00:01:05.280 --> 00:01:08.189
And then we have A, which
we multiply the hidden state

00:01:08.189 --> 00:01:11.980
as it travels between
time steps of the encoder,

00:01:11.980 --> 00:01:15.390
and also out to the decoder.

00:01:15.390 --> 00:01:19.379
We also have another
matrix of weights called V

00:01:19.379 --> 00:01:24.229
that we multiply the hidden
state when the decoder outputs

00:01:24.230 --> 00:01:27.060
its values, and
another one called

00:01:27.060 --> 00:01:31.560
B that goes that's multiplied
by the hidden state when

00:01:31.560 --> 00:01:36.120
it travels between
decoder time stamps.

00:01:36.120 --> 00:01:39.060
The last major component
of such a model

00:01:39.060 --> 00:01:41.549
is a fully connected
layer at the output

00:01:41.549 --> 00:01:43.530
to translate the
state of the network

00:01:43.530 --> 00:01:45.930
into an output we can choose.

00:01:45.930 --> 00:01:48.240
So these would be
the output logits.

00:01:48.239 --> 00:01:50.280
The number of these
logits is the same

00:01:50.280 --> 00:01:53.480
as the size of our vocabulary.

00:01:53.480 --> 00:01:56.730
The logit with the highest
value corresponds to the word

00:01:56.730 --> 00:01:58.680
that the model wants to output.

00:01:58.680 --> 00:01:59.955
So we'll take an example.

00:01:59.954 --> 00:02:01.079
We'll clarify a little bit.

00:02:01.079 --> 00:02:05.349
So let's look at this
simple vocabulary.

00:02:05.349 --> 00:02:08.549
So if we only have that one
example of input and output,

00:02:08.550 --> 00:02:15.090
how are you, I am good, we will
have a vocabulary of size 11.

00:02:15.090 --> 00:02:16.500
Never mind the first four.

00:02:16.500 --> 00:02:17.879
So these are set.

00:02:17.879 --> 00:02:20.400
These will be useful later.

00:02:20.400 --> 00:02:22.689
So we reserve the
first four spots

00:02:22.689 --> 00:02:28.020
in the vocabulary for PAD, for
EOS, which is in the sentence.

00:02:28.020 --> 00:02:31.240
UNK is unknown, and
then we also have GO.

00:02:31.240 --> 00:02:34.110
So we've seen GO
in some of these.

00:02:34.110 --> 00:02:39.270
But we'll get to that later
in the implementation stage.

00:02:39.270 --> 00:02:41.800
And then we have the
list of our words

00:02:41.800 --> 00:02:46.530
that we actually use
in our training data.

00:02:46.530 --> 00:02:47.800
So this is our vocabulary.

00:02:47.800 --> 00:02:50.280
It has 11 elements.

00:02:50.280 --> 00:02:53.969
And so at each time
step of the decoder,

00:02:53.969 --> 00:02:56.370
the output of this
network will tell us

00:02:56.370 --> 00:03:00.090
which element is most
likely to be generated,

00:03:00.090 --> 00:03:01.650
based on the training data.

00:03:01.650 --> 00:03:05.159
So this is a small example,
so we have a small vocabulary.

00:03:05.159 --> 00:03:07.109
In real data sets,
you'll outcome

00:03:07.110 --> 00:03:15.090
across vocabularies of tens of
thousands usually of examples.

00:03:15.090 --> 00:03:19.379
So you'll have tens of thousands
of logits in that last layer

00:03:19.379 --> 00:03:20.370
of the network.

00:03:20.370 --> 00:03:25.140
So luckily, this is
probably the last time

00:03:25.139 --> 00:03:27.719
you'll have to look at
this view of the network.

00:03:27.719 --> 00:03:30.469
Tensorflow defines
an API for sequence

00:03:30.469 --> 00:03:33.270
to sequence that hides away
a lot of these details.

00:03:33.270 --> 00:03:36.240
And you just have to use
its API to define how

00:03:36.240 --> 00:03:38.463
you want the network to work.

