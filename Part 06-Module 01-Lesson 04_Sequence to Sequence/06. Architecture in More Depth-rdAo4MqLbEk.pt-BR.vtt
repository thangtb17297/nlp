WEBVTT
Kind: captions
Language: pt-BR

00:00:00.200 --> 00:00:04.067
Agora que temos um exemplo
dos inputs e outputs da rede,

00:00:04.100 --> 00:00:06.167
vamos nos aprofundar
mais um nível

00:00:06.200 --> 00:00:08.934
e dar uma olhada
nos parâmetros do modelo.

00:00:08.968 --> 00:00:10.701
A esta altura do curso,

00:00:10.734 --> 00:00:13.868
você sabe que não podemos
só alimentar palavras à rede,

00:00:13.901 --> 00:00:17.033
precisamos transformar as palavras
em vetores primeiro.

00:00:17.067 --> 00:00:19.901
É o mesmo conceito que vimos
no word2vec,

00:00:19.934 --> 00:00:22.968
na lição sobre a rede neural
de análise de sentimentos.

00:00:23.000 --> 00:00:24.868
Nosso primeiro conjunto
de parâmetros

00:00:24.901 --> 00:00:28.167
serão os pesos pelos quais
multiplicamos os vetores do input

00:00:28.200 --> 00:00:30.501
todas as vezes
que pegamos um elemento.

00:00:30.534 --> 00:00:32.300
Esta é a questão,

00:00:32.334 --> 00:00:34.367
não precisa se preocupar
muito com isso

00:00:34.400 --> 00:00:37.467
porque o TensorFlow
nos protege disso,

00:00:37.501 --> 00:00:40.868
escondendo e tomando conta
de várias etapas,

00:00:40.901 --> 00:00:43.701
então não se preocupe
com implementação por enquanto.

00:00:43.734 --> 00:00:47.567
Apenas entenda o conceito geral
e falaremos de implementação depois.

00:00:47.601 --> 00:00:51.834
Essa é uma coisa que o TensorFlow
resolve para nós,

00:00:52.601 --> 00:00:55.467
mas é bom entender
de qualquer forma.

00:00:55.501 --> 00:00:59.834
Temos "U",
que é o peso do input,

00:01:01.667 --> 00:01:05.033
é a matriz de pesos com a qual
multiplicamos o input,

00:01:05.067 --> 00:01:07.934
e temos "A", com o qual
multiplicamos o estado oculto,

00:01:07.968 --> 00:01:11.734
já que se movimenta entre time steps
do codificador

00:01:11.767 --> 00:01:14.701
e também para fora,
para o decodificador.

00:01:15.234 --> 00:01:19.434
Também temos outra matriz de pesos
chamada "V",

00:01:19.467 --> 00:01:21.701
pela qual multiplicamos
o estado oculto

00:01:21.734 --> 00:01:24.834
quando o decodificador libera
seus valores de output,

00:01:25.534 --> 00:01:27.968
e outra chamada "B",

00:01:28.000 --> 00:01:31.167
que é multiplicada
pelo estado oculto

00:01:31.200 --> 00:01:34.434
quando ele se movimenta
pelos time steps do decodificador.

00:01:36.000 --> 00:01:38.934
O último componente importante
deste modelo

00:01:38.968 --> 00:01:41.501
é a camada totalmente conectada
no output,

00:01:41.534 --> 00:01:45.667
para traduzir o estado da rede
num output que podemos escolher.

00:01:45.701 --> 00:01:48.100
São os logits de output.

00:01:48.133 --> 00:01:52.534
O número de logits é o mesmo
que o tamanho do vocabulário.

00:01:53.434 --> 00:01:54.868
O logit de maior valor

00:01:54.901 --> 00:01:58.501
corresponde à palavra
que o modelo liberará como output.

00:01:58.534 --> 00:02:01.734
Mostrarei um exemplo,
para esclarecer um pouco.

00:02:01.767 --> 00:02:05.167
Vejamos este vocabulário
simples.

00:02:05.200 --> 00:02:08.400
Se só tivermos este exemplo
de input e output,

00:02:08.434 --> 00:02:10.968
"como vai você?",
"eu vou bem",

00:02:11.601 --> 00:02:14.767
teremos um vocabulário
de tamanho 11.

00:02:15.200 --> 00:02:16.701
Ignore os quatro primeiros,

00:02:16.734 --> 00:02:20.067
são configurações
que serão úteis mais tarde.

00:02:20.100 --> 00:02:23.501
Reservamos os quatro primeiros
lugares do vocabulário

00:02:23.534 --> 00:02:27.868
para &lt;PAD&gt;, &lt;EOS&gt;,
que é o final da frase,

00:02:27.901 --> 00:02:31.067
&lt;UNK&gt; é "desconhecido"
e &lt;GO&gt;.

00:02:31.100 --> 00:02:34.300
Vimos &lt;GO&gt; algumas vezes,

00:02:34.334 --> 00:02:38.501
mas falaremos disso depois,
na fase da implementação.

00:02:38.534 --> 00:02:42.534
E, então, temos as palavras
que de fato usamos

00:02:42.567 --> 00:02:46.000
nos dados de treino.

00:02:46.434 --> 00:02:49.434
Este é o nosso vocabulário,
tem 11 elementos.

00:02:50.234 --> 00:02:53.834
A cada time step
do decodificador,

00:02:53.868 --> 00:02:57.033
o output desta rede
nos dirá qual elemento

00:02:57.067 --> 00:03:01.634
é o mais provável de ser gerado
com base nos dados do treino.

00:03:01.667 --> 00:03:05.167
Este é um pequeno exemplo,
então temos um vocabulário pequeno.

00:03:05.200 --> 00:03:09.567
Em bancos de dados de verdade,
você encontrará vocabulários

00:03:09.601 --> 00:03:14.934
com dezenas de milhares
de exemplos,

00:03:14.968 --> 00:03:17.434
então você terá
dezenas de milhares

00:03:17.467 --> 00:03:20.100
de flutuantes
na última camada da rede.

00:03:20.133 --> 00:03:21.801
Então,

00:03:21.834 --> 00:03:24.601
por sorte, esta provavelmente
será a última vez

00:03:24.634 --> 00:03:27.467
que você terá que olhar
para esta vista da rede.

00:03:27.501 --> 00:03:30.534
O TensorFlow define uma API
para sequence-to-sequence

00:03:30.567 --> 00:03:32.767
que esconde muitos
destes detalhes,

00:03:33.133 --> 00:03:35.200
então você
só precisa usar a API

00:03:35.234 --> 00:03:38.033
para definir como quer
que a rede funcione.

