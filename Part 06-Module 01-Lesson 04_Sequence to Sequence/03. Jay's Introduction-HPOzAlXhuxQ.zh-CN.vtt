WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.919
大家好 我叫 J

00:00:02.919 --> 00:00:06.379
我是优达学城的一名内容开发工程师

00:00:06.379 --> 00:00:10.129
今天我将讨论一个非常强大的 RNN 技巧

00:00:10.130 --> 00:00:11.930
叫做序列到序列

00:00:11.929 --> 00:00:14.509
在上一节课中

00:00:14.509 --> 00:00:16.609
Andrew Trask 讲解了如何使用

00:00:16.609 --> 00:00:20.300
常规前馈神经网络进行情感分析

00:00:20.300 --> 00:00:22.400
神经网络能够了解

00:00:22.399 --> 00:00:25.250
每个单词的语气是积极的还是消极的

00:00:25.250 --> 00:00:28.490
并能够判断整个句子对讲述的主体

00:00:28.489 --> 00:00:31.189
持正面态度还是负面态度

00:00:31.190 --> 00:00:33.109
但是当我们想要制作更复杂的模型

00:00:33.109 --> 00:00:36.950
以便处理语言和序列数据时

00:00:36.950 --> 00:00:40.120
我们遇到了问题

00:00:40.119 --> 00:00:41.500
我们看个示例

00:00:41.500 --> 00:00:44.329
深度学习图书的作者

00:00:44.329 --> 00:00:46.969
给出了下面的示例

00:00:46.969 --> 00:00:49.100
假设有下面两句话

00:00:49.100 --> 00:00:55.250
“I went to Nepal in 2009”和“In 2009, I went to Nepal”

00:00:55.250 --> 00:00:57.920
如果我们对一个模型进行训练

00:00:57.920 --> 00:01:02.030
要求该模型读取这些输入内容并提取出此人去尼泊尔的年份

00:01:02.030 --> 00:01:04.099
我们肯定希望将 2009

00:01:04.099 --> 00:01:07.430
看做我们要获取的信息 对吧

00:01:07.430 --> 00:01:11.180
如果我们训练一个普通前馈神经网络去处理这个任务

00:01:11.180 --> 00:01:14.720
它将对每个输入特征

00:01:14.719 --> 00:01:16.969
分配单独的参数

00:01:16.969 --> 00:01:21.109
理论上来说 它需要单独学习

00:01:21.109 --> 00:01:25.890
输入句子中每个位置的语言规则

00:01:25.890 --> 00:01:28.439
在之前的另一节课中

00:01:28.439 --> 00:01:32.789
Matt 向我们展示了第一个循环神经网络 (RNN)

00:01:32.790 --> 00:01:36.440
循环神经网络是一类强大的神经网络

00:01:36.439 --> 00:01:38.750
负责处理序列数据

00:01:38.750 --> 00:01:42.439
它们非常适合处理语言和翻译任务

00:01:42.439 --> 00:01:46.269
因为它们可以扩展至任何长度的序列

00:01:46.269 --> 00:01:49.789
更重要的是 它们可以共享

00:01:49.790 --> 00:01:51.470
不同时间步长的参数

00:01:51.469 --> 00:01:54.469
所以当它们学会一个语言模型后

00:01:54.469 --> 00:01:56.319
它们的学习速度比

00:01:56.319 --> 00:02:00.109
传统前馈神经网络要快很多

00:02:00.109 --> 00:02:03.230
当我们提到序列数据时

00:02:03.230 --> 00:02:06.230
可以指代模型的输入数据

00:02:06.230 --> 00:02:09.379
或输出数据

00:02:09.379 --> 00:02:13.340
我猜你之前可能见过这个图表

00:02:13.340 --> 00:02:17.659
该图表摘自 Andrej Karpathy 的

00:02:17.659 --> 00:02:19.509
精彩 RNN 论文

00:02:19.509 --> 00:02:21.409
图中展示了不同类型的 RNN

00:02:21.409 --> 00:02:24.500
它们适合不同类型的任务

00:02:24.500 --> 00:02:28.969
这个是 Matt 展示的情感分析 RNN

00:02:28.969 --> 00:02:31.490
它读取单词序列

00:02:31.490 --> 00:02:33.719
然后输出一个值

00:02:33.719 --> 00:02:36.830
也就是多对一网络

00:02:36.830 --> 00:02:40.700
但是如果你想构建聊天机器人或翻译服务

00:02:40.699 --> 00:02:42.949
则需要具有序列输入

00:02:42.949 --> 00:02:44.750
和序列输出

00:02:44.750 --> 00:02:47.870
也就是该图表右侧的

00:02:47.870 --> 00:02:50.060
多对多模式

00:02:50.060 --> 00:02:52.069
有这两种模式

00:02:52.069 --> 00:02:54.289
如果我们使用单个 RNN

00:02:54.289 --> 00:02:59.900
那么就需要输出最多和输入一样多的向量

00:02:59.900 --> 00:03:02.150
这对聊天机器人来说不可行

00:03:02.150 --> 00:03:06.170
我们还希望在开始生成回复之前

00:03:06.169 --> 00:03:08.822
模型能传入整个输入

00:03:08.823 --> 00:03:12.626
所以不能满足我们的需求

00:03:12.626 --> 00:03:14.000
但是再看看这个模式

00:03:14.000 --> 00:03:18.669
也是多对多 RNN

00:03:18.669 --> 00:03:23.149
幸运的是 在 2014 年出现了两种 RNN 结构

00:03:23.150 --> 00:03:25.909
可以将任何长度的序列映射到

00:03:25.909 --> 00:03:28.400
另一个任何长度的序列

00:03:28.400 --> 00:03:30.590
基本原理是

00:03:30.590 --> 00:03:34.310
使用两个 RNN 一个读取输入序列

00:03:34.310 --> 00:03:37.159
然后将所学的规律

00:03:37.159 --> 00:03:41.810
发送给另一个 RNN 该 RNN 接着生成输出序列

00:03:41.810 --> 00:03:44.180
在接下来的视频中 我们将详细了解这种模式

00:03:44.180 --> 00:03:47.659
以及重要 RNN 概念

00:03:47.659 --> 00:03:50.210
我们还将详细介绍

00:03:50.210 --> 00:03:53.210
如何在 TensorFlow 中实现序列到序列模型

00:03:53.210 --> 00:03:55.189
但在此之前

00:03:55.189 --> 00:03:56.930
请先思考下当你能够像这样训练网络后

00:03:56.930 --> 00:04:01.497
网络可以完成的各种复杂任务

