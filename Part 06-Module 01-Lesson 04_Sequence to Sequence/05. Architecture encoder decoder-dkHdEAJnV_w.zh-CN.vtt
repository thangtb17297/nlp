WEBVTT
Kind: captions
Language: zh-CN

00:00:00.500 --> 00:00:03.209
我们再仔细研究下序列到序列模型的

00:00:03.209 --> 00:00:03.890
工作原理

00:00:03.890 --> 00:00:06.720
我们先从更高层面去了解

00:00:06.719 --> 00:00:09.189
然后逐渐深入讲解

00:00:09.189 --> 00:00:12.390
这是两个循环神经网络

00:00:12.390 --> 00:00:14.490
左侧的叫做编码器

00:00:14.490 --> 00:00:16.679
负责读取输入序列

00:00:16.679 --> 00:00:19.320
然后将所发现的规律传递给

00:00:19.320 --> 00:00:22.460
右侧的 RNN 我们称之为解码器

00:00:22.460 --> 00:00:25.679
解码器会生成输出序列

00:00:25.679 --> 00:00:27.689
传递给解码器的“规律”

00:00:27.690 --> 00:00:31.710
是固定大小的张量

00:00:31.710 --> 00:00:35.100
称为状态 有时候叫做上下文

00:00:35.100 --> 00:00:37.050
C 表示上下文

00:00:37.049 --> 00:00:40.859
无论这里的输入和输出有多短或多长

00:00:40.859 --> 00:00:42.630
上下文的大小保持不变

00:00:42.630 --> 00:00:46.290
我们一开始构建该模型时就声明了它的大小

00:00:46.289 --> 00:00:48.570
从高的层面看就是

00:00:48.570 --> 00:00:52.079
推导流程是将输入传递给编码器

00:00:52.079 --> 00:00:53.879
编码器将所发现的规律

00:00:53.880 --> 00:00:57.300
总结为上下文变量或者叫状态

00:00:57.299 --> 00:00:59.609
并传递给解码器

00:00:59.609 --> 00:01:03.629
解码器然后加以处理并输出序列

00:01:03.630 --> 00:01:05.820
如果我们再深入看看

00:01:05.819 --> 00:01:09.869
我们发现 因为编码器和解码器是 RNN

00:01:09.870 --> 00:01:12.719
它们自然都有循环

00:01:12.719 --> 00:01:15.840
这样才能处理这些输入和输出

00:01:15.840 --> 00:01:17.100
序列

00:01:17.099 --> 00:01:18.649
我们来看个示例

00:01:18.650 --> 00:01:20.700
假设模型是个聊天机器人

00:01:20.700 --> 00:01:23.939
我们想问“How are you ?”

00:01:23.939 --> 00:01:26.750
首先我们需要把句子拆成单词

00:01:26.750 --> 00:01:31.170
将其分解为四个小块

00:01:31.170 --> 00:01:33.090
因为有四个元素

00:01:33.090 --> 00:01:35.490
所以该 RNN 需要四个时间步长

00:01:35.489 --> 00:01:37.140
来读取整个序列

00:01:37.140 --> 00:01:39.000
每次都会读取一个输入

00:01:39.000 --> 00:01:41.790
然后在隐藏状态上进行转换

00:01:41.790 --> 00:01:45.430
并将该隐藏状态发送给下个时间步长

00:01:45.430 --> 00:01:48.660
时钟符号表示我们将时间步长

00:01:48.659 --> 00:01:50.729
往后移了一个

00:01:50.730 --> 00:01:54.030
表示 RNN 数据流的一种实用方法是

00:01:54.030 --> 00:01:57.299
“展开”RNN

00:01:57.299 --> 00:02:00.119
即用图表将每个时间步长

00:02:00.120 --> 00:02:01.650
表示为一个单独的单元

00:02:01.650 --> 00:02:05.070
虽然实际上是同一单元

00:02:05.069 --> 00:02:07.439
只是处理了新的输入

00:02:07.439 --> 00:02:09.900
并包含上一个时间步长的

00:02:09.900 --> 00:02:12.599
隐藏状态

00:02:12.599 --> 00:02:15.164
你可能会问 什么是隐藏状态

00:02:15.164 --> 00:02:17.039
最简单的情况下 可以想象为

00:02:17.039 --> 00:02:20.789
单元中一系列隐藏的单位

00:02:20.789 --> 00:02:22.739
实际上 更有可能是

00:02:22.740 --> 00:02:25.740
长短记忆网络 (LSTM) 单元中的

00:02:25.740 --> 00:02:28.409
隐藏状态

00:02:28.409 --> 00:02:31.829
网络大小通常是我们在构建模型时

00:02:31.830 --> 00:02:34.050
可以设置的另一个超参数

00:02:34.050 --> 00:02:36.510
隐藏状态越大

00:02:36.509 --> 00:02:38.639
网络大小就越大 模型的学习容量越大

00:02:38.639 --> 00:02:44.429
并越能查看和了解规律

00:02:44.430 --> 00:02:47.550
但是训练或部署所需的资源也越多

00:02:47.550 --> 00:02:51.160
也就是对处理器和内存

00:02:51.159 --> 00:02:51.659
要求越高

00:02:51.659 --> 00:02:54.299
所以通常你需要

00:02:54.300 --> 00:02:57.510
对模型的这一利弊进行权衡

00:02:57.509 --> 00:03:00.789
解码器也是类似的流程

00:03:00.789 --> 00:03:03.900
我们向其提供编码器生成的数据

00:03:03.900 --> 00:03:07.230
然后逐个元素地生成输出

00:03:07.229 --> 00:03:09.389
如果像之前展开编码器那样

00:03:09.389 --> 00:03:12.059
展开解码器的话

00:03:12.060 --> 00:03:15.300
我们可以看到我们实际上不断把输出当作新的

00:03:15.300 --> 00:03:16.590
输入 传入到模型中

00:03:16.590 --> 00:03:20.400
这样更具连贯性

00:03:20.400 --> 00:03:23.280
因为每个时间步长都能差不多记住上一个时间步长的

00:03:23.280 --> 00:03:24.210
内容

00:03:24.210 --> 00:03:27.420
在下个视频中 我们将进一步了解

00:03:27.419 --> 00:03:31.269
该结构的一些细节部分

