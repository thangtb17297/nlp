WEBVTT
Kind: captions
Language: pt-BR

00:00:00.400 --> 00:00:03.200
O tamanho de mini-lote
é outro hiperparâmetro

00:00:03.233 --> 00:00:06.367
que você já deve
ter encontrado muitas vezes.

00:00:06.933 --> 00:00:10.467
Ele afeta a exigência de recursos
do processo de treinamento,

00:00:10.500 --> 00:00:13.333
a velocidade do treinamento
e o número de iterações

00:00:13.367 --> 00:00:16.933
de uma forma
menos trivial do que parece.

00:00:16.967 --> 00:00:20.667
É importante revisarmos
a terminologia antes.

00:00:21.000 --> 00:00:23.233
Historicamente,
tem havido debates

00:00:23.267 --> 00:00:28.167
se é melhor fazer o treinamento
on-line - o estocástico -,

00:00:28.200 --> 00:00:33.233
em que o input é de só 1 exemplo
de dados para o modelo a cada passo.

00:00:33.267 --> 00:00:36.333
Usa-se esse exemplo
para fazer o passo direto,

00:00:36.367 --> 00:00:39.133
calcular o erro
e fazer a backpropagation

00:00:39.167 --> 00:00:41.967
para ajustar os valores
dos parâmetros.

00:00:42.267 --> 00:00:45.467
Você faz isso
com cada exemplo dos dados.

00:00:45.900 --> 00:00:50.400
Ou se é melhor que o input
seja todo o conjunto de dados

00:00:50.433 --> 00:00:53.033
e calcular o gradiente
com o erro gerado

00:00:53.067 --> 00:00:55.833
pela análise
de todos os exemplos.

00:00:55.867 --> 00:00:58.167
Isso se chama
treinamento em lote.

00:00:58.467 --> 00:01:02.433
A abstração usada hoje é
determinar um tamanho de mini-lote.

00:01:02.467 --> 00:01:06.133
O treinamento on-line é quando
esse tamanho é 1.

00:01:06.367 --> 00:01:09.333
O treinamento em lote
é quando esse tamanho

00:01:09.367 --> 00:01:12.867
é o mesmo que o número de exemplos
dos dados.

00:01:12.900 --> 00:01:14.833
O tamanho do mini-lote

00:01:14.867 --> 00:01:17.667
pode estar
entre esses dois valores.

00:01:17.867 --> 00:01:20.967
Os valores iniciais recomendados
para experimentos

00:01:21.000 --> 00:01:22.867
estão entre 1 e 256.

00:01:22.900 --> 00:01:25.867
32 é um bom candidato.

00:01:26.100 --> 00:01:29.400
Um tamanho de mini-lote maior
permite uma melhora computacional

00:01:29.433 --> 00:01:33.200
que usa multiplicação de matrizes
nos cálculos de treinamento.

00:01:33.233 --> 00:01:37.000
Mas, assim, o processo
exige mais memória

00:01:37.033 --> 00:01:39.833
e mais recursos
computacionais.

00:01:39.867 --> 00:01:43.133
Alguns erros de falta de memória
em TensorFlow

00:01:43.167 --> 00:01:46.333
podem ser eliminados
reduzindo o tamanho do mini-lote.

00:01:46.367 --> 00:01:49.700
Mas é importante saber
que essa melhora computacional

00:01:49.733 --> 00:01:51.233
tem um preço.

00:01:51.267 --> 00:01:53.533
Na prática, tamanhos
de mini-lote pequenos

00:01:53.567 --> 00:01:56.333
têm mais ruído
nos cálculos de erro.

00:01:56.367 --> 00:01:59.900
Esse ruído é útil em impedir
que o processo de treinamento

00:01:59.933 --> 00:02:02.600
pare no mínimo local
na curva

00:02:02.633 --> 00:02:06.133
em vez de no mínimo global,
responsável pelo melhor modelo.

00:02:06.167 --> 00:02:09.867
Enquanto a melhora computacional
nos incentiva

00:02:09.900 --> 00:02:12.033
a aumentar o tamanho
do mini-lote,

00:02:12.067 --> 00:02:14.533
esse benefício prático
do algoritmo

00:02:14.567 --> 00:02:17.200
nos incentiva a reduzi-lo.

00:02:17.233 --> 00:02:21.800
Além de 32,
experimente com 64,

00:02:21.833 --> 00:02:25.367
com 128 e com 256.

00:02:25.400 --> 00:02:27.333
Dependendo dos dados
e da tarefa,

00:02:27.367 --> 00:02:30.333
talvez você experimente
usar outros valores.

00:02:30.367 --> 00:02:32.400
Este é o resultado
de uma experiência

00:02:32.433 --> 00:02:36.867
sobre o efeito do tamanho de lote
em redes neurais convolutivas.

00:02:36.900 --> 00:02:38.400
É do trabalho

00:02:38.433 --> 00:02:42.433
"Systematic evaluation of CNN
advances on the ImageNet".

00:02:42.467 --> 00:02:46.667
Com a mesma taxa de aprendizado,
a precisão do modelo diminui

00:02:46.700 --> 00:02:49.467
quanto maior for
o tamanho do mini-lote.

00:02:49.800 --> 00:02:52.700
Isso não é consequência
só do tamanho,

00:02:52.733 --> 00:02:56.300
já que mudamos
a taxa de aprendizado

00:02:56.333 --> 00:02:58.300
quando mudamos
o tamanho do mini-lote.

00:02:58.333 --> 00:03:00.733
Se ajustarmos
a taxa de aprendizado

00:03:00.767 --> 00:03:06.133
quando aumentamos o tamanho do lote,
a precisão diminui, mas muito pouco,

00:03:06.167 --> 00:03:08.167
com esse aumento.

00:03:08.200 --> 00:03:11.233
Para resumir, em relação
ao tamanho do mini-lote,

00:03:11.267 --> 00:03:13.467
pequeno demais
pode ser lento demais.

00:03:13.500 --> 00:03:16.533
Se for grande demais,
pode exigir muitos recursos

00:03:16.567 --> 00:03:19.067
e ser mais impreciso.

00:03:19.100 --> 00:03:21.567
Um valor inicial
entre 32 e 256

00:03:21.600 --> 00:03:25.300
são bons para você
experimentar.

