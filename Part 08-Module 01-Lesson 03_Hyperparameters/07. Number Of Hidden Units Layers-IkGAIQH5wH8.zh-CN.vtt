WEBVTT
Kind: captions
Language: zh-CN

00:00:04.000 --> 00:00:06.400
现在我们来聊聊与模型本身

00:00:06.400 --> 00:00:09.400
相关的超参数 而非与训练或

00:00:09.400 --> 00:00:11.740
优化过程相关的超参数

00:00:11.740 --> 00:00:13.870
特别是隐藏单元数这个超参数

00:00:13.869 --> 00:00:17.349
当我刚开始学机器学习时

00:00:17.350 --> 00:00:20.260
我觉得这是最神秘的部分

00:00:20.260 --> 00:00:23.650
这块的主要要求是设置一些隐藏单元

00:00:23.649 --> 00:00:27.219
它们要“足够大”

00:00:27.219 --> 00:00:30.500
使神经网络能够通过学习它们来估计函数

00:00:30.500 --> 00:00:34.210
或者对于预测任务 它要有足够的

00:00:34.210 --> 00:00:37.670
“能力”来学习函数

00:00:37.670 --> 00:00:39.640
函数越复杂

00:00:39.640 --> 00:00:42.310
模型需要的学习能力就越高

00:00:42.310 --> 00:00:44.440
隐藏单元的数量和架构

00:00:44.439 --> 00:00:48.219
是衡量模型学习能力的主要标准

00:00:48.219 --> 00:00:50.820
但是如果我们赋予模型太多的能力

00:00:50.820 --> 00:00:53.590
模型会倾向于过拟合

00:00:53.590 --> 00:00:56.140
结果只会记住训练集

00:00:56.140 --> 00:00:58.420
如果你发现模型在过拟合数据

00:00:58.420 --> 00:01:00.850
也就是说训练准确度

00:01:00.850 --> 00:01:03.195
远高于验证准确度

00:01:03.195 --> 00:01:04.569
你可能需要尝试

00:01:04.569 --> 00:01:06.480
减少隐藏单元的数量

00:01:06.480 --> 00:01:09.010
你也可以使用正则化技术

00:01:09.010 --> 00:01:12.609
如丢弃或 L2 正则化

00:01:12.609 --> 00:01:17.620
所以就隐藏单元的数量来说

00:01:17.620 --> 00:01:21.160
越多越好 稍微超过理想数量不成问题

00:01:21.159 --> 00:01:23.109
但是如果过多

00:01:23.109 --> 00:01:25.510
往往会导致模型过拟合

00:01:25.510 --> 00:01:27.640
所以如果你的模型无法训练

00:01:27.640 --> 00:01:31.060
就向它添加更多隐藏层 并跟踪验证误差

00:01:31.060 --> 00:01:33.579
持续添加隐藏单元 直到

00:01:33.579 --> 00:01:35.620
验证误差开始变大

00:01:35.620 --> 00:01:38.439
涉及第一个隐藏层的另一个启发是

00:01:38.439 --> 00:01:41.019
据观察 将它设为大于

00:01:41.019 --> 00:01:43.959
输入数量的一个数

00:01:43.959 --> 00:01:46.549
在多个任务中是有益的

00:01:46.549 --> 00:01:48.509
那层数呢？

00:01:48.510 --> 00:01:51.820
Andrej Karpathy 告诉我们 在实践中

00:01:51.819 --> 00:01:54.609
三层神经网络的性能

00:01:54.609 --> 00:01:57.700
往往优于两层网络的性能

00:01:57.700 --> 00:02:01.570
但继续增加层却作用不大

00:02:01.569 --> 00:02:04.569
不过 卷积神经网络除外

00:02:04.569 --> 00:02:08.459
它们往往是越深性能越好

