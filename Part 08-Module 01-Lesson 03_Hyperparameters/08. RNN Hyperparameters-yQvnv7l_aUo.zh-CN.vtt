WEBVTT
Kind: captions
Language: zh-CN

00:00:04.000 --> 00:00:05.799
在构建循环神经网络时

00:00:05.799 --> 00:00:09.189
我们需要做的两个主要选择是

00:00:09.189 --> 00:00:14.820
选一个 cell 类型 例如长短时记忆 cell

00:00:14.820 --> 00:00:20.980
vanilla RNN cell 或门控循环单元 cell

00:00:20.980 --> 00:00:22.350
以及模型的深度

00:00:22.350 --> 00:00:24.929
即我们要堆叠多少个层？

00:00:24.929 --> 00:00:26.730
而且由于当我们的输入是单词时

00:00:26.730 --> 00:00:29.039
我们将需要单词嵌入

00:00:29.039 --> 00:00:31.679
我们还应考虑嵌入维度

00:00:31.679 --> 00:00:35.219
在实践中 LSTM 和 GRU 的性能优于

00:00:35.219 --> 00:00:36.629
vanilla RNN

00:00:36.630 --> 00:00:38.550
这一点大家都很明确

00:00:38.549 --> 00:00:41.439
那么 你应使用这两个中的哪一个呢？

00:00:41.439 --> 00:00:44.189
虽然 LSTM 似乎更常用

00:00:44.189 --> 00:00:47.070
但这更多地取决于任务和数据集

00:00:47.070 --> 00:00:49.289
很多对比这两者的研究论文

00:00:49.289 --> 00:00:51.269
最终都没能给出一个明确的答案

00:00:51.270 --> 00:00:54.359
我们来看一个示例任务 字符级

00:00:54.359 --> 00:00:56.060
语言建模

00:00:56.060 --> 00:00:59.920
一篇名为《可视化和理解你的循环网络》

00:00:59.920 --> 00:01:02.550
的优秀论文使用两个不同的数据集

00:01:02.549 --> 00:01:04.649
比较了这两者

00:01:04.650 --> 00:01:07.260
第一个数据集的结果表明

00:01:07.260 --> 00:01:09.960
GRU 优于 LSTM

00:01:09.959 --> 00:01:13.140
这里的优于指的是在大小不同情况下

00:01:13.140 --> 00:01:15.629
交叉熵损失更低

00:01:15.629 --> 00:01:19.829
在另一个数据集上 它们两结合在了一起 每个分别在

00:01:19.829 --> 00:01:21.689
不同的大小情况下得分更高

00:01:21.689 --> 00:01:24.299
在视频下的文本中 我们对这个对比

00:01:24.299 --> 00:01:25.739
做了详细说明

00:01:25.739 --> 00:01:28.559
但我建议在你的数据集和任务上两个都试试

00:01:28.560 --> 00:01:30.540
对比看看

00:01:30.540 --> 00:01:33.900
注意 你不需要在整个数据集上进行测试

00:01:33.900 --> 00:01:37.680
而是使用你的数据的一个随机子集

00:01:37.680 --> 00:01:41.190
对于层数 字符级语言建模的

00:01:41.189 --> 00:01:44.579
结果显示至少两层的深度

00:01:44.579 --> 00:01:46.289
较为有益

00:01:46.290 --> 00:01:50.650
但增加到三实际上会使结果有点混乱

00:01:50.650 --> 00:01:55.050
另一个任务 如高级语音识别

00:01:55.049 --> 00:01:58.950
显示在没有 LSTM cell 的情况下 五层甚至七层

00:01:58.950 --> 00:02:01.109
会改善结果

00:02:01.109 --> 00:02:02.700
在视频下的文本中 我们

00:02:02.700 --> 00:02:05.040
提供了不同任务的示例架构

00:02:05.040 --> 00:02:08.219
以及每个示例的层大小

00:02:08.219 --> 00:02:10.175
和数量

00:02:10.175 --> 00:02:13.500
如果你的 RNN 将使用单词作为输入

00:02:13.500 --> 00:02:16.620
那么你需要为嵌入向量选择大小

00:02:16.620 --> 00:02:19.560
那么我们该如何选择这个数字呢？

00:02:19.560 --> 00:02:21.840
一篇名为《如何生成良好的单词嵌入》的

00:02:21.840 --> 00:02:24.960
论文中的实验结果

00:02:24.960 --> 00:02:27.990
表明嵌入的大小越大 某些任务

00:02:27.990 --> 00:02:30.590
的性能越高

00:02:30.590 --> 00:02:33.650
大小至少到 200

00:02:33.650 --> 00:02:36.810
但在其他测试中 超过 50 的大小

00:02:36.810 --> 00:02:39.531
仅取得了非常小的改进

