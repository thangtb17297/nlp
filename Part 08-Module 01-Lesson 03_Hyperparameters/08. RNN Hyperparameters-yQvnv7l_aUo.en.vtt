WEBVTT
Kind: captions
Language: en

00:00:04.000 --> 00:00:05.799
Two main choices
we need to make

00:00:05.799 --> 00:00:09.189
when we want to build a
recurrent neural network

00:00:09.189 --> 00:00:14.820
are choosing a cell type,
so a long short term memory

00:00:14.820 --> 00:00:20.980
cell or a vanilla RNN cell or
a gated recurrent unit cell,

00:00:20.980 --> 00:00:22.350
and how deep the model is.

00:00:22.350 --> 00:00:24.929
How many layers will we stack?

00:00:24.929 --> 00:00:26.730
And since we'll
need word embeddings

00:00:26.730 --> 00:00:29.039
if our inputs are
words, we'll also

00:00:29.039 --> 00:00:31.679
look at embedding
dimensionality.

00:00:31.679 --> 00:00:35.219
In practice, LSTMs and
GRUs perform better

00:00:35.219 --> 00:00:36.629
than vanilla RNN.

00:00:36.630 --> 00:00:38.550
That much is clear.

00:00:38.549 --> 00:00:41.439
So which of the
two should you use?

00:00:41.439 --> 00:00:44.189
While LSTMs seemed to
be more commonly used,

00:00:44.189 --> 00:00:47.070
it really depends on the
task and the dataset.

00:00:47.070 --> 00:00:49.289
Multiple research
papers comparing the two

00:00:49.289 --> 00:00:51.269
did not announce a clear winner.

00:00:51.270 --> 00:00:54.359
Let's take an example
task, character level

00:00:54.359 --> 00:00:56.060
language modeling.

00:00:56.060 --> 00:00:59.920
A great paper titled
Visualizing and understanding

00:00:59.920 --> 00:01:02.550
your recurrent networks
compare the two

00:01:02.549 --> 00:01:04.649
on two different datasets.

00:01:04.650 --> 00:01:07.260
The result on the
first dataset saw

00:01:07.260 --> 00:01:09.960
GRUs doing better than LSTMs.

00:01:09.959 --> 00:01:13.140
Better here being lower
across entropy loss

00:01:13.140 --> 00:01:15.629
when comparing them
at different sizes.

00:01:15.629 --> 00:01:19.829
On a different dataset, the two
are tied each scoring better

00:01:19.829 --> 00:01:21.689
at different sizes.

00:01:21.689 --> 00:01:24.299
In the text below the
video we elaborate more

00:01:24.299 --> 00:01:25.739
on this comparison.

00:01:25.739 --> 00:01:28.559
But the recommendation here
is to try both on your dataset

00:01:28.560 --> 00:01:30.540
and task and compare.

00:01:30.540 --> 00:01:33.900
Note that you don't have to test
this on your entire dataset.

00:01:33.900 --> 00:01:37.680
You can try it on a random
subset of your data.

00:01:37.680 --> 00:01:41.190
Regarding the number of layers,
results for character level

00:01:41.189 --> 00:01:44.579
language modeling show that
a depth of at least two

00:01:44.579 --> 00:01:46.289
is shown to be beneficial.

00:01:46.290 --> 00:01:50.650
But increasing it to three
actually gives mixed results.

00:01:50.650 --> 00:01:55.050
Another task like advanced
speech recognition

00:01:55.049 --> 00:01:58.950
can show improvements with five
and even seven layers often

00:01:58.950 --> 00:02:01.109
without LSTM cells.

00:02:01.109 --> 00:02:02.700
In the text below
the video, we have

00:02:02.700 --> 00:02:05.040
provided a number of
example architectures

00:02:05.040 --> 00:02:08.219
for different tasks along
with the sizes and number

00:02:08.219 --> 00:02:10.175
of layers for each example.

00:02:10.175 --> 00:02:13.500
If your RNN will be
using words as inputs,

00:02:13.500 --> 00:02:16.620
then you'll need to choose a
size for the embedding vector.

00:02:16.620 --> 00:02:19.560
How do we go about
choosing this number?

00:02:19.560 --> 00:02:21.840
Experimental results
reporting a paper

00:02:21.840 --> 00:02:24.960
titled how to generate
a good word embedding

00:02:24.960 --> 00:02:27.990
show that the
performance of some tasks

00:02:27.990 --> 00:02:30.590
improve the larger we
make the embedding,

00:02:30.590 --> 00:02:33.650
at least until a size of 200.

00:02:33.650 --> 00:02:36.810
In other tests, however,
only marginal improvements

00:02:36.810 --> 00:02:39.531
are realized beyond
the size of 50.

