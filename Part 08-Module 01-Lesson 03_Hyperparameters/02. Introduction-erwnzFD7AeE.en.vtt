WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:02.620
Hello, I'm Jay.

00:00:02.620 --> 00:00:04.379
In this lesson,
we will be talking

00:00:04.379 --> 00:00:07.019
about a number of
different hyper parameters

00:00:07.019 --> 00:00:10.320
and identifying possible
starting values and intuitions

00:00:10.320 --> 00:00:13.199
for a number of hyper parameters
that you may have already come

00:00:13.199 --> 00:00:15.929
across, and that you'll
need to know in your work

00:00:15.929 --> 00:00:17.489
with deep learning.

00:00:17.489 --> 00:00:19.709
A hyper parameter is
a variable that we

00:00:19.710 --> 00:00:21.990
need to set before
applying a learning

00:00:21.989 --> 00:00:24.389
algorithm into a dataset.

00:00:24.390 --> 00:00:26.730
The challenge with
hyper parameters

00:00:26.730 --> 00:00:30.839
is that there are no magic
numbers that work everywhere.

00:00:30.839 --> 00:00:34.799
The best numbers depend on
each task and each dataset.

00:00:34.799 --> 00:00:37.679
So in addition to talking
about starting values,

00:00:37.679 --> 00:00:40.170
we'll try to touch on
the intuition of why

00:00:40.170 --> 00:00:44.280
we'd want to nudge a hyper
parameter one way or another.

00:00:44.280 --> 00:00:47.130
Generally speaking, we can
break hyper parameters down

00:00:47.130 --> 00:00:49.030
into two categories.

00:00:49.030 --> 00:00:53.020
The first category is
optimizer hyper parameters.

00:00:53.020 --> 00:00:54.700
These are the
variables related more

00:00:54.700 --> 00:00:57.480
to the optimization
and training process

00:00:57.479 --> 00:00:59.640
than to the model itself.

00:00:59.640 --> 00:01:05.129
These include the learning
rate, the minibatch size,

00:01:05.129 --> 00:01:09.030
and the number of training
iterations or epochs.

00:01:09.030 --> 00:01:13.450
The second category is
model hyper parameters.

00:01:13.450 --> 00:01:15.689
These are the variables
that are more involved

00:01:15.689 --> 00:01:18.149
in the structure of the model.

00:01:18.150 --> 00:01:22.230
These include the number
of layers and hidden units

00:01:22.230 --> 00:01:26.040
and model specific hyper
parameters for architectures

00:01:26.040 --> 00:01:27.840
like RNMs.

00:01:27.840 --> 00:01:30.570
In the next video, we'll
start with the single most

00:01:30.569 --> 00:01:35.179
important hyper parameter
of all, the learning rate.

