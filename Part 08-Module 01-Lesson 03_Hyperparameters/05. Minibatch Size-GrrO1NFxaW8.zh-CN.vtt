WEBVTT
Kind: captions
Language: zh-CN

00:00:00.510 --> 00:00:03.240
Minibatch 大小是另一个超参数

00:00:03.240 --> 00:00:06.990
想必你已经遇到过很多次了

00:00:06.990 --> 00:00:09.539
它对训练过程的资源要求有影响

00:00:09.539 --> 00:00:12.359
但也会影响训练速度和

00:00:12.359 --> 00:00:14.669
迭代次数 而且这些影响

00:00:14.669 --> 00:00:17.100
不容小视

00:00:17.100 --> 00:00:19.590
我们有必要先来了解一下

00:00:19.589 --> 00:00:20.969
这个术语

00:00:20.969 --> 00:00:24.269
一直以来 人们都在争论哪种做法更好

00:00:24.269 --> 00:00:28.410
一种是在线随机训练

00:00:28.410 --> 00:00:30.809
你在单个训练步长中将数据集

00:00:30.809 --> 00:00:33.240
中的单个示例馈送给模型

00:00:33.240 --> 00:00:36.359
然后仅用这一个示例进行前向传递

00:00:36.359 --> 00:00:38.850
计算误差 然后反向传播

00:00:38.850 --> 00:00:42.300
并设置所有参数的调整数值

00:00:42.299 --> 00:00:45.869
然后对数据集中的每个示例执行这个过程

00:00:45.869 --> 00:00:49.619
另一种是将整个数据集馈送给训练步长

00:00:49.619 --> 00:00:51.989
使用数据集中

00:00:51.990 --> 00:00:54.120
所有示例的误差

00:00:54.119 --> 00:00:55.949
来计算整个数据集的梯度

00:00:55.950 --> 00:00:58.490
这叫做批量训练

00:00:58.490 --> 00:01:00.420
如今普遍使用的抽象

00:01:00.420 --> 00:01:02.340
是设置一个 minibatch 大小

00:01:02.340 --> 00:01:06.450
那么 在线训练的 minibatch 大小就为 1

00:01:06.450 --> 00:01:08.820
而批量训练的 minibatch 大小

00:01:08.819 --> 00:01:11.819
与训练集中的示例数量

00:01:11.819 --> 00:01:12.899
相同

00:01:12.900 --> 00:01:14.609
我们可以将 minibatch 大小设置为

00:01:14.609 --> 00:01:17.879
这两个值之间的任意值

00:01:17.879 --> 00:01:20.939
当你进行试验时 建议初始值

00:01:20.939 --> 00:01:23.939
设在 1 到几百之间 32 通常

00:01:23.939 --> 00:01:26.159
是一个不错的选择

00:01:26.159 --> 00:01:29.369
较大的 minibatch 大小会

00:01:29.370 --> 00:01:32.070
使训练计算中使用矩阵乘法的

00:01:32.069 --> 00:01:33.239
计算速度加快

00:01:33.239 --> 00:01:35.939
但这也会在训练过程中

00:01:35.939 --> 00:01:37.799
占用更多的内存 总体来说

00:01:37.799 --> 00:01:40.349
需要更多的计算资源

00:01:40.349 --> 00:01:43.079
如果遇到内存不足错误和 TensorFlow 问题

00:01:43.079 --> 00:01:46.439
可以通过减小 minibatch 大小来解决

00:01:46.439 --> 00:01:49.649
但是 需要注意的一点是计算速度增加

00:01:49.650 --> 00:01:51.240
是有代价的

00:01:51.239 --> 00:01:54.539
在实践中 较小的 minibatch 大小会使误差计算

00:01:54.540 --> 00:01:57.540
中有更多的噪声 而且此噪声

00:01:57.540 --> 00:01:59.880
通常有助于防止训练过程

00:01:59.879 --> 00:02:02.909
在误差曲线上的局部最小值处停止

00:02:02.909 --> 00:02:06.209
而非创建最佳模型的全局最小值

00:02:06.209 --> 00:02:09.659
所以 虽然计算速度的提高会促使我们

00:02:09.659 --> 00:02:12.060
增加 minibatch 大小

00:02:12.060 --> 00:02:14.580
但实际的算法好处会使我们

00:02:14.580 --> 00:02:17.380
选择将它变小

00:02:17.379 --> 00:02:19.650
除了 32 之外 你还可以

00:02:19.650 --> 00:02:25.560
使用 64、128 和 256 进行试验

00:02:25.560 --> 00:02:27.210
根据你的数据和任务

00:02:27.210 --> 00:02:30.390
你可能还需要尝试其他值

00:02:30.389 --> 00:02:33.899
这是卷积神经网络上有效批量的

00:02:33.900 --> 00:02:36.930
实验结果

00:02:36.930 --> 00:02:40.439
它来自一篇名为 “Systematic evaluation of CNN advances

00:02:40.439 --> 00:02:42.449
on the ImageNet” 的文章

00:02:42.449 --> 00:02:44.219
它显示在学习率相同的情况下

00:02:44.219 --> 00:02:47.490
minibatch 大小越大

00:02:47.490 --> 00:02:49.860
模型的准确度越低

00:02:49.860 --> 00:02:52.590
这不仅在于 minibatch 大小的影响

00:02:52.590 --> 00:02:56.189
而且事实是当我们改变批量大小时

00:02:56.189 --> 00:02:58.289
我们还需要改变学习率

00:02:58.289 --> 00:03:01.849
如果我们在增加批量大小的同时

00:03:01.849 --> 00:03:05.009
调整学习率 可以看到准确度会随批量大小

00:03:05.009 --> 00:03:08.310
增加而下降 不过只是轻微的下降

00:03:08.310 --> 00:03:11.280
所以总结来说 如果 minibatch 大小

00:03:11.280 --> 00:03:15.870
太小 会使训练速度很慢 太大会导致

00:03:15.870 --> 00:03:21.659
计算成本过高 并降低准确度 所以 32 至 256

00:03:21.659 --> 00:03:24.090
是你进行试验的

00:03:24.090 --> 00:03:26.330
不错的初始值选择

