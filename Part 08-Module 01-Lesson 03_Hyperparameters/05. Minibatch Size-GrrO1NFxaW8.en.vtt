WEBVTT
Kind: captions
Language: en

00:00:00.510 --> 00:00:03.240
Minibatch size is
another hyper parameter

00:00:03.240 --> 00:00:06.990
that no doubt you've run into
a number of times already.

00:00:06.990 --> 00:00:09.539
It has an effect on the resource
requirements of the training

00:00:09.539 --> 00:00:12.359
process but also impacts
training speed and number

00:00:12.359 --> 00:00:14.669
of iterations in a
way that might not

00:00:14.669 --> 00:00:17.100
be as trivial as you may think.

00:00:17.100 --> 00:00:19.590
It's important to review a
little bit of terminology

00:00:19.589 --> 00:00:20.969
here first.

00:00:20.969 --> 00:00:24.269
Historically there had been
debate on whether it's better

00:00:24.269 --> 00:00:28.410
to do online also called
stochastic training

00:00:28.410 --> 00:00:30.809
where you fit a single
example of the dataset

00:00:30.809 --> 00:00:33.240
to the model during
a training step.

00:00:33.240 --> 00:00:36.359
And using only one
example, do a forward pass,

00:00:36.359 --> 00:00:38.850
calculate the error,
and then back propagate

00:00:38.850 --> 00:00:42.300
and set adjusted values
for all your parameters.

00:00:42.299 --> 00:00:45.869
And then do this again for
each example in the dataset.

00:00:45.869 --> 00:00:49.619
Or if it was better to feed the
entire dataset to the training

00:00:49.619 --> 00:00:51.989
step and calculate
that gradient using

00:00:51.990 --> 00:00:54.120
the error generated
by looking at all

00:00:54.119 --> 00:00:55.949
the examples in the dataset.

00:00:55.950 --> 00:00:58.490
This is called batch training.

00:00:58.490 --> 00:01:00.420
The abstraction
commonly used today

00:01:00.420 --> 00:01:02.340
is to set a minibatch size.

00:01:02.340 --> 00:01:06.450
So online training is when
the minibatch size is one,

00:01:06.450 --> 00:01:08.820
and batch training
is when the minibatch

00:01:08.819 --> 00:01:11.819
size is the same as the number
of examples in the training

00:01:11.819 --> 00:01:12.899
set.

00:01:12.900 --> 00:01:14.609
And we can set
the minibatch size

00:01:14.609 --> 00:01:17.879
to any value between
these two values.

00:01:17.879 --> 00:01:20.939
The recommended starting
values for your experimentation

00:01:20.939 --> 00:01:23.939
are between one and
a few hundred with 32

00:01:23.939 --> 00:01:26.159
often being a good candidate.

00:01:26.159 --> 00:01:29.369
A larger minibatch size
allows computational boosts

00:01:29.370 --> 00:01:32.070
that utilizes matrix
multiplication, in the training

00:01:32.069 --> 00:01:33.239
calculations.

00:01:33.239 --> 00:01:35.939
But that comes at the expense
of needing more memory

00:01:35.939 --> 00:01:37.799
for the training
process, and generally,

00:01:37.799 --> 00:01:40.349
more computational resources.

00:01:40.349 --> 00:01:43.079
Some out of memory
errors and TensorFlow

00:01:43.079 --> 00:01:46.439
can be eliminated by
decreasing the minibatch size.

00:01:46.439 --> 00:01:49.649
It's important, however, to note
that this computational boost

00:01:49.650 --> 00:01:51.240
comes at a cost.

00:01:51.239 --> 00:01:54.539
In practice, small minibatch
sizes have more noise

00:01:54.540 --> 00:01:57.540
in their error
calculations, and this noise

00:01:57.540 --> 00:01:59.880
is often helpful in preventing
the training process

00:01:59.879 --> 00:02:02.909
from stopping at local
minima on the error curve

00:02:02.909 --> 00:02:06.209
rather than the global minima
that creates the best model.

00:02:06.209 --> 00:02:09.659
So while the computational
boost incentivizes

00:02:09.659 --> 00:02:12.060
us to increase the
minibatch size,

00:02:12.060 --> 00:02:14.580
this practical
algorithmic benefit

00:02:14.580 --> 00:02:17.380
incentivizes us to
actually make it smaller.

00:02:17.379 --> 00:02:19.650
So in addition to
32, you might also

00:02:19.650 --> 00:02:25.560
want to try to experiment
with 64 and 128 and 256.

00:02:25.560 --> 00:02:27.210
And depending on
your data and task,

00:02:27.210 --> 00:02:30.390
you might have to experiment
with other values as well.

00:02:30.389 --> 00:02:33.899
This is an experimental result
for the effective batch size

00:02:33.900 --> 00:02:36.930
on convolutional neural nets.

00:02:36.930 --> 00:02:40.439
It's from the paper titled
Systematic evaluation of CNN

00:02:40.439 --> 00:02:42.449
advances on the image net.

00:02:42.449 --> 00:02:44.219
It shows that using
the same learning

00:02:44.219 --> 00:02:47.490
rate, the accuracy of the
model decreases the larger

00:02:47.490 --> 00:02:49.860
the minibatch size becomes.

00:02:49.860 --> 00:02:52.590
Now this is not only the
effect of the minibike size,

00:02:52.590 --> 00:02:56.189
but due to the fact that we
need to change the learning rate

00:02:56.189 --> 00:02:58.289
if we change the batch size.

00:02:58.289 --> 00:03:01.849
If we do adjust the learning
rate as we increase the batch

00:03:01.849 --> 00:03:05.009
size, we can see that the
accuracy does decrease,

00:03:05.009 --> 00:03:08.310
but only slightly, the more
increase the batch size.

00:03:08.310 --> 00:03:11.280
So to sum up for
the minibatch size,

00:03:11.280 --> 00:03:15.870
too small could be too slow, too
large could be computationally

00:03:15.870 --> 00:03:21.659
taxing and could result in
worse accuracy, and 32 to 256

00:03:21.659 --> 00:03:24.090
are potentially good
starting values for you

00:03:24.090 --> 00:03:26.330
to experiment with.

