WEBVTT
Kind: captions
Language: zh-CN

00:00:00.540 --> 00:00:03.370
学习率是最重要的

00:00:03.370 --> 00:00:05.190
一个超参数

00:00:05.190 --> 00:00:07.850
即使你将他人构建的模型应用于

00:00:07.849 --> 00:00:10.800
自己的数据集 你也会发现你可能

00:00:10.800 --> 00:00:12.780
需要尝试多个不同的学习率值

00:00:12.779 --> 00:00:16.769
才能使模型正确训练

00:00:16.769 --> 00:00:20.730
如果你归一化模型的输入

00:00:20.730 --> 00:00:24.660
一个好的起始点通常是 0.01

00:00:24.660 --> 00:00:28.559
这些是学习率的通常假设

00:00:28.559 --> 00:00:30.660
如果你试一个 你的模型并没有正确训练

00:00:30.660 --> 00:00:33.240
你可以试试此列表中其他的

00:00:33.240 --> 00:00:35.700
那么你该试哪个呢？

00:00:35.700 --> 00:00:39.410
这取决于训练误差的行为

00:00:39.409 --> 00:00:41.269
要更好地理解这一点 我们需要

00:00:41.270 --> 00:00:44.380
来看看学习率的直觉判断

00:00:44.380 --> 00:00:46.609
在此课程早些时候 我们看到

00:00:46.609 --> 00:00:50.869
当我们使用梯度下降来训练神经网络模型时

00:00:50.869 --> 00:00:53.329
训练任务的关键在于尽可能降低

00:00:53.329 --> 00:00:56.600
损失函数计算的

00:00:56.600 --> 00:00:58.460
误差值

00:00:58.460 --> 00:01:03.560
在学习过程中 我们在模型中进行前向迭代

00:01:03.560 --> 00:01:07.159
计算损失 然后获得梯度

00:01:07.159 --> 00:01:09.289
假设在最简单的情况下

00:01:09.290 --> 00:01:11.410
我们的模型只有一个权重

00:01:11.409 --> 00:01:13.579
梯度将告诉我们向哪个方向

00:01:13.579 --> 00:01:16.670
推动当前权重 以提高我们预测

00:01:16.670 --> 00:01:18.379
的准确度

00:01:18.379 --> 00:01:21.019
为了可视化学习率的动态

00:01:21.019 --> 00:01:25.369
我们绘制一个权重值与获得的误差值

00:01:25.370 --> 00:01:27.530
的关系图 使用它来计算

00:01:27.530 --> 00:01:31.189
随机训练数据点的预测值

00:01:31.189 --> 00:01:33.250
在理想情况下 如果我们放大

00:01:33.250 --> 00:01:35.120
曲线的正确部分

00:01:35.120 --> 00:01:37.730
权重和误差值的关系

00:01:37.730 --> 00:01:40.939
看起来就像这个完美的 U 型

00:01:40.939 --> 00:01:45.319
选择一个随机权重并计算误差值

00:01:45.319 --> 00:01:48.809
会使我们获得曲线上这样一个点

00:01:48.810 --> 00:01:52.010
注意 我们一开始时并不知道曲线是什么样的

00:01:52.010 --> 00:01:54.680
知道它的唯一方式是计算

00:01:54.680 --> 00:01:56.330
每个权重点的误差

00:01:56.329 --> 00:01:59.659
我们在这里将它画出来只是为了展示它的动态变化

00:01:59.659 --> 00:02:01.879
计算梯度会告诉我们

00:02:01.879 --> 00:02:04.939
朝哪个方向可以降低误差

00:02:04.939 --> 00:02:06.620
如果我们的计算是正确的

00:02:06.620 --> 00:02:08.419
梯度会告诉我们下一步的

00:02:08.419 --> 00:02:12.020
方向 即增大或减小

00:02:12.020 --> 00:02:14.240
权重的当前值

00:02:14.240 --> 00:02:16.340
学习率是我们用于

00:02:16.340 --> 00:02:20.099
将权重推向正确方向的乘数

00:02:20.099 --> 00:02:24.019
如果我们奇迹般地做出了正确的

00:02:24.020 --> 00:02:27.650
学习率选择 那么只需一个

00:02:27.650 --> 00:02:30.110
训练步长 我们就能到达最佳权重

00:02:30.110 --> 00:02:32.330
如果我们选择的学习率小于

00:02:32.330 --> 00:02:34.760
理想的学习率 没关系

00:02:34.759 --> 00:02:37.699
我们的模型将继续学习 直到

00:02:37.699 --> 00:02:39.149
找到权重的最佳值

00:02:39.150 --> 00:02:42.530
每个训练步长 它都会更近一步

00:02:42.530 --> 00:02:45.560
直到最终到达最佳权重

00:02:45.560 --> 00:02:48.830
但是 如果学习率太小

00:02:48.830 --> 00:02:52.910
那么我们的训练误差就会降低 但是非常慢

00:02:52.909 --> 00:02:56.299
我们可能经过数千个训练步长

00:02:56.300 --> 00:02:59.120
也无法到达模型的最佳值

00:02:59.120 --> 00:03:02.569
很明显 在这种情况下 我们需要做的

00:03:02.569 --> 00:03:05.539
是提高学习率

00:03:05.539 --> 00:03:08.989
另一种情况是 如果我们选择一个

00:03:08.990 --> 00:03:12.140
大于理想学习率的学习率

00:03:12.139 --> 00:03:14.989
更新的值则将越过理想的

00:03:14.990 --> 00:03:15.881
权重值

00:03:15.881 --> 00:03:17.379
下一次更新时 它会

00:03:17.379 --> 00:03:18.949
在反方向越过最佳值

00:03:18.949 --> 00:03:21.829
但它会越来越靠近 可能最终会

00:03:21.830 --> 00:03:24.620
收敛到一个合理的值

00:03:24.620 --> 00:03:27.080
但是如果我们选择的学习率

00:03:27.080 --> 00:03:28.580
比理想学习率大很多 

00:03:28.580 --> 00:03:33.210
大两倍以上 这就会产生问题

00:03:33.210 --> 00:03:35.510
在这种情况下 我们会看到权重采取

00:03:35.509 --> 00:03:39.259
较大的步长 它不仅越过理想权值

00:03:39.259 --> 00:03:41.629
而且实际上离我们每步获得的

00:03:41.629 --> 00:03:45.919
最佳误差越来越远

00:03:45.919 --> 00:03:49.659
导致这种偏离的因素在于梯度

00:03:49.659 --> 00:03:52.729
梯度不仅指出方向 它还

00:03:52.729 --> 00:03:56.000
提供曲线上该点的切线

00:03:56.000 --> 00:03:58.129
斜率对应的值

00:03:58.129 --> 00:04:00.769
曲线上的点越高 斜率越大

00:04:00.770 --> 00:04:04.370
梯度的值也就越大

00:04:04.370 --> 00:04:08.730
这会使学习率过大的问题更糟

00:04:08.729 --> 00:04:11.750
所以如果我们的训练误差在增加

00:04:11.750 --> 00:04:14.150
不妨试试降低学习率

00:04:14.150 --> 00:04:16.009
看看会发生什么

00:04:16.009 --> 00:04:17.719
这些是你在调整学习率时

00:04:17.720 --> 00:04:20.390
会遇到的普遍情况

00:04:20.389 --> 00:04:23.629
但是注意这是只有一个参数的

00:04:23.629 --> 00:04:27.560
简单示例 并且是理想情况下的凹形误差曲线

00:04:27.560 --> 00:04:29.970
但现实生活中的情况会复杂得多

00:04:29.970 --> 00:04:31.040
想必你都见到过

00:04:31.040 --> 00:04:33.740
你的模型会有成百上千的

00:04:33.740 --> 00:04:36.110
参数 每个都有自己的误差曲线

00:04:36.110 --> 00:04:39.680
会随着其他权重值的变化而变化

00:04:39.680 --> 00:04:42.410
学习率必须引导所有这些

00:04:42.410 --> 00:04:45.800
到达产生的误差最小的最佳值

00:04:45.800 --> 00:04:48.889
更糟糕的是

00:04:48.889 --> 00:04:50.719
我们实际上无法保证

00:04:50.720 --> 00:04:53.810
误差曲线会是整洁的 U 形

00:04:53.810 --> 00:04:56.180
事实上 它们会成为更复杂的形状

00:04:56.180 --> 00:04:58.910
而且学习算法可能会错误地将局部最小值

00:04:58.910 --> 00:05:02.090
当做最佳值进行收敛

00:05:02.089 --> 00:05:04.399
这个图显然已经简化了很多

00:05:04.399 --> 00:05:07.250
因为三个参数与误差值的

00:05:07.250 --> 00:05:10.850
关系曲线在思维空间中是一个平面

00:05:10.850 --> 00:05:12.240
很难可视化

00:05:12.240 --> 00:05:16.069
想想这些先进的算法

00:05:16.069 --> 00:05:17.870
帮我们克服了多少复杂性

00:05:17.870 --> 00:05:21.769
数千万的权重很容易让人感到害怕

00:05:21.769 --> 00:05:23.449
而且它们都有自己的误差曲线

00:05:23.449 --> 00:05:27.589
这些曲线要取决于其他值 而且每个权重

00:05:27.589 --> 00:05:31.219
都有随机的起始值 我们勤快的学习率

00:05:31.220 --> 00:05:34.280
将它们左推右推 以拟合训练数据

00:05:34.279 --> 00:05:36.349
并找到最佳模型

00:05:36.350 --> 00:05:39.290
这是你在此纳米学位课程中

00:05:39.290 --> 00:05:42.430
一次次不断杀死的怪兽

00:05:42.430 --> 00:05:44.180
我只想花一秒钟时间 感叹一下

00:05:44.180 --> 00:05:46.850
这些奇妙的算法赋予我们的

00:05:46.850 --> 00:05:49.385
这些难以置信的力量

00:05:49.384 --> 00:05:51.509
现在我们已经了解了学习率的

00:05:51.509 --> 00:05:55.219
直接判断 以及训练误差

00:05:55.220 --> 00:05:57.470
如何帮助我们调整学习率

00:05:57.470 --> 00:05:59.060
下面我们来看一个在调整学习率时

00:05:59.060 --> 00:06:02.079
经常会遇到的一个具体情形

00:06:02.079 --> 00:06:06.319
假设我们选择了一个合理的学习率

00:06:06.319 --> 00:06:09.949
它可以降低误差 但只能到某一个点

00:06:09.949 --> 00:06:11.959
在那之后就无法下降了

00:06:11.959 --> 00:06:14.599
尽管它还没到达底部

00:06:14.600 --> 00:06:18.500
它会一直在两个值之间振荡 它们

00:06:18.500 --> 00:06:22.129
优于刚开始训练时的误差差

00:06:22.129 --> 00:06:25.250
但却不是此模型的最佳值

00:06:25.250 --> 00:06:28.639
在这种情形下 让我们的训练算法

00:06:28.639 --> 00:06:31.279
降低整个训练过程的学习率

00:06:31.279 --> 00:06:32.569
会比较有用

00:06:32.569 --> 00:06:36.079
此技术叫作学习率衰减

00:06:36.079 --> 00:06:39.289
这么做的直观方式是线性

00:06:39.290 --> 00:06:40.980
降低学习率

00:06:40.980 --> 00:06:44.240
假设 每 5 个 epoch 减半

00:06:44.240 --> 00:06:46.439
就像这里这个示例

00:06:46.439 --> 00:06:49.189
你也可以按指数方式降低学习率

00:06:49.189 --> 00:06:51.800
例如 每 8 个 epoch

00:06:51.800 --> 00:06:55.879
对学习率乘以 0.1

00:06:55.879 --> 00:06:58.480
除了直接降低学习率外

00:06:58.480 --> 00:07:00.920
还有一些聪明的学习算法

00:07:00.920 --> 00:07:03.199
具有自适应学习率

00:07:03.199 --> 00:07:05.240
这些算法能根据其

00:07:05.240 --> 00:07:07.670
知道的问题以及目前为止看到的数据

00:07:07.670 --> 00:07:10.670
调整学习率

00:07:10.670 --> 00:07:13.280
这不仅代表在需要时降低学习率

00:07:13.279 --> 00:07:16.859
还表示在学习率太低时升高它

00:07:16.860 --> 00:07:19.050
在此视频下面 你会找到关于在 TensorFlow 中

00:07:19.050 --> 00:07:24.170
使用自适应学习算法的一些说明

