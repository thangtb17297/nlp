WEBVTT
Kind: captions
Language: pt-BR

00:00:00.024 --> 00:00:04.550
Neste conceito, veremos algumas
aplicações e tarefas

00:00:04.583 --> 00:00:06.758
potencializadas pela atenção.

00:00:06.791 --> 00:00:08.374
No texto abaixo do vídeo,

00:00:08.407 --> 00:00:11.311
há links para vários artigos,
caso queira se aprofundar

00:00:11.344 --> 00:00:14.271
em alguma aplicação
ou tarefa específica.

00:00:14.304 --> 00:00:16.639
Nos concentraremos
na legendagem de imagens,

00:00:16.672 --> 00:00:19.159
e em um dos principais artigos
de 2016,

00:00:19.192 --> 00:00:21.476
intitulado
"Show, Attend and Tell".

00:00:21.509 --> 00:00:23.547
Esse trabalho
apresentou um modelo

00:00:23.580 --> 00:00:25.875
que alcançou um desempenho
impecável

00:00:25.908 --> 00:00:29.371
na geração de legendas
em vários conjuntos de dados.

00:00:29.404 --> 00:00:32.834
Por exemplo, quando apresentada
uma imagem como esta,

00:00:32.867 --> 00:00:34.930
a legenda gerada foi:

00:00:34.963 --> 00:00:37.979
"Uma mulher está jogando frisbee
em um parque."

00:00:38.012 --> 00:00:40.259
Quando apresentada uma imagem
como esta,

00:00:40.292 --> 00:00:42.147
a legenda gerada foi:

00:00:42.180 --> 00:00:45.755
"Uma girafa de pé em uma floresta
com árvores ao fundo."

00:00:45.788 --> 00:00:48.701
Modelos como estes são treinados
em um conjunto de dados

00:00:48.734 --> 00:00:50.187
como o MS COCO,

00:00:50.220 --> 00:00:52.811
que tem um conjunto
de cerca de 200.000 imagens,

00:00:52.844 --> 00:00:56.004
cada uma com cinco legendas
descritas por pessoas.

00:00:56.037 --> 00:01:00.675
Isso é obtido por meio de algo como
o Amazon Mechanical Turk.

00:01:00.708 --> 00:01:03.714
Por exemplo, esta imagem
do conjunto de dados

00:01:03.747 --> 00:01:07.371
tem cinco legendas
como rótulos,

00:01:07.404 --> 00:01:11.228
e este é o conjunto de dados
para treinar um modelo como este.

00:01:11.261 --> 00:01:12.987
Se nós olharmos com atenção,

00:01:13.020 --> 00:01:16.483
o modelo é muito semelhante
aos modelos de sequência que vimos

00:01:16.516 --> 00:01:17.890
nesta aula.

00:01:17.923 --> 00:01:22.331
Neste caso, o modelo usa a imagem
como input do codificador,

00:01:22.364 --> 00:01:25.081
o codificador
gera um contexto,

00:01:25.114 --> 00:01:26.906
passa para o decodificador

00:01:26.939 --> 00:01:30.371
e o decodificador produz
uma legenda como output.

00:01:30.404 --> 00:01:33.116
O modelo gera a legenda
sequencialmente

00:01:33.149 --> 00:01:36.603
e usa a atenção para se concentrar
no local apropriado da imagem,

00:01:36.636 --> 00:01:39.522
enquanto ele gera
cada palavra da legenda.

00:01:39.555 --> 00:01:42.579
Por exemplo,
quando apresentamos esta imagem,

00:01:42.612 --> 00:01:43.827
no primeiro passo,

00:01:43.860 --> 00:01:46.930
o modelo treinado
se concentra nesta região.

00:01:46.963 --> 00:01:49.108
Esta é a miniatura
desta imagem.

00:01:49.868 --> 00:01:54.103
As áreas brancas são onde o modelo
está prestando mais atenção.

00:01:54.136 --> 00:01:57.355
Vemos que se concentra
nas asas.

00:01:57.388 --> 00:01:59.756
Em seguida,
ele exibe o primeiro elemento

00:01:59.789 --> 00:02:01.818
na sequência de output
ou na legenda,

00:02:01.851 --> 00:02:04.131
que é a palavra "um".

00:02:04.164 --> 00:02:06.114
No próximo passo
do decodificador,

00:02:06.147 --> 00:02:08.397
ele se concentra nesta região,

00:02:08.430 --> 00:02:10.789
no corpo do pássaro -
como podemos ver -

00:02:10.822 --> 00:02:13.268
e o output será "pássaro".

00:02:13.301 --> 00:02:16.042
Depois ele expande a área
de concentração

00:02:16.075 --> 00:02:17.850
para a área
em torno do pássaro

00:02:17.883 --> 00:02:20.466
para tentar descobrir
o que descrever em seguida.

00:02:20.499 --> 00:02:24.074
E o output deste passo
será "voando".

00:02:24.107 --> 00:02:26.931
Isso continua.
Podemos ver como a atenção

00:02:26.964 --> 00:02:30.090
começa a irradiar
para fora do pássaro

00:02:30.123 --> 00:02:33.731
e se concentrar nas coisas atrás
ou ao redor dele.

00:02:33.764 --> 00:02:38.587
Então, isso gera "um pássaro
voando sobre um corpo de" -

00:02:38.620 --> 00:02:41.482
e então o foco ignora
completamente o pássaro,

00:02:41.515 --> 00:02:44.972
tentando olhar para qualquer
outro lugar da imagem - "água".

00:02:45.771 --> 00:02:49.425
A primeira vez que vi isso -
a legenda de imagens -,

00:02:49.458 --> 00:02:51.329
foi alucinante,

00:02:51.362 --> 00:02:54.114
mas agora temos uma ideia
de como isso funciona.

00:02:54.147 --> 00:02:57.756
O modelo é feito de um codificador
e um decodificador.

00:02:57.789 --> 00:03:01.025
O codificador, neste caso,
é uma rede neural convolucional,

00:03:01.058 --> 00:03:03.813
que produz um conjunto
de vetores de características,

00:03:03.846 --> 00:03:06.325
cada um deles correspondendo
a uma parte da imagem

00:03:06.358 --> 00:03:08.387
ou a um recurso dela.

00:03:08.420 --> 00:03:13.491
Para ser mais exato, o artigo usou
uma rede convolucional da rede VGG

00:03:13.524 --> 00:03:15.452
treinada na rede de imagens.

00:03:15.485 --> 00:03:19.307
As anotações foram criadas
a partir deste mapa de recursos.

00:03:19.340 --> 00:03:24.523
Este volume de recurso
tem dimensões de 14x14x512,

00:03:24.556 --> 00:03:27.783
o que significa
que tem 512 recursos,

00:03:27.816 --> 00:03:31.835
cada um deles
com as dimensões de 14x14.

00:03:31.868 --> 00:03:34.036
Para criar o vetor de anotação,

00:03:34.069 --> 00:03:36.205
precisamos achatar
cada recurso,

00:03:36.238 --> 00:03:39.220
mudando de 14x14

00:03:39.253 --> 00:03:42.195
para 196x1.

00:03:42.228 --> 00:03:46.124
Então, isso está simplesmente
reformulando a matriz.

00:03:46.157 --> 00:03:50.404
Depois disso, teremos
uma matriz de 196x512.

00:03:50.437 --> 00:03:53.332
Então, temos 512 recursos,

00:03:53.365 --> 00:03:57.684
cada um deles
como um vetor de 196 números.

00:03:58.996 --> 00:04:01.612
Então, esse é o nosso vetor
de contexto.

00:04:01.645 --> 00:04:03.093
Podemos continuar a usá-lo

00:04:03.126 --> 00:04:06.347
como usamos o vetor de contexto
nos vídeos anteriores,

00:04:06.380 --> 00:04:10.046
nos quais pontuamos cada um
dos recursos e depois os fundimos

00:04:10.079 --> 00:04:13.707
para produzir
o vetor de contexto de atenção.

00:04:13.740 --> 00:04:16.896
O decodificador
é uma rede neural recorrente,

00:04:16.929 --> 00:04:20.486
que usa a atenção para se concentrar
no vetor de anotação apropriado

00:04:20.519 --> 00:04:22.067
em cada instante de tempo.

00:04:22.100 --> 00:04:25.340
Nós ligamos isso ao processo
de atenção que mostramos,

00:04:25.373 --> 00:04:28.107
e esse será o nosso modelo
de legendagem de imagens.

00:04:28.140 --> 00:04:29.975
Verifique no texto
abaixo do vídeo

00:04:30.008 --> 00:04:35.224
algumas aplicações interessantes
da atenção na visão computacional.

