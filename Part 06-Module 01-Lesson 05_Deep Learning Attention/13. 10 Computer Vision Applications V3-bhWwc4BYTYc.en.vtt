WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.669
In this concept, we'll go over some of

00:00:02.669 --> 00:00:06.605
the computer vision applications and tasks that attention empowers.

00:00:06.605 --> 00:00:08.134
In the text below the video,

00:00:08.134 --> 00:00:10.800
we'll link to a number of papers in case you want to go

00:00:10.800 --> 00:00:14.054
deeper into any specific application or task.

00:00:14.054 --> 00:00:17.009
In this video, we'll focus on image captioning and

00:00:17.010 --> 00:00:19.695
one of the key papers from 2016 titled,

00:00:19.695 --> 00:00:21.405
"Show, Attend and Tell."

00:00:21.405 --> 00:00:25.020
This paper presented a model that achieved the state of

00:00:25.019 --> 00:00:29.195
the art performance in caption generation in a number of datasets.

00:00:29.195 --> 00:00:32.825
For example, when presented with an image like this,

00:00:32.825 --> 00:00:34.670
the generated caption was,

00:00:34.670 --> 00:00:37.890
"A woman is throwing a frisbee in a park."

00:00:37.890 --> 00:00:40.240
When presented with an image like this,

00:00:40.240 --> 00:00:42.149
the generated caption was,

00:00:42.149 --> 00:00:45.534
"A giraffe standing in a forest with trees in the background."

00:00:45.534 --> 00:00:49.984
Models like these are trained on a dataset like the MS Coco,

00:00:49.984 --> 00:00:52.839
which has a set of about 200,000 images,

00:00:52.840 --> 00:00:55.870
each with five captions written by people.

00:00:55.869 --> 00:01:00.409
This is sourced through something like Amazon's Mechanical Turk Service.

00:01:00.409 --> 00:01:04.265
For example, this image from the dataset has

00:01:04.265 --> 00:01:06.280
these five captions as it's

00:01:06.280 --> 00:01:10.870
labels and this is the dataset that we used to train a model like this.

00:01:10.870 --> 00:01:12.775
If we look under the hood,

00:01:12.775 --> 00:01:14.900
the model is very similar to the sequence to

00:01:14.900 --> 00:01:17.670
sequence models we've looked at earlier in the lesson.

00:01:17.670 --> 00:01:22.134
In this case, the model takes the image as an input to it's encoder,

00:01:22.134 --> 00:01:24.944
the encoder generates a context,

00:01:24.944 --> 00:01:26.750
passes it to the decoder.

00:01:26.750 --> 00:01:30.109
The decoder then proceeds to output a caption.

00:01:30.109 --> 00:01:34.400
The model generates the caption sequentially and uses attention to

00:01:34.400 --> 00:01:39.335
focus on the appropriate place of the image as it generates each word of the caption.

00:01:39.334 --> 00:01:42.489
For example, when presented with this image,

00:01:42.489 --> 00:01:43.859
in the first step,

00:01:43.859 --> 00:01:46.719
the trained model focuses on this region.

00:01:46.719 --> 00:01:49.454
So, this is the thumbnail of this image.

00:01:49.454 --> 00:01:53.875
The white areas is where the model is paying the most attention right now.

00:01:53.875 --> 00:01:57.094
So, we can see that it's mainly focused on the wings.

00:01:57.094 --> 00:02:01.765
It then outputs the first element in the output sequence or the caption,

00:02:01.765 --> 00:02:03.885
which is the word "a."

00:02:03.885 --> 00:02:05.880
The next step at the decoder,

00:02:05.879 --> 00:02:08.375
it focuses on this region,

00:02:08.375 --> 00:02:10.669
so mainly the body of the bird as you see,

00:02:10.669 --> 00:02:13.069
and the output would be "bird",

00:02:13.069 --> 00:02:16.609
and then it expands it's focus area to this area

00:02:16.610 --> 00:02:20.235
around the bird to try to figure out what to describe next,

00:02:20.235 --> 00:02:23.795
and the output at this step is "flying".

00:02:23.794 --> 00:02:26.969
This goes on. We can see how it's attention.

00:02:26.969 --> 00:02:29.990
Right now is starting to radiate out of the bird

00:02:29.990 --> 00:02:33.500
and focus on things behind it or around it.

00:02:33.500 --> 00:02:38.370
So, it's generating "a bird flying over a body of",

00:02:38.370 --> 00:02:41.280
and then the focus here completely sort of ignores the bird,

00:02:41.280 --> 00:02:45.180
and trying to look at everywhere else in the image, "water."

00:02:45.180 --> 00:02:49.495
The first time I looked at something like this, image captioning specifically,

00:02:49.495 --> 00:02:51.289
it was mind-blowing to me,

00:02:51.289 --> 00:02:53.894
but now, we have an idea of how it works.

00:02:53.895 --> 00:02:57.800
The model here is made of an encoder and a decoder as we've mentioned.

00:02:57.800 --> 00:02:59.240
The encoder in this case is

00:02:59.240 --> 00:03:03.800
a convolutional neural network that produces a set of feature vectors,

00:03:03.800 --> 00:03:08.035
each of which corresponds to a part of the image or a feature of the image.

00:03:08.034 --> 00:03:10.835
To be more exact, the paper used

00:03:10.835 --> 00:03:15.254
a VGG net convolutional network trained on the Image net.

00:03:15.254 --> 00:03:19.180
The annotations were created from this feature map.

00:03:19.180 --> 00:03:24.230
This feature volume has dimensions of 14 x 14 x 512,

00:03:24.229 --> 00:03:27.789
meaning that it has 512 features,

00:03:27.789 --> 00:03:31.579
each of them has the dimensions of 14 x 14.

00:03:31.580 --> 00:03:34.090
To create our annotation vector,

00:03:34.090 --> 00:03:36.085
we need to flatten each feature,

00:03:36.085 --> 00:03:41.835
turning it from 14 x 14 to 196 x 1.

00:03:41.835 --> 00:03:45.909
So, this is simply reshaping the matrix.

00:03:45.909 --> 00:03:50.234
After we reshape it to end up with a matrix of 196 x 512.

00:03:50.235 --> 00:03:53.460
So, we have 512 features,

00:03:53.460 --> 00:03:58.205
each one of them is a vector of 196 numbers.

00:03:58.205 --> 00:04:01.500
So, this is our context vector.

00:04:01.500 --> 00:04:06.289
We can proceed to use it just like we've used the context vector in the previous videos,

00:04:06.289 --> 00:04:09.709
where we score each of these features and then we merge

00:04:09.710 --> 00:04:13.525
them to produce our attention context vector.

00:04:13.525 --> 00:04:16.834
The decoder is a recurrent neural network,

00:04:16.834 --> 00:04:21.769
which uses attention to focus on the appropriate annotation vector at each time step.

00:04:21.769 --> 00:04:24.469
We plug this into the attention process we've

00:04:24.470 --> 00:04:27.795
outlined before and that's our image captioning model.

00:04:27.795 --> 00:04:30.140
Be sure to check the text below the video for

00:04:30.139 --> 00:04:35.699
some very exciting applications in computer vision for attention.

