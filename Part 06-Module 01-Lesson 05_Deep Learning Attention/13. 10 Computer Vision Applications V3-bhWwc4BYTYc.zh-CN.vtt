WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.669
在这一部分 我们将介绍运用到注意力机制的

00:00:02.669 --> 00:00:06.605
一些计算机视觉应用和任务

00:00:06.605 --> 00:00:08.134
在视频下方的文本中

00:00:08.134 --> 00:00:10.800
我们给出了多篇论文的链接

00:00:10.800 --> 00:00:14.054
方便你深入了解任何特定应用或任务

00:00:14.054 --> 00:00:17.009
在此视频中 我们将重点讲解图像说明

00:00:17.010 --> 00:00:19.695
并介绍 2016 年的一篇重要论文

00:00:19.695 --> 00:00:21.405
“Show, Attend and Tell.”

00:00:21.405 --> 00:00:25.020
这篇论文中介绍的模型

00:00:25.019 --> 00:00:29.195
在大量数据集中针对说明生成都取得了先进的效果

00:00:29.195 --> 00:00:32.825
例如 对于这张图像

00:00:32.825 --> 00:00:34.670
模型生成的说明是

00:00:34.670 --> 00:00:37.890
“A woman is throwing a frisbee in a park.”

00:00:37.890 --> 00:00:40.240
对于这张图像

00:00:40.240 --> 00:00:42.149
生成的说明是

00:00:42.149 --> 00:00:45.534
“A giraffe standing in a forest with trees in the background.”

00:00:45.534 --> 00:00:49.984
这样的模型是用 MS Coco 等数据集训练的

00:00:49.984 --> 00:00:52.839
该数据集有大约 200,000 张图像

00:00:52.840 --> 00:00:55.870
每个图像都由人工编写了 5 个说明

00:00:55.869 --> 00:01:00.409
这些说明是通过亚马逊的 Mechanical Turk Service 获得的

00:01:00.409 --> 00:01:04.265
例如 数据集中的这张图像

00:01:04.265 --> 00:01:06.280
具有这五个说明标签

00:01:06.280 --> 00:01:10.870
我们使用这个数据集训练这样的模型

00:01:10.870 --> 00:01:12.775
如果研究一下背后原理的话

00:01:12.775 --> 00:01:14.900
会发现该模型和我们之前看到的

00:01:14.900 --> 00:01:17.670
序列到序列模型很相似

00:01:17.670 --> 00:01:22.134
在这种情形下 模型将图像作为编码器的输入

00:01:22.134 --> 00:01:24.944
编码器生成语境

00:01:24.944 --> 00:01:26.750
将语境传递给解码器

00:01:26.750 --> 00:01:30.109
解码器然后做出处理并生成说明

00:01:30.109 --> 00:01:34.400
模型按顺序生成说明

00:01:34.400 --> 00:01:39.335
在生成说明的每个单词时侧重于图像的相应部分

00:01:39.334 --> 00:01:42.489
例如 对于这个图像

00:01:42.489 --> 00:01:43.859
在第一步

00:01:43.859 --> 00:01:46.719
训练模型侧重于这个区域

00:01:46.719 --> 00:01:49.454
这是该图像的缩略图

00:01:49.454 --> 00:01:53.875
模型现在重点查看的是白色区域

00:01:53.875 --> 00:01:57.094
可以看出 它主要关注的是翅膀

00:01:57.094 --> 00:02:01.765
然后输出输出序列（说明）中的第一个元素

00:02:01.765 --> 00:02:03.885
即单词“a”

00:02:03.885 --> 00:02:05.880
解码器的下一步

00:02:05.879 --> 00:02:08.375
侧重于这个区域

00:02:08.375 --> 00:02:10.669
主要是小鸟的身体

00:02:10.669 --> 00:02:13.069
输出单词“bird”

00:02:13.069 --> 00:02:16.609
然后将聚焦区域扩展到小鸟周围的区域

00:02:16.610 --> 00:02:20.235
尝试查看下一步该描述什么

00:02:20.235 --> 00:02:23.795
这一步的输出是“flying”

00:02:23.794 --> 00:02:26.969
一直持续下去

00:02:26.969 --> 00:02:29.990
可以看出注意力现在开始从小鸟身上转移

00:02:29.990 --> 00:02:33.500
并侧重于小鸟背后或周围的事物

00:02:33.500 --> 00:02:38.370
生成了“a bird flying over a body of”

00:02:38.370 --> 00:02:41.280
然后 焦点完全忽略小鸟

00:02:41.280 --> 00:02:45.180
尝试查看图像中的所有剩余区域 生成“water”

00:02:45.180 --> 00:02:49.495
当我第一次看到图像说明这种应用时

00:02:49.495 --> 00:02:51.289
简直被震撼了

00:02:51.289 --> 00:02:53.894
现在知道原理了

00:02:53.895 --> 00:02:57.800
正如之前提到的 这里的模型由编码器和解码器组成

00:02:57.800 --> 00:02:59.240
这里的编码器是一个卷积神经网络

00:02:59.240 --> 00:03:03.800
生成一组特征向量

00:03:03.800 --> 00:03:08.035
每个向量对应于图像的某个部分 即图像的特征

00:03:08.034 --> 00:03:10.835
更准确地说 该论文使用了

00:03:10.835 --> 00:03:15.254
通过 ImageNet 进行训练的 VGG 卷积网络

00:03:15.254 --> 00:03:19.180
注释是用这个特征图创建的

00:03:19.180 --> 00:03:24.230
这个特征图的大小是 14 x 14 x 512

00:03:24.229 --> 00:03:27.789
表明有 512 个特征

00:03:27.789 --> 00:03:31.579
每个特征的大小是 14 x 14

00:03:31.580 --> 00:03:34.090
要创建注释向量

00:03:34.090 --> 00:03:36.085
我们需要扁平化每个特征

00:03:36.085 --> 00:03:41.835
将其从 14 x 14 变成 196 x 1

00:03:41.835 --> 00:03:45.909
也就是调整矩阵的形状

00:03:45.909 --> 00:03:50.234
调整形状后 变成 196 x 512 的矩阵

00:03:50.235 --> 00:03:53.460
有 512 个特征

00:03:53.460 --> 00:03:58.205
每个特征都是有 196 个数字的向量

00:03:58.205 --> 00:04:01.500
这是语境向量

00:04:01.500 --> 00:04:06.289
可以像在之前的视频中一样使用这个语境向量

00:04:06.289 --> 00:04:09.709
对每个特征评分 然后合并结果

00:04:09.710 --> 00:04:13.525
生成注意力语境向量

00:04:13.525 --> 00:04:16.834
解码器是一个循环神经网络

00:04:16.834 --> 00:04:21.769
它使用注意力在每个时间步侧重于相应的注释

00:04:21.769 --> 00:04:24.469
将这个代入之前讲解的注意力流程中

00:04:24.470 --> 00:04:27.795
这就是我们的图像说明模型

00:04:27.795 --> 00:04:30.140
请务必阅读视频下方的文本

00:04:30.139 --> 00:04:35.699
了解注意力在计算机视觉中的一些精彩应用

