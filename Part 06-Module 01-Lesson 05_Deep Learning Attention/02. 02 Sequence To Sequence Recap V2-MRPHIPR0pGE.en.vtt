WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.330
Welcome back. In this video,

00:00:02.330 --> 00:00:05.984
we'll briefly recap how sequence to sequence models work.

00:00:05.985 --> 00:00:11.480
A sequence to sequence model takes in an input that is a sequence of items,

00:00:11.480 --> 00:00:15.675
and then it produces another sequence of items as an output.

00:00:15.675 --> 00:00:18.335
In a machine translation application,

00:00:18.335 --> 00:00:22.385
the input sequence is a series of words in one language,

00:00:22.385 --> 00:00:26.530
and the output is the translation in another language.

00:00:26.530 --> 00:00:31.170
In text summarization, the input is a long sequence of words,

00:00:31.170 --> 00:00:34.310
and the output is a short one.

00:00:34.509 --> 00:00:40.364
A sequence to sequence model usually consists of an encoder and a decoder,

00:00:40.365 --> 00:00:43.940
and it works by the encoder first processing all of

00:00:43.939 --> 00:00:48.769
the inputs turning the inputs into a single representation,

00:00:48.770 --> 00:00:51.255
typically a single vector.

00:00:51.255 --> 00:00:53.535
This is called the context vector,

00:00:53.534 --> 00:00:56.179
and it contains whatever information the encoder

00:00:56.179 --> 00:00:59.144
was able to capture from the input sequence.

00:00:59.145 --> 00:01:04.935
This vector is then sent to the decoder which uses it to formulate an output sequence.

00:01:04.935 --> 00:01:07.560
In machine translation scenarios,

00:01:07.560 --> 00:01:11.585
the encoder and decoder are both recurrent neural networks,

00:01:11.584 --> 00:01:14.464
typically LSTM cells in practice,

00:01:14.465 --> 00:01:16.310
and in this scenario,

00:01:16.310 --> 00:01:19.700
the context vector is a vector of numbers encoding

00:01:19.700 --> 00:01:23.754
the information that the encoder captured from the input sequence.

00:01:23.754 --> 00:01:31.259
In real-world scenarios, this vector can have a length of 256 or 512 or more.

00:01:31.260 --> 00:01:33.430
As a visual representation,

00:01:33.430 --> 00:01:37.700
we'll start showing the hidden states as this vector of length four.

00:01:37.700 --> 00:01:40.219
Just think of the brightness of the cells corresponding to

00:01:40.219 --> 00:01:43.310
how high or low the value of that cell is.

00:01:43.310 --> 00:01:45.939
Let's look at our basic example again,

00:01:45.939 --> 00:01:50.629
but this time we will look at the hidden states of the encoder as they develop.

00:01:50.629 --> 00:01:56.254
The first step, we process the first word and generate a first hidden state.

00:01:56.254 --> 00:02:01.799
Second step, we take the second word in the first hidden state as inputs to the RNN,

00:02:01.799 --> 00:02:04.584
and produce a second hidden state.

00:02:04.584 --> 00:02:06.184
In the third step,

00:02:06.185 --> 00:02:10.935
we process the last word and generate the last hidden state.

00:02:10.935 --> 00:02:16.585
This is the hidden state that would be the context vector will send to the decoder.

00:02:16.585 --> 00:02:21.504
Now, this here is the limitation of sequence to sequence models.

00:02:21.504 --> 00:02:24.379
The encoder is confined to sending

00:02:24.379 --> 00:02:29.000
a single vector no matter how long or short the input sequence is.

00:02:29.000 --> 00:02:32.150
Choosing a reasonable size for this vector

00:02:32.150 --> 00:02:35.634
makes the model have problems with long input sequences.

00:02:35.633 --> 00:02:40.339
Now, one can say, let's just use a very large number of hidden units in the encoder,

00:02:40.340 --> 00:02:43.069
so that the context is very large.

00:02:43.069 --> 00:02:46.930
But then your model overfits with short sequences,

00:02:46.930 --> 00:02:51.349
and you take a performance hit as you increase the number of parameters.

00:02:51.349 --> 00:02:55.269
This is the problem that attention solves.

