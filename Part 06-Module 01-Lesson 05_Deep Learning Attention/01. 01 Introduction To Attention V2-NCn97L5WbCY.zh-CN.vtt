WEBVTT
Kind: captions
Language: zh-CN

00:00:06.049 --> 00:00:09.464
大家好 我叫 Jay

00:00:09.464 --> 00:00:10.769
在这节课 我们将讨论

00:00:10.769 --> 00:00:15.394
在过去几年内深度学习领域最重要的创新之一 即注意力

00:00:15.394 --> 00:00:17.370
注意力一开始出现在计算机视觉领域

00:00:17.370 --> 00:00:20.880
用来尝试模仿人类感知力

00:00:20.879 --> 00:00:24.949
这段话摘自 2014 年的一篇视觉注意力论文

00:00:24.949 --> 00:00:28.699
“人类感知力的一个重要特性是”

00:00:28.699 --> 00:00:32.589
“我们不需要同时处理整个场景”

00:00:32.590 --> 00:00:36.590
“相反 我们选择性地将注意力集中在视觉空间的某些部分”

00:00:36.590 --> 00:00:40.580
“以便在合适的时间和地点获取信息”

00:00:40.579 --> 00:00:43.820
“然后将一段时间内来自不同视点的信息相结合”

00:00:43.820 --> 00:00:47.780
“在大脑内对整个场景构建表示结果”

00:00:47.780 --> 00:00:50.789
“为后续眼球运动和决策制定提供指导”

00:00:50.789 --> 00:00:55.089
这段话的含义是 当我们在日常生活中看到某个场景时

00:00:55.090 --> 00:00:59.345
我们的大脑并不是一次性处理一个视觉快照

00:00:59.344 --> 00:01:03.500
而是选择性地侧重于图像的不同部分

00:01:03.500 --> 00:01:07.299
并且逐渐按顺序收集和处理这些视觉信息

00:01:07.299 --> 00:01:09.799
例如 你在商场中购物

00:01:09.799 --> 00:01:14.189
我们将看到的视频录像是由眼球追踪设备拍摄的

00:01:14.189 --> 00:01:15.659
如果你没见过这种设备的话 解释下

00:01:15.659 --> 00:01:18.259
它是一种同时录制眼前景象

00:01:18.260 --> 00:01:21.150
和眼球运动的设备

00:01:21.150 --> 00:01:24.410
我们可以将这些视频相结合

00:01:24.409 --> 00:01:27.974
判断出在视频中的每个时刻 你看向什么位置

00:01:27.974 --> 00:01:33.179
我们看到的这段录像是佩戴眼球追踪设备的人录制的片段

00:01:33.180 --> 00:01:37.790
橘色圆圈表示每时每刻此人看到的位置

00:01:37.790 --> 00:01:41.820
我们可以在普通的视觉感知中发现注意力

00:01:41.819 --> 00:01:46.934
还可以在阅读中发现注意力 即每次尝试处理文本中的一个单词

00:01:46.935 --> 00:01:48.969
这种设备可以用于

00:01:48.969 --> 00:01:51.340
用户体验测试

00:01:51.340 --> 00:01:55.730
如果你想为应用或网站构建用户界面

00:01:55.730 --> 00:01:57.950
并且想检测最重要的元素

00:01:57.950 --> 00:02:00.579
是否吸引了用户的注意力

00:02:00.579 --> 00:02:06.439
因为这是人类在一段时间内按顺序理解视觉图片的方式

00:02:06.439 --> 00:02:09.409
计算机视觉研究人员希望找到一种方法

00:02:09.409 --> 00:02:13.104
使计算机视觉模型能够这么操作

00:02:13.104 --> 00:02:16.114
在机器学习中 注意力方法

00:02:16.115 --> 00:02:20.080
使我们能够向机器学习模型中添加选择性焦点

00:02:20.080 --> 00:02:23.830
通常是按序列进行处理

00:02:23.830 --> 00:02:26.330
注意力为某些性能最佳的模型提供了支持

00:02:26.330 --> 00:02:31.820
包括自然语言处理和计算机视觉模型

00:02:31.819 --> 00:02:35.424
这些模型包括神经机器翻译

00:02:35.425 --> 00:02:38.189
图像说明 语音识别

00:02:38.189 --> 00:02:41.319
和文本摘要等等

00:02:41.319 --> 00:02:45.104
举个图像说明的例子

00:02:45.104 --> 00:02:47.049
在使用注意力技巧之前

00:02:47.050 --> 00:02:49.955
卷积神经网络能够通过查看整个图像

00:02:49.955 --> 00:02:54.300
对图像进行分类并输出分类标签

00:02:54.300 --> 00:02:58.515
但是为了生成分类标签 并非整个图像都有必要查看

00:02:58.514 --> 00:03:02.389
只需查看部分像素就能判断这是个鸟类图像

00:03:02.389 --> 00:03:07.319
这时候 注意力就派上用场了 可以处理这些最重要的像素

00:03:07.319 --> 00:03:09.409
此外

00:03:09.409 --> 00:03:13.039
注意力还提高了

00:03:13.039 --> 00:03:15.259
我们使用完整句子描述图像的能力

00:03:15.259 --> 00:03:19.194
因为我们在生成句子时 会侧重于图像的不同部分

00:03:19.194 --> 00:03:22.669
注意力变得火热起来

00:03:22.669 --> 00:03:26.894
它在神经机器翻译等任务中变得非常实用

00:03:26.895 --> 00:03:31.520
但是 随着序列到序列模型开始取得显著成果

00:03:31.520 --> 00:03:33.950
它们受到了一些限制

00:03:33.949 --> 00:03:37.439
导致难以处理长句子等

00:03:37.439 --> 00:03:40.699
没有注意力的传统序列到序列模型

00:03:40.699 --> 00:03:44.719
需要一次性查看要翻译的原始句子

00:03:44.719 --> 00:03:50.009
然后利用整个输入信息生成所有单个小的输出结果

00:03:50.009 --> 00:03:53.419
但是 注意力使模型能够查看

00:03:53.419 --> 00:03:58.834
输入的相对小部分并逐渐生成输出结果

00:03:58.835 --> 00:04:03.110
当注意力被引入序列到序列模型后

00:04:03.110 --> 00:04:06.540
这些模型成为了先进的神经机器翻译模型

00:04:06.539 --> 00:04:10.534
因此 在 2016 年末 Google 将具有注意力的神经机器翻译

00:04:10.534 --> 00:04:15.685
用作 Google 翻译的翻译引擎

00:04:15.685 --> 00:04:20.670
在这节课 我们将了解注意力的原理 以及如何及在何处应用注意力

