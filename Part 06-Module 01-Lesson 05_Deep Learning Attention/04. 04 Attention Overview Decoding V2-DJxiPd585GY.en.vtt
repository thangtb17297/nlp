WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.650
Now, let's look at the attention decoder and how it works at a very high level.

00:00:04.650 --> 00:00:06.570
At every time step,

00:00:06.570 --> 00:00:08.880
an attention decoder pays attention to

00:00:08.880 --> 00:00:13.005
the appropriate part of the input sequence using the context factor.

00:00:13.005 --> 00:00:15.810
How does the attention decoder know which of the parts

00:00:15.810 --> 00:00:18.589
of the input sequence to focus on at each step?

00:00:18.589 --> 00:00:21.870
That process is learned during the training phase,

00:00:21.870 --> 00:00:26.700
and it's not just stupidly going sequentially from the first and the second to the third.

00:00:26.699 --> 00:00:29.814
It can learn some sophisticated behavior.

00:00:29.815 --> 00:00:36.789
Let's look at this example of translating a French sentence to an English one.

00:00:36.789 --> 00:00:41.000
So let's say we have this input sentence in French.

00:00:41.000 --> 00:00:44.645
Let's say we pass this to our encoder

00:00:44.645 --> 00:00:50.020
and now we're ready to look at each step in the decoding phase.

00:00:50.020 --> 00:00:51.870
In the first step,

00:00:51.869 --> 00:00:57.349
the attention decoder would pay attention to the first part of the sentence.

00:00:57.350 --> 00:00:59.395
This is a trained model, right.

00:00:59.395 --> 00:01:02.690
So the more light the square is is the more attention that he

00:01:02.689 --> 00:01:06.424
gave to that word in particular.

00:01:06.424 --> 00:01:12.120
So it pays attention to the first word and it outputs a first English word.

00:01:12.120 --> 00:01:14.135
In the second step,

00:01:14.135 --> 00:01:15.920
it pays attention to the second word in

00:01:15.920 --> 00:01:21.450
the input sequence and translates that word as well.

00:01:21.450 --> 00:01:25.460
It goes on sequentially for about four steps

00:01:25.459 --> 00:01:30.349
and it produces reasonable English translation so far.

00:01:30.349 --> 00:01:34.004
Then something different happens here in the fifth step.

00:01:34.004 --> 00:01:37.814
So, when we're generating the fifth word of the output,

00:01:37.814 --> 00:01:43.855
the attention actually jumped two words to translate European.

00:01:43.855 --> 00:01:46.219
So, we have zone, economique, europeenne,

00:01:46.219 --> 00:01:49.170
so on the English side it's not going to be in the same order.

00:01:49.170 --> 00:01:52.100
So, europeenne is translated as

00:01:52.099 --> 00:01:56.569
European and then in the next step it focuses on the word before that,

00:01:56.569 --> 00:02:03.159
economique, economic, and it focuses on zone and it outputs area.

00:02:03.159 --> 00:02:09.650
This is a case where the order of these words in the French language does not follow how

00:02:09.650 --> 00:02:10.849
it would be ordered in

00:02:10.849 --> 00:02:16.055
the English language and the model was able to learn that just from a training data set.

00:02:16.055 --> 00:02:19.450
The rest of the sentence goes on pretty much sequentially.

00:02:19.449 --> 00:02:23.839
So, this is a really cool example of how attention is able to make

00:02:23.840 --> 00:02:30.069
these models focus on the right parts at the right moments based on what dataset we have.

