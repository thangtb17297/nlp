WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.445
In this video, we'll look at the third commonly used scoring method.

00:00:05.445 --> 00:00:12.025
It's called concat, and the way to do it is to use a feedforward neural network.

00:00:12.025 --> 00:00:13.580
To take a simple example,

00:00:13.580 --> 00:00:17.859
let's say we're scoring this encoder hidden state,

00:00:17.859 --> 00:00:20.219
at the fourth time step at the decoder.

00:00:20.219 --> 00:00:24.629
Again this is an oversimplified example scoring only one,

00:00:24.629 --> 00:00:29.140
while in practice we'll actually do a matrix and do it all discord in one step.

00:00:29.140 --> 00:00:31.795
The concat scoring method,

00:00:31.795 --> 00:00:35.070
is commonly done by concatenating the two vectors,

00:00:35.070 --> 00:00:38.880
and making that the input to a feed forward neural network.

00:00:38.880 --> 00:00:41.740
Let's see how that works. So, we merge them,

00:00:41.740 --> 00:00:44.380
we concat them into one vector,

00:00:44.380 --> 00:00:47.830
and then we pass them through a neural network.

00:00:47.829 --> 00:00:50.239
This network has a single hidden layer,

00:00:50.240 --> 00:00:53.469
and outputs this score.

00:00:53.469 --> 00:00:55.549
The parameters of this network,

00:00:55.549 --> 00:00:57.629
are learned during the training process.

00:00:57.630 --> 00:01:00.890
Namely the WA weights matrix,

00:01:00.890 --> 00:01:03.344
and the VA weights matrix.

00:01:03.344 --> 00:01:06.060
To look at how the calculation is done,

00:01:06.060 --> 00:01:08.109
this is our concatenated vector,

00:01:08.109 --> 00:01:11.450
we simply multiply it by W of A,

00:01:11.450 --> 00:01:17.990
we apply 10H activation producing this two by one matrix.

00:01:17.989 --> 00:01:22.399
We multiply that by the V of A weights matrix,

00:01:22.400 --> 00:01:26.315
and we get the score for this encoder hidden state.

00:01:26.314 --> 00:01:29.329
Formally, it is expressed like this,

00:01:29.329 --> 00:01:34.280
where H of T as we've mentioned is the hidden state at the current time step,

00:01:34.280 --> 00:01:39.924
and H of S is the collection of the set of encoder hidden states.

00:01:39.924 --> 00:01:43.879
This is the concatenation and then we multiply it by W of A,

00:01:43.879 --> 00:01:48.914
tan H activation and then multiply it by V of A transpose.

00:01:48.915 --> 00:01:51.359
One thing to note is the difference.

00:01:51.359 --> 00:01:56.935
So concat is very similar to the scoring method from Mark Daniel paper,

00:01:56.935 --> 00:01:58.680
but this is the one,

00:01:58.680 --> 00:02:01.190
this is the concat method from the Lung paper,

00:02:01.189 --> 00:02:03.834
where there's only one weight matrix.

00:02:03.834 --> 00:02:08.039
In the Mark Daniel paper there are two major differences that we can look at.

00:02:08.039 --> 00:02:11.275
One of them is that the weights matrix is split into two,

00:02:11.275 --> 00:02:12.539
so we don't have just W of A,

00:02:12.539 --> 00:02:14.405
we have W of A and U of A,

00:02:14.405 --> 00:02:18.060
and each is applied to the respective vector.

00:02:18.060 --> 00:02:20.569
The decoder hidden state in this case,

00:02:20.569 --> 00:02:23.314
and the encoder hidden state at this case.

00:02:23.314 --> 00:02:26.944
Another thing to note is that the Mark Daniel paper used

00:02:26.944 --> 00:02:31.685
the hidden state from the previous time step at the decoder.

00:02:31.685 --> 00:02:37.699
While in the loop paper it uses the one from the current time step at the decoder.

00:02:37.699 --> 00:02:42.179
Let's make a note on notation here,

00:02:42.180 --> 00:02:45.319
in case you're planning to read the papers.

00:02:45.319 --> 00:02:49.844
Here we've used the notation mainly from the Lung paper where

00:02:49.844 --> 00:02:53.895
we referred to the encoder and the decoder hidden states as H. So,

00:02:53.895 --> 00:02:56.055
H of T for the decoder,

00:02:56.055 --> 00:02:58.610
and H of S for the encoder.

00:02:58.610 --> 00:03:02.945
This means so H is for hidden state,T is for target,

00:03:02.944 --> 00:03:04.759
so that's the target sequence that we're going to

00:03:04.759 --> 00:03:06.935
output so that's associated with the decoder.

00:03:06.935 --> 00:03:09.129
S is for source.

00:03:09.129 --> 00:03:11.245
In the Mark Daniel paper,

00:03:11.245 --> 00:03:13.599
this is called S. So,

00:03:13.599 --> 00:03:16.780
it is not H, it's called S. So,

00:03:16.780 --> 00:03:18.224
now the picture is complete.

00:03:18.224 --> 00:03:21.745
Now, we've gone over the entire attention process.

00:03:21.745 --> 00:03:26.009
It's time now to look at some applications in the next video.

