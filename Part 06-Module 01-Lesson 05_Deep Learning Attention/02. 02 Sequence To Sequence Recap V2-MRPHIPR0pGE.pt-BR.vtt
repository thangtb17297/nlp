WEBVTT
Kind: captions
Language: pt-BR

00:00:00.247 --> 00:00:01.359
Bem-vindo de volta!

00:00:01.392 --> 00:00:03.463
Neste vídeo, vamos recapitular

00:00:03.496 --> 00:00:06.096
como funcionam
os modelos Sequence to Sequence.

00:00:06.129 --> 00:00:09.156
O modelo Sequence to Sequence
recebe um input

00:00:09.189 --> 00:00:11.567
que é uma sequência de itens.

00:00:11.600 --> 00:00:15.556
Depois produz outra sequência
de itens como output.

00:00:16.035 --> 00:00:18.299
Num aplicativo
de tradução automática,

00:00:18.332 --> 00:00:22.482
a sequência de input é
uma série de palavras em um idioma,

00:00:22.515 --> 00:00:26.723
e o output é a tradução
em outro idioma.

00:00:26.756 --> 00:00:28.402
Na sumarização automática,

00:00:28.435 --> 00:00:31.058
o input é
uma longa sequência de palavras,

00:00:31.091 --> 00:00:34.016
e o output é
uma sequência curta.

00:00:35.248 --> 00:00:37.809
O modelo Sequence to Sequence
costuma consistir

00:00:37.842 --> 00:00:40.642
de um codificador
e um decodificador,

00:00:40.675 --> 00:00:45.113
e funciona fazendo o codificador
processar todos os inputs,

00:00:45.146 --> 00:00:48.808
transformando os inputs
em uma única representação,

00:00:48.841 --> 00:00:51.527
que tipicamente é
um único vetor.

00:00:51.560 --> 00:00:53.428
Ele se chama
"vetor de contexto"

00:00:53.461 --> 00:00:57.265
e contém todas as informações
que o codificador conseguiu captar

00:00:57.298 --> 00:00:59.113
da sequência de input.

00:00:59.146 --> 00:01:01.613
Em seguida, esse vetor
é enviado ao decodificador,

00:01:01.646 --> 00:01:04.463
que o utiliza para formular
uma sequência de output.

00:01:05.247 --> 00:01:07.555
No âmbito
da tradução automática,

00:01:07.588 --> 00:01:11.523
o codificador e o decodificador são
redes neurais recorrentes -

00:01:11.556 --> 00:01:14.091
na prática, células LSTM.

00:01:14.732 --> 00:01:17.752
Nessa situação,
o vetor de contexto

00:01:17.785 --> 00:01:20.300
é um vetor de números
contendo as informações

00:01:20.333 --> 00:01:23.943
que o codificador captou
da sequência de input.

00:01:23.976 --> 00:01:25.444
Em situações do mundo real,

00:01:25.477 --> 00:01:30.978
esse vetor pode ter um comprimento
de 256, 512 ou mais.

00:01:31.638 --> 00:01:33.332
Como representação visual,

00:01:33.365 --> 00:01:35.465
começaremos a mostrar
os estados ocultos

00:01:35.498 --> 00:01:37.717
como este vetor
de comprimento igual a 4.

00:01:37.750 --> 00:01:39.425
Pense que o brilho das células

00:01:39.458 --> 00:01:43.342
corresponde a quão alto ou baixo
é o valor das células.

00:01:43.375 --> 00:01:46.038
Vamos rever
o nosso exemplo básico.

00:01:46.071 --> 00:01:48.979
Mas, desta vez, veremos
os estados ocultos do codificador

00:01:49.012 --> 00:01:50.803
conforme se desenvolvem.

00:01:50.836 --> 00:01:53.583
O 1º passo é
processar a 1ª palavra

00:01:53.616 --> 00:01:56.372
e gerar o 1º estado oculto.

00:01:56.405 --> 00:01:59.063
O 2º passo é
pegar a 2ª palavra

00:01:59.096 --> 00:02:01.789
e o 1º estado oculto
como inputs da RNN,

00:02:01.822 --> 00:02:04.692
produzindo o 2º estado oculto.

00:02:04.725 --> 00:02:08.178
No 3º passo,
processamos a última palavra

00:02:08.211 --> 00:02:11.201
e geramos
o último estado oculto.

00:02:11.234 --> 00:02:14.534
Esse é o estado oculto
que será o vetor de contexto

00:02:14.567 --> 00:02:16.673
que enviaremos
ao decodificador.

00:02:16.706 --> 00:02:21.043
Agora, esta é justamente a limitação
dos modelos Sequence to Sequence.

00:02:21.919 --> 00:02:25.619
O codificador se limita a enviar
um único vetor,

00:02:25.652 --> 00:02:29.132
por mais que a sequência de input
seja longa ou curta.

00:02:29.165 --> 00:02:32.000
Escolher um tamanho adequado
para esse vetor

00:02:32.033 --> 00:02:35.605
faz com que o modelo tenha problemas
com longas sequências de input.

00:02:35.638 --> 00:02:40.200
Poderíamos usar um número grande
de unidades ocultas no codificador,

00:02:40.233 --> 00:02:43.393
de modo que o contexto
seja bem grande,

00:02:43.426 --> 00:02:47.039
mas o modelo geraria sobreajuste
com sequências curtas,

00:02:47.072 --> 00:02:51.335
e seu desempenho pioraria conforme
elevássemos o número de parâmetros.

00:02:51.368 --> 00:02:54.567
Esse é o problema
que a atenção resolve.

