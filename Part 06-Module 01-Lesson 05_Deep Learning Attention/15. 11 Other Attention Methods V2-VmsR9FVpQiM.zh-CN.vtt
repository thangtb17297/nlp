WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:05.230
因为这两篇主要论文是在 2014 和 2015 年发表的

00:00:05.230 --> 00:00:09.255
注意力一直是个比较活跃的研究领域 出现了很多新的发展

00:00:09.255 --> 00:00:12.600
虽然这两个机制一直都是常用机制

00:00:12.599 --> 00:00:15.834
但是这几年出现了重大发展

00:00:15.835 --> 00:00:18.179
在这个视频中 我们将讲解其中一篇论文

00:00:18.179 --> 00:00:22.739
叫做“Attention Is All You Need”

00:00:22.739 --> 00:00:28.559
这篇论文指出 具有注意力机制的编码器-解码器模型的复杂性

00:00:28.559 --> 00:00:34.984
可以通过采用一种新的模型来简化 这种模型仅使用注意力 不使用 RNN

00:00:34.984 --> 00:00:38.225
他们将这种新模型称为 Transformer

00:00:38.225 --> 00:00:41.685
在两项机器翻译任务实验中

00:00:41.685 --> 00:00:44.285
该模型都呈现出极高的质量

00:00:44.284 --> 00:00:47.329
并且训练时间大大缩减

00:00:47.329 --> 00:00:51.320
Transformer 将序列作为输入并生成序列

00:00:51.320 --> 00:00:54.405
和我们到目前为止见到的序列-序列模型一样

00:00:54.405 --> 00:00:56.359
但是区别在于

00:00:56.359 --> 00:00:58.479
它并不像 RNN 那样

00:00:58.479 --> 00:01:00.254
逐个地获取输入

00:01:00.255 --> 00:01:03.395
它可以并行地生成所有输入

00:01:03.395 --> 00:01:07.564
如果可以的话 可以用单独的 GPU 处理每个元素

00:01:07.564 --> 00:01:12.500
然后逐个地生成输出 也不使用 RNN

00:01:12.500 --> 00:01:17.359
Transformer 模型也由编码器和解码器组成

00:01:17.359 --> 00:01:18.840
但使用的不是 RNN

00:01:18.840 --> 00:01:23.950
而是前馈神经网络和自注意力机制

00:01:23.950 --> 00:01:28.740
这种组合使编码器和解码器不需要使用 RNN

00:01:28.739 --> 00:01:31.489
这样可以显著改善性能

00:01:31.489 --> 00:01:35.759
因为可以并行处理 而这一点 RNN 却做不到

00:01:35.760 --> 00:01:41.120
Transformer 包含一组相同的编码器和解码器

00:01:41.120 --> 00:01:43.683
这篇论文推荐 6 对

00:01:43.683 --> 00:01:47.670
我们重点看看包含一个层级的编码器并仔细研究下

00:01:47.670 --> 00:01:51.695
每个编码器层级包含两个子层级

00:01:51.694 --> 00:01:56.224
一个多头自注意力层级和一个前馈层级

00:01:56.224 --> 00:01:57.679
你可能注意到了

00:01:57.680 --> 00:02:01.790
这个注意力组件完全位于编码器一端

00:02:01.790 --> 00:02:06.830
而不像之前的注意力机制那样 在解码器端也有

00:02:06.829 --> 00:02:11.764
这个注意力组件通过侧重于与要处理的每个输入元素

00:02:11.764 --> 00:02:17.449
相关的输入序列的其他部分 帮助编码器理解输入

00:02:17.449 --> 00:02:20.869
这种理念是对自注意力概念雏形的延伸

00:02:20.870 --> 00:02:24.879
有助于理解输入

00:02:24.879 --> 00:02:27.294
例如 在一篇论文中

00:02:27.294 --> 00:02:31.679
这种注意力用在了机器阅读中

00:02:31.680 --> 00:02:36.025
这种技巧的实验结果与

00:02:36.025 --> 00:02:40.824
当时针对某些任务的最先进技术的效果相当或表现更好

00:02:40.824 --> 00:02:44.825
这些任务包括语言建模 情感分析和自然语言推理

00:02:44.824 --> 00:02:47.929
他们依然使用了 RNN

00:02:47.930 --> 00:02:51.855
但是利用这种概念进行了增强 后来发展成自注意力

00:02:51.854 --> 00:02:54.530
他们在这个机器阅读论文中使用的示例

00:02:54.530 --> 00:03:00.215
展示了训练模型在阅读每个单词时 将注意力集中在何处

00:03:00.215 --> 00:03:05.004
例如 当模型使用 LSTM 阅读这个句子时

00:03:05.004 --> 00:03:07.819
它学会了在处理输入的每个单词时

00:03:07.819 --> 00:03:11.104
应该注意输入的哪些其他部分

00:03:11.104 --> 00:03:14.179
红色表示正在阅读的位置

00:03:14.180 --> 00:03:17.640
蓝色表示阅读这个单词时注意力集中的位置

00:03:17.639 --> 00:03:21.500
在每一步 它都阅读一个单词

00:03:21.500 --> 00:03:26.259
并注意到有助于理解该单词的前面相关单词

00:03:26.259 --> 00:03:29.564
但是 Transformer 的结构

00:03:29.564 --> 00:03:34.189
使编码器不仅能够侧重于输入序列中之前的单词

00:03:34.189 --> 00:03:37.664
而且能够注意到之后出现的单词

00:03:37.664 --> 00:03:42.150
但是这个并不是

00:03:42.150 --> 00:03:45.885
解码器包含两个注意力组件

00:03:45.884 --> 00:03:49.280
一个使其能够侧重于输入的相关部分

00:03:49.280 --> 00:03:54.860
另一个仅注意到之前的解码器输出

00:03:54.860 --> 00:03:56.480
就这样

00:03:56.479 --> 00:03:59.924
这就是 Transformer 组件的概览

00:03:59.925 --> 00:04:03.700
可以看出这个模型大量使用了注意力机制

00:04:03.699 --> 00:04:06.339
这里有三个注意力组件

00:04:06.340 --> 00:04:09.049
它们并非行为完全一样

00:04:09.049 --> 00:04:12.400
但是差不多都是乘法注意力

00:04:12.400 --> 00:04:14.719
我们已经讲过这种注意力

