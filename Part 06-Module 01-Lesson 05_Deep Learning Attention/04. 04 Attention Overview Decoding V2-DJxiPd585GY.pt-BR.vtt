WEBVTT
Kind: captions
Language: pt-BR

00:00:00.060 --> 00:00:04.881
Vamos ver o decodificador de atenção
e como ele funciona de modo geral.

00:00:04.914 --> 00:00:07.660
A cada instante de tempo,
um decodificador de atenção

00:00:07.693 --> 00:00:11.015
dá atenção à parte adequada
da sequência de input

00:00:11.048 --> 00:00:13.142
usando o fator contexto.

00:00:13.175 --> 00:00:14.941
Como o decodificador de atenção
sabe

00:00:14.974 --> 00:00:18.578
em que parte da sequência de input
deve se concentrar em cada passo?

00:00:18.611 --> 00:00:21.611
Esse processo é aprendido
durante a fase do treinamento

00:00:21.644 --> 00:00:26.692
e não se resume a avançar
da 1ª sequência à 2ª e à 3ª.

00:00:26.725 --> 00:00:29.382
Ele é capaz de aprender
comportamentos sofisticados.

00:00:30.390 --> 00:00:35.349
Vejamos este exemplo,
que traduz uma frase em francês

00:00:35.382 --> 00:00:37.063
para o inglês.

00:00:37.096 --> 00:00:41.006
Vamos supor que temos
esta frase de input em francês.

00:00:41.039 --> 00:00:45.103
Vamos passá-la
para o codificador

00:00:45.136 --> 00:00:48.320
e agora estamos prontos
para analisar cada passo

00:00:48.353 --> 00:00:49.988
da fase de decodificação.

00:00:50.021 --> 00:00:51.646
No 1º passo,

00:00:51.679 --> 00:00:57.367
o decodificador de atenção
dá atenção à 1ª parte da frase.

00:00:57.400 --> 00:00:59.297
Este é um modelo treinado,

00:00:59.330 --> 00:01:01.379
então, quanto mais claro
for o quadrado,

00:01:01.412 --> 00:01:06.384
mais atenção ele deu
a essa palavra específica.

00:01:06.417 --> 00:01:11.681
Então ele dá atenção à 1ª palavra
e produz a 1ª palavra em inglês.

00:01:12.586 --> 00:01:17.610
No 2º passo, ele dá atenção
à 2ª palavra da sequência de input

00:01:17.643 --> 00:01:21.704
e traduz essa palavra.

00:01:21.737 --> 00:01:25.196
O modelo repete esse processo
por cerca de quatro passos.

00:01:25.896 --> 00:01:30.825
E produz traduções em inglês
aceitáveis, por enquanto.

00:01:30.858 --> 00:01:34.018
Até que acontece algo diferente
no 5º passo.

00:01:34.051 --> 00:01:37.991
Quando geramos
a 5ª palavra de output,

00:01:38.024 --> 00:01:43.486
a atenção pulou duas palavras
para traduzir "européenne".

00:01:44.052 --> 00:01:46.789
As palavras "zone",
"économique" e "européenne"

00:01:46.822 --> 00:01:50.444
não seguem a mesma ordem
no inglês.

00:01:50.477 --> 00:01:53.590
"Européenne" é traduzida
como "European".

00:01:53.623 --> 00:01:57.911
No passo seguinte, o modelo traduz
a palavra anterior, "économique",

00:01:57.944 --> 00:01:59.255
ou "Economic".

00:01:59.288 --> 00:02:03.335
Depois se concentra em "zone"
e produz "Area".

00:02:03.368 --> 00:02:07.648
Este é um caso em que a ordem
das palavras no francês

00:02:07.681 --> 00:02:12.524
não é a mesma no inglês.

00:02:12.557 --> 00:02:16.138
E o modelo conseguiu aprender isso
a partir do conjunto de treinamento.

00:02:16.171 --> 00:02:19.602
O resto da frase continua
na sequência normal.

00:02:19.635 --> 00:02:21.879
Então este é um ótimo exemplo

00:02:21.912 --> 00:02:24.322
de como a atenção consegue fazer
com que o modelo

00:02:24.355 --> 00:02:26.978
se concentre nas partes certas
nos momentos certos

00:02:27.011 --> 00:02:29.396
com base no conjunto de dados
que nós temos.

