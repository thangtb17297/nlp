WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.705
在这节课的早些时候

00:00:01.705 --> 00:00:07.960
我们了解了注意力的关键概念是计算注意力权重向量

00:00:07.960 --> 00:00:10.740
用来放大来自输入序列

00:00:10.740 --> 00:00:14.820
最相关部分的信号

00:00:14.820 --> 00:00:17.329
同时忽略不相关部分

00:00:17.329 --> 00:00:20.070
在这个视频中

00:00:20.070 --> 00:00:24.125
我们将讲解生成这些注意力权重的评分函数

00:00:24.125 --> 00:00:28.649
注意力评分函数会接受编码器的隐藏状态

00:00:28.649 --> 00:00:34.424
以及解码器的一组隐藏状态

00:00:34.424 --> 00:00:40.114
因为对于解码器来说 我们将在每个时间步执行这一计算

00:00:40.115 --> 00:00:43.130
因此仅使用该时间步的解码器隐藏状态

00:00:43.130 --> 00:00:47.870
或者在某些评分函数中使用上个时间步的解码器隐藏状态

00:00:47.869 --> 00:00:49.789
给定这两个输入

00:00:49.789 --> 00:00:52.585
这个向量和这个矩阵

00:00:52.585 --> 00:00:57.529
它会生成对每列评分的向量

00:00:57.529 --> 00:01:00.020
在查看矩阵版本之前

00:01:00.020 --> 00:01:04.375
即计算一个时间步所有编码器隐藏状态的得分

00:01:04.375 --> 00:01:09.734
我们先简化下 了解如何对单个编码器隐藏状态评分

00:01:09.734 --> 00:01:13.670
第一个评分函数 也是最简单的函数

00:01:13.670 --> 00:01:17.540
将计算这两个输入向量的点积

00:01:17.540 --> 00:01:23.705
两个向量的点积生成一个数字 不错

00:01:23.704 --> 00:01:27.969
但是重要的是这个数字的显著性

00:01:27.969 --> 00:01:32.450
从几何角度来讲 两个向量的点积

00:01:32.450 --> 00:01:37.520
等于将两个向量的长度乘以夹角的余弦

00:01:37.519 --> 00:01:39.799
我们知道余弦有个很方便的特性

00:01:39.799 --> 00:01:46.250
如果夹角为 0 则余弦为 1

00:01:46.250 --> 00:01:48.474
夹角越大 余弦越小

00:01:48.474 --> 00:01:53.899
意味着 如果两个向量的长度一样

00:01:53.900 --> 00:01:56.469
夹角越小

00:01:56.469 --> 00:01:59.609
点积越大

00:01:59.609 --> 00:02:04.825
这个点积衡量的是两个向量的相似性

00:02:04.825 --> 00:02:07.659
如果点积越大

00:02:07.659 --> 00:02:10.585
则表明向量夹角越小

00:02:10.585 --> 00:02:14.210
但是 在实际操作中 我们希望通过

00:02:14.210 --> 00:02:17.735
一次性对所有编码器隐藏状态评分加快计算流程

00:02:17.735 --> 00:02:23.060
这就得出了点积注意力的正式数学定义

00:02:23.060 --> 00:02:25.125
即这一部分

00:02:25.125 --> 00:02:29.405
它是当前时间步的隐藏状态转置后

00:02:29.405 --> 00:02:35.569
乘以编码器隐藏时间步的矩阵

00:02:35.569 --> 00:02:41.324
看起来是这样的 生成得分向量

00:02:41.324 --> 00:02:45.079
简化这个方法就会导致一个缺点

00:02:45.080 --> 00:02:48.965
即假设编码器和解码器具有相同的嵌入空间

00:02:48.965 --> 00:02:52.944
虽然这个方法适用于文本摘要

00:02:52.944 --> 00:02:58.430
其中编码器和解码器使用相同的语言和嵌入空间

00:02:58.430 --> 00:03:00.705
但是对于机器翻译来说

00:03:00.705 --> 00:03:05.460
你会发现每种语言都有自己的嵌入空间

00:03:05.460 --> 00:03:10.435
这时候可能就要用到第二种评分函数了

00:03:10.435 --> 00:03:13.284
它对第一种函数稍加修改

00:03:13.284 --> 00:03:17.854
将编码器隐藏状态和

00:03:17.854 --> 00:03:23.199
解码器隐藏状态相乘 引入权重矩阵

00:03:23.199 --> 00:03:28.609
这个权重矩阵是线性转换 允许输入和输出使用不同的嵌入

00:03:28.610 --> 00:03:34.790
这个乘法的结果是权重向量

00:03:34.789 --> 00:03:37.774
我们再回到这个动画

00:03:37.774 --> 00:03:41.275
并包含关于注意力的所有知识

00:03:41.275 --> 00:03:47.150
注意力解码器的第一个时间步

00:03:47.150 --> 00:03:53.469
首先获得初始隐藏状态和结束符号的嵌入

00:03:53.469 --> 00:03:59.840
它进行计算并生成该时间步的隐藏状态

00:03:59.840 --> 00:04:02.284
这里 我们将忽略 RNN 的实际输出

00:04:02.284 --> 00:04:04.409
只使用隐藏状态

00:04:04.409 --> 00:04:07.435
然后是注意力步骤

00:04:07.435 --> 00:04:12.969
获取编码器隐藏状态矩阵

00:04:12.969 --> 00:04:15.794
像之前提到的那样 生成得分

00:04:15.794 --> 00:04:17.500
如果执行乘法注意力运算

00:04:17.500 --> 00:04:18.574
则使用点积

00:04:18.574 --> 00:04:23.079
总之 生成得分

00:04:23.079 --> 00:04:25.359
应用 softmax

00:04:25.360 --> 00:04:32.389
将 softmax 得分与编码器中的每个相应隐藏状态相乘

00:04:32.389 --> 00:04:38.399
求和 生成注意力语境向量

00:04:38.399 --> 00:04:42.154
下一步是将注意力语境向量

00:04:42.154 --> 00:04:46.269
与该时间步的解码器隐藏状态（即 h4）相连

00:04:46.269 --> 00:04:50.264
也就是 c4 与 h4 相连

00:04:50.264 --> 00:04:52.060
即这里的步骤

00:04:52.060 --> 00:04:56.750
将它们合并成一个向量

00:04:56.750 --> 00:05:00.680
然后将它们传入一个完全连接的神经网络

00:05:00.680 --> 00:05:05.920
也就是乘以权重矩阵 wc 应用双曲正切激活函数

00:05:05.920 --> 00:05:09.230
这个完全连接层的输出

00:05:09.230 --> 00:05:14.134
将是输出序列中的第一个输出单词

00:05:14.134 --> 00:05:16.879
现在进入下一步

00:05:16.879 --> 00:05:23.634
将隐藏状态传递给它 并接受第一个解码器时间步的输出

00:05:23.634 --> 00:05:31.055
生成 h5 在这个时间步也执行注意力运算 评分

00:05:31.055 --> 00:05:33.915
生成权重向量

00:05:33.915 --> 00:05:36.270
应用 softmax 相乘

00:05:36.269 --> 00:05:38.889
相加 生成 c5

00:05:38.889 --> 00:05:41.930
得出第五步的注意力语境向量

00:05:41.930 --> 00:05:44.179
将它与隐藏状态合并

00:05:44.178 --> 00:05:49.939
传入相同的完全连接网络并应用双曲正切激活函数

00:05:49.939 --> 00:05:53.089
生成输出中的第二个单词 一直持续下去

00:05:53.089 --> 00:05:57.599
直到完成输出序列

00:05:57.600 --> 00:06:03.415
这就是序列到序列模型中的注意力原理的完整流程图

00:06:03.415 --> 00:06:07.819
在下个视频中 我们将讲解加法注意力

