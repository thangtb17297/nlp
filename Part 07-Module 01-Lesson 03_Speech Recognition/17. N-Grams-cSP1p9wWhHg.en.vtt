WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.970
The job of the Language Model is to inject

00:00:02.970 --> 00:00:07.970
the language knowledge into the words to text step in speech recognition,

00:00:07.969 --> 00:00:11.400
providing another layer of processing between words and

00:00:11.400 --> 00:00:16.170
text to solve ambiguities in spelling and context.

00:00:16.170 --> 00:00:20.429
For example, since an Acoustic Model is based on sound,

00:00:20.429 --> 00:00:25.490
we can't distinguish the correct spelling for words that sound the same,

00:00:25.490 --> 00:00:28.559
such as hear and hear.

00:00:28.559 --> 00:00:34.725
Other sequences may not make sense but could be corrected with a little more information.

00:00:34.725 --> 00:00:39.990
The words produced by the Acoustic Model are not absolute choices.

00:00:39.990 --> 00:00:45.795
They can be thought of as a probability distribution over many different words.

00:00:45.795 --> 00:00:49.829
Each possible sequence can be calculated as the likelihood that

00:00:49.829 --> 00:00:54.564
the particular word sequence could have been produced by the audio signal.

00:00:54.564 --> 00:01:01.829
A statistical language model provides a probability distribution over sequences of words.

00:01:01.829 --> 00:01:03.509
If we have both of these,

00:01:03.509 --> 00:01:06.498
the Acoustic Model and the Language Model,

00:01:06.498 --> 00:01:10.185
then the most likely sequence would be a combination

00:01:10.185 --> 00:01:14.909
over all these possibilities with the greatest likelihood score.

00:01:14.909 --> 00:01:18.842
If all possibilities in both models were scored,

00:01:18.843 --> 00:01:23.400
this could be a very large dimension of computations.

00:01:23.400 --> 00:01:29.715
We can get a good estimate though by only looking at some limited depth of choices.

00:01:29.715 --> 00:01:32.025
It turns out that in practice,

00:01:32.025 --> 00:01:35.490
the words we speak at any time are primarily

00:01:35.489 --> 00:01:40.474
dependent upon only the previous three to four words.

00:01:40.474 --> 00:01:45.419
N-grams are probabilities of single words, ordered pairs,

00:01:45.420 --> 00:01:49.079
triples etc.. With N-grams

00:01:49.079 --> 00:01:52.980
we can approximate the sequence probability with the chain rule.

00:01:52.980 --> 00:01:57.990
The probability that the first word occurs is multiplied by the probability of the

00:01:57.989 --> 00:02:04.019
second given the first and so on to get probabilities of a given sequence.

00:02:04.019 --> 00:02:06.750
We can then score these probabilities along with

00:02:06.750 --> 00:02:09.840
the probabilities from the Acoustic Model to remove

00:02:09.840 --> 00:02:13.140
language ambiguities from the sequence options and

00:02:13.139 --> 00:02:17.059
provide a better estimate of the utterance in text.

