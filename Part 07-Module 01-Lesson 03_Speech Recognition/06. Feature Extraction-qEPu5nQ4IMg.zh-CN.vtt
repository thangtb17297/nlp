WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.809
哪部分音频信号对语音识别最重要？

00:00:04.809 --> 00:00:08.564
一个人说 另一个人听

00:00:08.564 --> 00:00:10.739
我们说的话会受

00:00:10.740 --> 00:00:15.120
发声系统和耳朵接收能力的限制

00:00:15.119 --> 00:00:18.269
我们先讲耳朵和人类可接收的音高

00:00:18.269 --> 00:00:21.644
梅尔刻度于 1937 年提出

00:00:21.644 --> 00:00:25.464
描述了人类可真正识别的音高

00:00:25.464 --> 00:00:29.579
其实 有些频率我们听起来并无不同

00:00:29.579 --> 00:00:34.409
但对我们来说 低频的差异要比高频的明显

00:00:34.409 --> 00:00:35.949
如果是我们听不到的音高

00:00:35.950 --> 00:00:38.440
那就没必要保留在数据里了

00:00:38.439 --> 00:00:41.640
如果人耳无法分辨两种不同的频率

00:00:41.640 --> 00:00:45.420
那为我们便利 就可以把这两种视为相同的频率

00:00:45.420 --> 00:00:48.140
为了进行特征提取

00:00:48.140 --> 00:00:51.689
我们可以给频谱图的频率分组

00:00:51.689 --> 00:00:56.604
分组依据与人耳接收能力有关 这样就能过滤我们听不见的声音

00:00:56.604 --> 00:01:01.164
从而大幅减少我们要处理的频率数

00:01:01.164 --> 00:01:03.354
而这还不够

00:01:03.354 --> 00:01:08.039
我们还得把不受讲者影响的声音元素分离出来

00:01:08.040 --> 00:01:12.765
为此 我们来看看人类说话用的发声系统

00:01:12.765 --> 00:01:15.510
人的嗓音各不相同

00:01:15.510 --> 00:01:19.079
哪怕人体基本解剖特征是一样的

00:01:19.079 --> 00:01:24.870
我们可以把人声生成模型想成是声源和过滤器的组合体

00:01:24.870 --> 00:01:29.010
每个人的声源都不同

00:01:29.010 --> 00:01:33.770
过滤器则是我们说话都会用到的单词发音

00:01:33.769 --> 00:01:39.314
倒谱分析就靠这个模型来区分这两者

00:01:39.314 --> 00:01:41.670
我们可以用算法将倒谱

00:01:41.670 --> 00:01:44.905
从信号中提取出来 具体算法请见参考部分

00:01:44.905 --> 00:01:49.590
你要记住的关键点是 我们要舍弃带有个体声带特征的语音成分

00:01:49.590 --> 00:01:55.520
保留声道形成的那部分声音形状

00:01:55.519 --> 00:01:59.549
将倒谱分析与梅尔频率分析相结合

00:01:59.549 --> 00:02:04.000
你就能得到与语音有关的 12 或 13 种 MFCC (梅尔频率倒谱系数) 特征

00:02:04.000 --> 00:02:11.150
你还可选择是否要在特征集里附加 Δ 和 ΔΔ MFCC 特征

00:02:11.150 --> 00:02:13.920
虽然附加特征会使特征数增加一倍或两倍

00:02:13.919 --> 00:02:17.429
但事实证明 这样 ASR 的效果更好

00:02:17.430 --> 00:02:22.620
使用 MFCC 特征提取的好处就是

00:02:22.620 --> 00:02:28.200
我们不仅能大幅减少数据维数 还能过滤掉系统噪声

00:02:28.199 --> 00:02:31.409
接下来 我们将从语言的角度来研究声音

00:02:31.409 --> 00:02:33.419
讲讲单词语音学

