<!-- udacimak v1.3.0 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Connectionist Tempora Classification (CTC)</title>
  <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../assets/css/plyr.css">
  <link rel="stylesheet" href="../assets/css/katex.min.css">
  <link rel="stylesheet" href="../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <link rel="shortcut icon" type="image/png" href="../assets/img/udacimak.png" />
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>Speech Recognition</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="01. Intro.html">01. Intro</a>
    </li>
    <li class="">
      <a href="02. Challenges in ASR.html">02. Challenges in ASR</a>
    </li>
    <li class="">
      <a href="03. Signal Analysis.html">03. Signal Analysis</a>
    </li>
    <li class="">
      <a href="04. References Signal Analysis.html">04. References: Signal Analysis</a>
    </li>
    <li class="">
      <a href="05. Quiz FFT.html">05. Quiz: FFT</a>
    </li>
    <li class="">
      <a href="06. Feature Extraction with MFCC.html">06. Feature Extraction with MFCC</a>
    </li>
    <li class="">
      <a href="07. References Feature Extraction.html">07. References: Feature Extraction</a>
    </li>
    <li class="">
      <a href="08. Quiz MFCC.html">08. Quiz: MFCC</a>
    </li>
    <li class="">
      <a href="09. Phonetics.html">09. Phonetics</a>
    </li>
    <li class="">
      <a href="10. References Phonetics.html">10. References: Phonetics</a>
    </li>
    <li class="">
      <a href="11. Quiz Phonetics.html">11. Quiz: Phonetics</a>
    </li>
    <li class="">
      <a href="12. Voice Data Lab Introduction.html">12. Voice Data Lab Introduction</a>
    </li>
    <li class="">
      <a href="13. Lab Voice Data.html">13. Lab: Voice Data</a>
    </li>
    <li class="">
      <a href="14. Acoustic Models and the Trouble with Time.html">14. Acoustic Models and the Trouble with Time</a>
    </li>
    <li class="">
      <a href="15. HMMs in Speech Recognition.html">15. HMMs in Speech Recognition</a>
    </li>
    <li class="">
      <a href="16. Language Models.html">16. Language Models</a>
    </li>
    <li class="">
      <a href="17. N-Grams.html">17. N-Grams</a>
    </li>
    <li class="">
      <a href="18. Quiz N-Grams.html">18. Quiz: N-Grams</a>
    </li>
    <li class="">
      <a href="19. References Traditional ASR.html">19. References: Traditional ASR</a>
    </li>
    <li class="">
      <a href="20. A New Paradigm.html">20. A New Paradigm</a>
    </li>
    <li class="">
      <a href="21. Deep Neural Networks as Speech Models.html">21. Deep Neural Networks as Speech Models</a>
    </li>
    <li class="">
      <a href="22. Connectionist Tempora Classification (CTC).html">22. Connectionist Tempora Classification (CTC)</a>
    </li>
    <li class="">
      <a href="23. References Deep Neural Network ASR.html">23. References: Deep Neural Network ASR</a>
    </li>
    <li class="">
      <a href="24. Outro.html">24. Outro</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">22. Connectionist Tempora Classification (CTC)</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <div>
  <p># Connectionist Temporal Classification (CTC)</p>
<h2 id="the-sequencing-problem">The Sequencing Problem</h2>
<p>The input data in speech recognition is a sequence of observations in the form of frame vectors from regular time intervals.  The desired output is a series of symbols: phonemes, graphemes, or words.  The basic problem is that the number of frames does not have a predictible correspondence to the number of the output symbols.  For example, if we assume 20ms per frame, the following audio signals of the word "speech" spoken at two different speeds have about 300 frames in the first example and something like 850 frames in the second example, yet they should both be decoded as the six-letter word, "speech".</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/two-speechsignals-aligned.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h4 id="sequencing-with-hmms">Sequencing with HMMs</h4>
<p>Hidden Markov models (HMMs) are well suited to sequencing because each hidden state can be mapped to any number of frames. During training, distributions are clustered in each hidden state to define the model. During decoding, the overall likelihood that a particular HMM model could have emitted a given series of frame vectors is calculated and compared to the likelihood that they were emitted by other HMMs.  The following diagram illustrates frame mappings for a single HMM. </p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/hmmstates.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h4 id="why-rnns-cant-sequence">Why RNNs can't sequence</h4>
<p>By contrast, a Recurrent Neural Network (RNN) produces a probability distribution of output symbols for each frame.  Suppose we want to train an RNN network to recognize 26 different letters plus an apostrophe, space, and blank (29 graphemes). There will be a softmax layer output that produces a probability distribution for these possibilities.  Each frame will produce one of these probability distribution vectors.  If the utterance has 300 observations, there will be 300 RNN softmax vectors.  If the utterance consists of 850 observations, there will be 850 vectors. </p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/rnndist.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>The RNN could learn what those graphemes should be if there was a label associated with each frame.  This would require some sort of manual pre-segmentation. </p>
<p>A more ideal solution would be to provide the network with a loss function across the entire label sequence that it could minimize when training. We would like the probability distribution of the softmax output to "spike" for each grapheme and provide blanks or some other consistently ignored result between the graphemes so that the transcription could be easily decoded. This would solve the sequencing problem as audio signals of arbitrary length are converted to text.</p>
<h2 id="ctc-to-the-rescue">CTC to the rescue</h2>
<p>A Connectionist Temporal Classification (CTC) loss function can be calculated to train the network.  When combined with the CTC decoding algorithm over the softmax output layer, the sequence-to-sequence transformation is achieved.  The details of how this works are in the seminal paper on CTC by Alex Graves:</p>
<p><a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_GravesFGS06.pdf" target="_blank">Graves, Alex, et al. "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks." Proceedings of the 23rd international conference on Machine learning. ACM, 2006.</a></p>
<p>Here are a few take-aways from the paper:</p>
<h4 id="training-with-the-ctc-loss-function">Training with the CTC Loss Function</h4>
<p>Graves derives an objective function to calculate the error between the labels expected and the softmax result.  The function calculates all the possible paths and likelihoods for the softmax output layer and calculates the maximum log likelihood for the correct labeling.  For example, a correct labeling of a "hello" utterance might have several valid paths with varying probabilities: </p>
<pre><code>  _ _ H _ _ E L _ _ L _ O _  probability = p1
  _ H _ E L _ L _ _ _ O _ _  probability = p2
  _ _ _ _ H E L _ L _ O _ _  probability = p3
  ...
  _ H E _ _ L _ L _ O _ _ _  probability = pn</code></pre>
<p>The maximum log likelihood is the sum of likelihoods for all correct paths.  From Graves: "The aim of maximum likelihood training is to simultaneously<br />
maximise the log probabilities of all the correct<br />
classifications in the training set. In our case, this<br />
means minimising the following objective function:"</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/ctcfunction.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h6 id="span-classmathquill-ud-mathsspan--the-set-of-all-possible-tuples-span-classmathquill-ud-mathmathbfxspan-span-classmathquill-ud-mathmathbfzspan"><span class="mathquill ud-math">S</span> = the set of all possible tuples (<span class="mathquill ud-math">\mathbf{x}</span> ,<span class="mathquill ud-math">\mathbf{z}</span>)</h6>
<h6 id="span-classmathquill-ud-mathmathbfxspan--the-input-sequence"><span class="mathquill ud-math">\mathbf{x}</span> = the input sequence</h6>
<h6 id="span-classmathquill-ud-mathmathbfzspan--the-target-sequence"><span class="mathquill ud-math">\mathbf{z}</span> = the target sequence</h6>
<h6 id="span-classmathquill-ud-mathmathcaln_wspan--the-network-weight-variables"><span class="mathquill ud-math">\mathcal{N}_{w}</span> = the network weight variables</h6>
<p></br><br />
The loss is then differentiated and used in standard gradient descent training of the network to correct the softmax output such that it matches the labels when decoded.  </p>
<h4 id="the-output-includes-blanks">The output includes "blanks"</h4>
<p>The softmax output for the CTC network includes "blank" symbols that can be ignored during decoding.</p>
<h4 id="ctc-decoding">CTC Decoding</h4>
<p>Extracting the most likely symbol from each softmax distribution will result in a string of symbols the length of the original input sequence (the frames).  However, with the CTC training, the probable symbols have become consolidated.  The CTC decoding algorithm can then compress the transcription to its correct length by ignoring adjacent duplicates and blanks. </p>
<p>A more complex CTC decoding can provide not only the most likely transcription, but also some number of top choices using a beam search of arbitrary size.  This is useful if the result will then be processed with a language model for additional accuracy.</p>
<h2 id="ctc-implementation-options">CTC Implementation options</h2>
<ul>
<li><a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/connectionist_temporal_classification__ctc_" target="_blank">TensorFlow</a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/keras/backend/ctc_batch_cost" target="_blank">Keras</a></li>
<li><a href="https://github.com/baidu-research/warp-ctc" target="_blank">CTC-Warp</a></li>
</ul>
</div>

</div>
<div class="divider"></div>
          </div>

          <div class="col-12">
            <p class="text-right">
              <a href="23. References Deep Neural Network ASR.html" class="btn btn-outline-primary mt-4" role="button">Next Concept</a>
            </p>
          </div>
        </div>
      </main>

      <footer class="footer">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <p class="text-center">
                <a href="https://github.com/udacimak/udacimak#readme" target="_blank">udacimak v1.3.0</a>
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </div>


  <script src="../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../assets/js/bootstrap.min.js"></script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('22. Connectionist Tempora Classification (CTC)')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
</body>

</html>
