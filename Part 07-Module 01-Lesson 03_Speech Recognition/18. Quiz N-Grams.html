<!-- udacimak v1.3.0 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Quiz: N-Grams</title>
  <link rel="stylesheet" href="../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../assets/css/plyr.css">
  <link rel="stylesheet" href="../assets/css/katex.min.css">
  <link rel="stylesheet" href="../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <link rel="shortcut icon" type="image/png" href="../assets/img/udacimak.png" />
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>Speech Recognition</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="01. Intro.html">01. Intro</a>
    </li>
    <li class="">
      <a href="02. Challenges in ASR.html">02. Challenges in ASR</a>
    </li>
    <li class="">
      <a href="03. Signal Analysis.html">03. Signal Analysis</a>
    </li>
    <li class="">
      <a href="04. References Signal Analysis.html">04. References: Signal Analysis</a>
    </li>
    <li class="">
      <a href="05. Quiz FFT.html">05. Quiz: FFT</a>
    </li>
    <li class="">
      <a href="06. Feature Extraction with MFCC.html">06. Feature Extraction with MFCC</a>
    </li>
    <li class="">
      <a href="07. References Feature Extraction.html">07. References: Feature Extraction</a>
    </li>
    <li class="">
      <a href="08. Quiz MFCC.html">08. Quiz: MFCC</a>
    </li>
    <li class="">
      <a href="09. Phonetics.html">09. Phonetics</a>
    </li>
    <li class="">
      <a href="10. References Phonetics.html">10. References: Phonetics</a>
    </li>
    <li class="">
      <a href="11. Quiz Phonetics.html">11. Quiz: Phonetics</a>
    </li>
    <li class="">
      <a href="12. Voice Data Lab Introduction.html">12. Voice Data Lab Introduction</a>
    </li>
    <li class="">
      <a href="13. Lab Voice Data.html">13. Lab: Voice Data</a>
    </li>
    <li class="">
      <a href="14. Acoustic Models and the Trouble with Time.html">14. Acoustic Models and the Trouble with Time</a>
    </li>
    <li class="">
      <a href="15. HMMs in Speech Recognition.html">15. HMMs in Speech Recognition</a>
    </li>
    <li class="">
      <a href="16. Language Models.html">16. Language Models</a>
    </li>
    <li class="">
      <a href="17. N-Grams.html">17. N-Grams</a>
    </li>
    <li class="">
      <a href="18. Quiz N-Grams.html">18. Quiz: N-Grams</a>
    </li>
    <li class="">
      <a href="19. References Traditional ASR.html">19. References: Traditional ASR</a>
    </li>
    <li class="">
      <a href="20. A New Paradigm.html">20. A New Paradigm</a>
    </li>
    <li class="">
      <a href="21. Deep Neural Networks as Speech Models.html">21. Deep Neural Networks as Speech Models</a>
    </li>
    <li class="">
      <a href="22. Connectionist Tempora Classification (CTC).html">22. Connectionist Tempora Classification (CTC)</a>
    </li>
    <li class="">
      <a href="23. References Deep Neural Network ASR.html">23. References: Deep Neural Network ASR</a>
    </li>
    <li class="">
      <a href="24. Outro.html">24. Outro</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">18. Quiz: N-Grams</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <div>
  <h1 id="n-grams">N-Grams</h1>
<p>An N-Gram is an ordered sequence of words. For example:</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/ngrams-numbers.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>In the following series of quizes, you will work with 2-grams, or <a href="https://en.wikipedia.org/wiki/Bigram" target="_blank">bigrams</a>, as they are more commonly called.<br />
The objective is to create a function that calculates the probability that a particular sentence<br />
could occur in a corpus of text, based on the probabilities of its component bigrams.  We'll do this in stages though:</p>
<ul>
<li>Quiz 1 - Extract tokens and bigrams from a sentence</li>
<li>Quiz 2 - Calculate probabilities for bigrams</li>
<li>Quiz 3 - Calculate the log probability of a given sentence based on a corpus of text using bigrams</li>
</ul>
<h4 id="assumptions-and-terminology">Assumptions and terminology</h4>
<p>We will assume that text data is in the form of sentences with no punctuation.  If a sentence is in a single line, we will add add a token for<br />
start of sentence: <code>&lt;s&gt;</code> and end of sentence: <code>&lt;/s&gt;</code>.  For example, if the sentence is "I love language models." it will appear in code as:</p>
<pre><code>'I love language models'</code></pre>
<p>The <strong>tokens</strong> for this sentence are represented as an ordered list of the lower case words plus the start and end sentence tags:</p>
<pre><code>tokens = ['&lt;s&gt;', 'i', 'love', 'language', 'models', '&lt;/s&gt;']</code></pre>
<p>The <strong>bigrams</strong> for this sentence are represented as a list of lower case ordered pairs of tokens:</p>
<pre><code>bigrams = [('&lt;s&gt;', 'i'), ('i', 'love'), ('love', 'language'), ('language', 'models'), ('models', '&lt;/s&gt;')]</code></pre>
<h2 id="quiz-1-instructions">Quiz 1 Instructions</h2>
<p>In the quiz below, write a function that returns a list of tokens and a list of bigrams for a given sentence.  You will need to first break a sentence into words in a list, then add a <code>&lt;s&gt;</code> and <code>&lt;s/&gt;</code> token to the<br />
start and end of the list to represent the start and end of the sentence.</p>
<p>Your final lists should be in the format shown above and called out in the function doc string.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>

  <h4>Start Quiz:</h4>
  <div>
  <div class="nav nav-tabs nav-fill" role="tablist" id="question-tabs">
    <a href="#340907-function-py" class="nav-item nav-link  active show" id="tab-340907-function-py" data-toggle="tab" role="tab"
      aria-controls="340907-function-py" aria-selected="true">function.py</a>
    <a href="#340907-solution-py" class="nav-item nav-link " id="tab-340907-solution-py" data-toggle="tab" role="tab"
      aria-controls="340907-solution-py" aria-selected="false">solution.py</a>
  </div>

  <div class="tab-content" style="padding: 20px 0;" id="question-tab-contents">
    <div class="tab-pane  active show" id="340907-function-py" aria-labelledby="tab-340907-function-py" role="tabpanel">
      <pre><code></code>test_sentences &#x3D; [
    &#x27;the old man spoke to me&#x27;,
    &#x27;me to spoke man old the&#x27;,
    &#x27;old man me old man me&#x27;,
]

def sentence_to_bigrams(sentence):
    &quot;&quot;&quot;
    Add start &#x27;&lt;s&gt;&#x27; and stop &#x27;&lt;/s&gt;&#x27; tags to the sentence and tokenize it into a list
    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)
    :param sentence: string
    :return: list, list
        sentence_tokens: ordered list of words found in the sentence
        sentence_bigrams: a list of ordered two-word tuples found in the sentence
    &quot;&quot;&quot;
    #TODO implement
    sentence_tokens &#x3D; None
    sentence_bigrams &#x3D; None
    return sentence_tokens, sentence_bigrams</code></pre>
    </div>
    <div class="tab-pane " id="340907-solution-py" aria-labelledby="tab-340907-solution-py" role="tabpanel">
      <pre><code></code>test_sentences &#x3D; [
    &#x27;the old man spoke to me&#x27;,
    &#x27;me to spoke man old the&#x27;,
    &#x27;old man me old man me&#x27;,
]

def sentence_to_bigrams(sentence):
    &quot;&quot;&quot;
    Add start &#x27;&lt;s&gt;&#x27; and stop &#x27;&lt;/s&gt;&#x27; tags to the sentence and tokenize it into a list
    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)
    :param sentence: string
    :return: list, list
        sentence_tokens: ordered list of words found in the sentence
        sentence_bigrams: a list of ordered two-word tuples found in the sentence
    &quot;&quot;&quot;
    sentence_tokens &#x3D; [&#x27;&lt;s&gt;&#x27;] + sentence.lower().split() + [&#x27;&lt;/s&gt;&#x27;]
    sentence_bigrams &#x3D; []
    for i in range(len(sentence_tokens)-1):
        sentence_bigrams.append((sentence_tokens[i], sentence_tokens[i+1]))
    return sentence_tokens, sentence_bigrams</code></pre>
    </div>
  </div>
</div>



</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="probabilities-and-likelihoods-with-bigrams">Probabilities and Likelihoods with Bigrams</h2>
<p>Recall from a previous video that the probability of a series of words<br />
can be calculated from the chained probabilities of its history:</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/eqn-jointprob-words-in-sentence.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>The probabilities of sequence occurrences in a large textual corpus can be calculated this<br />
way and used as a language model to add grammar and contectual knowledge to a speech<br />
recognition system.  However, there is a prohibitively large number of calculations for all the <br />
possible sequences of varying length in a large textual corpus. </p>
<p>To address this problem, we use the <a href="https://en.wikipedia.org/wiki/Markov_property" target="_blank">Markov Assumption</a> to approximate<br />
a sequence probability with a shorter sequence:</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/eqn-markov-assumption-ngrams.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>In the bigram case, the equation reduces to a series of bigram probabilities multiplied together to find the approximate probability for a sentence.  A concrete example:</p>
<p><span class="mathquill ud-math">\qquad  \qquad  P("I\: Iove\: language\: models") \approx</span><br />
</br><br />
<span class="mathquill ud-math">\qquad  \qquad  \qquad  \qquad  P("love"|"I")P("language"|"love")P("models"|"language")</span></p>
<p>We can calculate the probabilities by using <strong>counts</strong> of the bigramsand individual tokens.  The counts are represented below with the <span class="mathquill ud-math">c()</span> operator:</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/eqn-bigram-mle.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <p>In Python, the  <a href="https://docs.python.org/3.6/library/collections.html#collections.Counter" target="_blank">Counter</a> method is useful for this task</p>
<pre><code>from collections import Counter
# Sentence: "I am as I am"
tokens = ['&lt;s&gt;', 'i', 'am', 'as', 'i', 'am', '&lt;/s&gt;']
token_counts = Counter(tokens)
print(token_counts)
# output:
# Counter({'&lt;/s&gt;': 1, '&lt;s&gt;': 1, 'am': 2, 'as': 1, 'i': 2})</code></pre>
<h2 id="quiz-2-instructions">Quiz 2 Instructions</h2>
<p>In the quiz below, write a function that returns a probability dictionary when given a lists of tokens and bigrams.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>

  <h4>Start Quiz:</h4>
  <div>
  <div class="nav nav-tabs nav-fill" role="tablist" id="question-tabs">
    <a href="#341448-function-py" class="nav-item nav-link  active show" id="tab-341448-function-py" data-toggle="tab" role="tab"
      aria-controls="341448-function-py" aria-selected="true">function.py</a>
    <a href="#341448-utils-py" class="nav-item nav-link " id="tab-341448-utils-py" data-toggle="tab" role="tab"
      aria-controls="341448-utils-py" aria-selected="false">utils.py</a>
    <a href="#341448-solution-py" class="nav-item nav-link " id="tab-341448-solution-py" data-toggle="tab" role="tab"
      aria-controls="341448-solution-py" aria-selected="false">solution.py</a>
    <a href="#341448-transcripts-txt" class="nav-item nav-link " id="tab-341448-transcripts-txt" data-toggle="tab" role="tab"
      aria-controls="341448-transcripts-txt" aria-selected="false">transcripts.txt</a>
  </div>

  <div class="tab-content" style="padding: 20px 0;" id="question-tab-contents">
    <div class="tab-pane  active show" id="341448-function-py" aria-labelledby="tab-341448-function-py" role="tabpanel">
      <pre><code></code>from collections import Counter
import utils


def sample_run():
    # sample usage by test code (this definition not actually run for the quiz)
    tokens, bigrams &#x3D; utils.bigrams_from_transcript(&#x27;transcripts.txt&#x27;)
    bg_dict &#x3D; bigram_mle(tokens, bigrams)
    print(bg_dict)


def bigram_mle(tokens, bigrams):
    &quot;&quot;&quot;
    provide a dictionary of probabilities for all bigrams in a corpus of text
    the calculation is based on maximum likelihood estimation and does not include
    any smoothing.  A tag &#x27;&lt;unk&gt;&#x27; has been added for unknown probabilities.
    :param tokens: list
        tokens: list of all tokens in the corpus
    :param bigrams: list
        bigrams: list of all two word tuples in the corpus
    :return: dict
        bg_mle_dict: a dictionary of bigrams:
            key: tuple of two bigram words, in order OR &lt;unk&gt; key
            value: float probability

    &quot;&quot;&quot;
    bg_mle_dict &#x3D; {}
    bg_mle_dict[&#x27;&lt;unk&gt;&#x27;] &#x3D; 0.
    #TODO implement
    return bg_mle_dict
</code></pre>
    </div>
    <div class="tab-pane " id="341448-utils-py" aria-labelledby="tab-341448-utils-py" role="tabpanel">
      <pre><code></code>def bigrams_from_transcript(filename):
    &quot;&quot;&quot;
    read a file of sentences, adding start &#x27;&lt;s&gt;&#x27; and stop &#x27;&lt;/s&gt;&#x27; tags; Tokenize it into a list of lower case words
    and bigrams
    :param filename: string 
        filename: path to a text file consisting of lines of non-puncuated text; assume one sentence per line
    :return: list, list
        tokens: ordered list of words found in the file
        bigrams: a list of ordered two-word tuples found in the file
    &quot;&quot;&quot;
    tokens &#x3D; []
    bigrams &#x3D; []
    with open(filename, &#x27;r&#x27;) as f:
        for line in f:
            line_tokens, line_bigrams &#x3D; sentence_to_bigrams(line)
            tokens &#x3D; tokens + line_tokens
            bigrams &#x3D; bigrams + line_bigrams
    return tokens, bigrams


def sentence_to_bigrams(sentence):
    &quot;&quot;&quot;
    Add start &#x27;&lt;s&gt;&#x27; and stop &#x27;&lt;/s&gt;&#x27; tags to the sentence and tokenize it into a list
    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)
    :param sentence: string
    :return: list, list
        sentence_tokens: ordered list of words found in the sentence
        sentence_bigrams: a list of ordered two-word tuples found in the sentence
    &quot;&quot;&quot;
    sentence_tokens &#x3D; [&#x27;&lt;s&gt;&#x27;] + sentence.lower().split() + [&#x27;&lt;/s&gt;&#x27;]
    sentence_bigrams &#x3D; []
    for i in range(len(sentence_tokens)-1):
        sentence_bigrams.append((sentence_tokens[i], sentence_tokens[i+1]))
    return sentence_tokens, sentence_bigrams</code></pre>
    </div>
    <div class="tab-pane " id="341448-solution-py" aria-labelledby="tab-341448-solution-py" role="tabpanel">
      <pre><code></code>from collections import Counter
import utils

def bigram_mle(tokens, bigrams):
    &quot;&quot;&quot;
    provide a dictionary of probabilities for all bigrams in a corpus of text
    the calculation is based on maximum likelihood estimation and does not include
    any smoothing.  A tag &#x27;&lt;unk&gt;&#x27; has been added for unknown probabilities.
    :param tokens: list
        tokens: list of all tokens in the corpus
    :param bigrams: list
        bigrams: list of all two word tuples in the corpus
    :return: dict
        bg_mle_dict: a dictionary of bigrams:
            key: tuple of two bigram words, in order OR &lt;unk&gt; key
            value: float probability
            
    &quot;&quot;&quot;
    bg_mle_dict &#x3D; {}
    bg_mle_dict[&#x27;&lt;unk&gt;&#x27;] &#x3D; 0.

    token_raw_counts &#x3D; Counter(tokens)
    bigram_raw_counts &#x3D; Counter(bigrams)
    for bg in bigram_raw_counts:
        bg_mle_dict[bg] &#x3D; bigram_raw_counts[bg] / token_raw_counts[bg[0]]
    return bg_mle_dict

</code></pre>
    </div>
    <div class="tab-pane " id="341448-transcripts-txt" aria-labelledby="tab-341448-transcripts-txt" role="tabpanel">
      <pre><code></code>GO DO YOU HEAR
BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT
AT THIS MOMENT THE WHOLE SOUL OF THE OLD MAN SEEMED CENTRED IN HIS EYES WHICH BECAME BLOODSHOT THE VEINS OF THE THROAT SWELLED HIS CHEEKS AND TEMPLES BECAME PURPLE AS THOUGH HE WAS STRUCK WITH EPILEPSY NOTHING WAS WANTING TO COMPLETE THIS BUT THE UTTERANCE OF A CRY
AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE
DAVRIGNY RUSHED TOWARDS THE OLD MAN AND MADE HIM INHALE A POWERFUL RESTORATIVE
DAVRIGNY UNABLE TO BEAR THE SIGHT OF THIS TOUCHING EMOTION TURNED AWAY AND VILLEFORT WITHOUT SEEKING ANY FURTHER EXPLANATION AND ATTRACTED TOWARDS HIM BY THE IRRESISTIBLE MAGNETISM WHICH DRAWS US TOWARDS THOSE WHO HAVE LOVED THE PEOPLE FOR WHOM WE MOURN EXTENDED HIS HAND TOWARDS THE YOUNG MAN
FOR SOME TIME NOTHING WAS HEARD IN THAT CHAMBER BUT SOBS EXCLAMATIONS AND PRAYERS
WHAT DO YOU MEAN SIR
OH YOU RAVE SIR EXCLAIMED VILLEFORT IN VAIN ENDEAVORING TO ESCAPE THE NET IN WHICH HE WAS TAKEN I RAVE
DO YOU KNOW THE ASSASSIN ASKED MORREL
NOIRTIER LOOKED UPON MORREL WITH ONE OF THOSE MELANCHOLY SMILES WHICH HAD SO OFTEN MADE VALENTINE HAPPY AND THUS FIXED HIS ATTENTION
SAID MORREL SADLY YES REPLIED NOIRTIER
THE OLD MANS EYES REMAINED FIXED ON THE DOOR
ASKED MORREL YES
MUST I LEAVE ALONE NO
BUT CAN HE UNDERSTAND YOU YES
GENTLEMEN HE SAID IN A HOARSE VOICE GIVE ME YOUR WORD OF HONOR THAT THIS HORRIBLE SECRET SHALL FOREVER REMAIN BURIED AMONGST OURSELVES THE TWO MEN DREW BACK
MY FATHER HAS REVEALED THE CULPRITS NAME MY FATHER THIRSTS FOR REVENGE AS MUCH AS YOU DO YET EVEN HE CONJURES YOU AS I DO TO KEEP THIS SECRET DO YOU NOT FATHER
MORREL SUFFERED AN EXCLAMATION OF HORROR AND SURPRISE TO ESCAPE HIM
THE OLD MAN MADE A SIGN IN THE AFFIRMATIVE
IT WAS SOMETHING TERRIBLE TO WITNESS THE SILENT AGONY THE MUTE DESPAIR OF NOIRTIER WHOSE TEARS SILENTLY ROLLED DOWN HIS CHEEKS
BUT HE STOPPED ON THE LANDING HE HAD NOT THE COURAGE TO AGAIN VISIT THE DEATH CHAMBER
THE TWO DOCTORS THEREFORE ENTERED THE ROOM ALONE
NOIRTIER WAS NEAR THE BED PALE MOTIONLESS AND SILENT AS THE CORPSE
THE DISTRICT DOCTOR APPROACHED WITH THE INDIFFERENCE OF A MAN ACCUSTOMED TO SPEND HALF HIS TIME AMONGST THE DEAD HE THEN LIFTED THE SHEET WHICH WAS PLACED OVER THE FACE AND JUST UNCLOSED THE LIPS
THE NEAREST SAID THE DISTRICT DOCTOR IS A GOOD ITALIAN ABBE WHO LIVES NEXT DOOR TO YOU SHALL I CALL ON HIM AS I PASS
DAVRIGNY SAID VILLEFORT BE SO KIND I BESEECH YOU AS TO ACCOMPANY THIS GENTLEMAN HERE IS THE KEY OF THE DOOR SO THAT YOU CAN GO IN AND OUT AS YOU PLEASE YOU WILL BRING THE PRIEST WITH YOU AND WILL OBLIGE ME BY INTRODUCING HIM INTO MY CHILDS ROOM DO YOU WISH TO SEE HIM
I ONLY WISH TO BE ALONE YOU WILL EXCUSE ME WILL YOU NOT
I AM GOING SIR AND I DO NOT HESITATE TO SAY THAT NO PRAYERS WILL BE MORE FERVENT THAN MINE
</code></pre>
    </div>
  </div>
</div>



</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="smoothing-and-logs">Smoothing and logs</h2>
<p>There are still a couple of problems to sort out before we use the bigram probability dictionary to calculate the probabilities of new sentences:</p>
<h6 id="1-some-possible-combinations-may-not-exist-in-our-probability-dictionary-but-are-still-possible--we-dont-want-to-multiply-in-a-probability-of-0-just-because-our-original-corpus-was-deficient-this-is-solved-through-smoothing--there-are-a-number-of-methods-for-this-but-a-simple-one-is-the-laplace-smoothinghttpsenwikipediaorgwikiadditive_smoothing-with-the-add-one-estimate-where-span-classmathquill-ud-mathvspan-is-the-size-of-the-vocabulary-for-the-corpus-ie-the-number-of-unique-tokens">1. Some possible combinations may not exist in our probability dictionary but are still possible.  We don't want to multiply in a probability of 0 just because our original corpus was deficient. This is solved through "smoothing".  There are a number of methods for this, but a simple one is the <a href="https://en.wikipedia.org/wiki/Additive_smoothing" target="_blank">Laplace smoothing</a> with the "add-one" estimate where <span class="mathquill ud-math">V</span> is the size of the vocabulary for the corpus, i.e. the number of unique tokens:</h6>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/eqn-addone-bigram-smoothing.png" alt="" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h6 id="2-repeated-multiplications-of-small-probabilities-can-cause-underflow-problems-in-computers-when">2. Repeated multiplications of small probabilities can cause underflow problems in computers when</h6>
<p>the values become to small.  To solve this, we will calculate all probabilities in log space:</p>
<p><span class="mathquill ud-math">\qquad \qquad \qquad log(p_1\times p_2\times p_3\times p_4) = \log p_1 + \log p_2 + \log p_3 + \log p_4 </span></p>
<h2 id="quiz-3-instructions">Quiz 3 Instructions</h2>
<p>In the following quiz, a utility named <code>utils.bigram_add1_logs</code> has been added for you with Laplace smoothing in the log space. Write a function that calculates the log probability for a given sentence, using this log probability dictionary.  If all goes well, you <em>should</em> observe that more likely sentences yield higher values for the log probabilities.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>

  <h4>Start Quiz:</h4>
  <div>
  <div class="nav nav-tabs nav-fill" role="tablist" id="question-tabs">
    <a href="#341531-function-py" class="nav-item nav-link  active show" id="tab-341531-function-py" data-toggle="tab" role="tab"
      aria-controls="341531-function-py" aria-selected="true">function.py</a>
    <a href="#341531-utils-py" class="nav-item nav-link " id="tab-341531-utils-py" data-toggle="tab" role="tab"
      aria-controls="341531-utils-py" aria-selected="false">utils.py</a>
    <a href="#341531-transcripts-txt" class="nav-item nav-link " id="tab-341531-transcripts-txt" data-toggle="tab" role="tab"
      aria-controls="341531-transcripts-txt" aria-selected="false">transcripts.txt</a>
    <a href="#341531-solution-py" class="nav-item nav-link " id="tab-341531-solution-py" data-toggle="tab" role="tab"
      aria-controls="341531-solution-py" aria-selected="false">solution.py</a>
  </div>

  <div class="tab-content" style="padding: 20px 0;" id="question-tab-contents">
    <div class="tab-pane  active show" id="341531-function-py" aria-labelledby="tab-341531-function-py" role="tabpanel">
      <pre><code></code>import utils

test_sentences &#x3D; [
    &#x27;the old man spoke to me&#x27;,
    &#x27;me to spoke man old the&#x27;,
    &#x27;old man me old man me&#x27;,
]

def sample_run():
    # sample usage by test code (this definition not actually run for the quiz)
    bigram_log_dict &#x3D; utils.bigram_add1_logs(&#x27;transcripts.txt&#x27;)
    for sentence in test_sentences:
        print(&#x27;*** &quot;{}&quot;&#x27;.format(sentence))
        print(log_prob_of_sentence(sentence, bigram_log_dict))

def log_prob_of_sentence(sentence, bigram_log_dict):
    total_log_prob &#x3D; 0.

    # TODO implement
    # get the sentence bigrams with utils.sentence_to_bigrams
    # look up the bigrams from the sentence in the bigram_log_dict
    # add all the the log probabilities together
    # if a word doesn&#x27;t exist, be sure to use the value of the &#x27;&lt;unk&gt;&#x27; lookup instead

    return total_log_prob</code></pre>
    </div>
    <div class="tab-pane " id="341531-utils-py" aria-labelledby="tab-341531-utils-py" role="tabpanel">
      <pre><code></code>from collections import Counter
import numpy as np

def bigrams_from_transcript(filename):
    &quot;&quot;&quot;
    read a file of sentences, adding start &#x27;&lt;s&gt;&#x27; and stop &#x27;&lt;/s&gt;&#x27; tags; Tokenize it into a list of lower case words
    and bigrams
    :param filename: string 
        filename: path to a text file consisting of lines of non-puncuated text; assume one sentence per line
    :return: list, list
        tokens: ordered list of words found in the file
        bigrams: a list of ordered two-word tuples found in the file
    &quot;&quot;&quot;
    tokens &#x3D; []
    bigrams &#x3D; []
    with open(filename, &#x27;r&#x27;) as f:
        for line in f:
            line_tokens, line_bigrams &#x3D; sentence_to_bigrams(line)
            tokens &#x3D; tokens + line_tokens
            bigrams &#x3D; bigrams + line_bigrams
    return tokens, bigrams


def sentence_to_bigrams(sentence):
    &quot;&quot;&quot;
    Add start &#x27;&lt;s&gt;&#x27; and stop &#x27;&lt;/s&gt;&#x27; tags to the sentence and tokenize it into a list
    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)
    :param sentence: string
    :return: list, list
        sentence_tokens: ordered list of words found in the sentence
        sentence_bigrams: a list of ordered two-word tuples found in the sentence
    &quot;&quot;&quot;
    sentence_tokens &#x3D; [&#x27;&lt;s&gt;&#x27;] + sentence.lower().split() + [&#x27;&lt;/s&gt;&#x27;]
    sentence_bigrams &#x3D; []
    for i in range(len(sentence_tokens)-1):
        sentence_bigrams.append((sentence_tokens[i], sentence_tokens[i+1]))
    return sentence_tokens, sentence_bigrams

def bigram_add1_logs(transcript_file):
    &quot;&quot;&quot;
    provide a smoothed log probability dictionary based on a transcript
    :param transcript_file: string
        transcript_file is the path filename containing unpunctuated text sentences
    :return: dict
        bg_add1_log_dict: dictionary of smoothed bigrams log probabilities including
        tags: &lt;s&gt;: start of sentence, &lt;/s&gt;: end of sentence, &lt;unk&gt;: unknown placeholder probability
    &quot;&quot;&quot;

    tokens, bigrams &#x3D; bigrams_from_transcript(transcript_file)
    token_counts &#x3D; Counter(tokens)
    bigram_counts &#x3D; Counter(bigrams)
    vocab_count &#x3D; len(token_counts)

    bg_addone_dict &#x3D; {}
    for bg in bigram_counts:
        bg_addone_dict[bg] &#x3D; np.log((bigram_counts[bg] + 1.) / (token_counts[bg[0]] + vocab_count))
    bg_addone_dict[&#x27;&lt;unk&gt;&#x27;] &#x3D; np.log(1. / vocab_count)
    return bg_addone_dict</code></pre>
    </div>
    <div class="tab-pane " id="341531-transcripts-txt" aria-labelledby="tab-341531-transcripts-txt" role="tabpanel">
      <pre><code></code>GO DO YOU HEAR
BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT
AT THIS MOMENT THE WHOLE SOUL OF THE OLD MAN SEEMED CENTRED IN HIS EYES WHICH BECAME BLOODSHOT THE VEINS OF THE THROAT SWELLED HIS CHEEKS AND TEMPLES BECAME PURPLE AS THOUGH HE WAS STRUCK WITH EPILEPSY NOTHING WAS WANTING TO COMPLETE THIS BUT THE UTTERANCE OF A CRY
AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE
DAVRIGNY RUSHED TOWARDS THE OLD MAN AND MADE HIM INHALE A POWERFUL RESTORATIVE
DAVRIGNY UNABLE TO BEAR THE SIGHT OF THIS TOUCHING EMOTION TURNED AWAY AND VILLEFORT WITHOUT SEEKING ANY FURTHER EXPLANATION AND ATTRACTED TOWARDS HIM BY THE IRRESISTIBLE MAGNETISM WHICH DRAWS US TOWARDS THOSE WHO HAVE LOVED THE PEOPLE FOR WHOM WE MOURN EXTENDED HIS HAND TOWARDS THE YOUNG MAN
FOR SOME TIME NOTHING WAS HEARD IN THAT CHAMBER BUT SOBS EXCLAMATIONS AND PRAYERS
WHAT DO YOU MEAN SIR
OH YOU RAVE SIR EXCLAIMED VILLEFORT IN VAIN ENDEAVORING TO ESCAPE THE NET IN WHICH HE WAS TAKEN I RAVE
DO YOU KNOW THE ASSASSIN ASKED MORREL
NOIRTIER LOOKED UPON MORREL WITH ONE OF THOSE MELANCHOLY SMILES WHICH HAD SO OFTEN MADE VALENTINE HAPPY AND THUS FIXED HIS ATTENTION
SAID MORREL SADLY YES REPLIED NOIRTIER
THE OLD MANS EYES REMAINED FIXED ON THE DOOR
ASKED MORREL YES
MUST I LEAVE ALONE NO
BUT CAN HE UNDERSTAND YOU YES
GENTLEMEN HE SAID IN A HOARSE VOICE GIVE ME YOUR WORD OF HONOR THAT THIS HORRIBLE SECRET SHALL FOREVER REMAIN BURIED AMONGST OURSELVES THE TWO MEN DREW BACK
MY FATHER HAS REVEALED THE CULPRITS NAME MY FATHER THIRSTS FOR REVENGE AS MUCH AS YOU DO YET EVEN HE CONJURES YOU AS I DO TO KEEP THIS SECRET DO YOU NOT FATHER
MORREL SUFFERED AN EXCLAMATION OF HORROR AND SURPRISE TO ESCAPE HIM
THE OLD MAN MADE A SIGN IN THE AFFIRMATIVE
IT WAS SOMETHING TERRIBLE TO WITNESS THE SILENT AGONY THE MUTE DESPAIR OF NOIRTIER WHOSE TEARS SILENTLY ROLLED DOWN HIS CHEEKS
BUT HE STOPPED ON THE LANDING HE HAD NOT THE COURAGE TO AGAIN VISIT THE DEATH CHAMBER
THE TWO DOCTORS THEREFORE ENTERED THE ROOM ALONE
NOIRTIER WAS NEAR THE BED PALE MOTIONLESS AND SILENT AS THE CORPSE
THE DISTRICT DOCTOR APPROACHED WITH THE INDIFFERENCE OF A MAN ACCUSTOMED TO SPEND HALF HIS TIME AMONGST THE DEAD HE THEN LIFTED THE SHEET WHICH WAS PLACED OVER THE FACE AND JUST UNCLOSED THE LIPS
THE NEAREST SAID THE DISTRICT DOCTOR IS A GOOD ITALIAN ABBE WHO LIVES NEXT DOOR TO YOU SHALL I CALL ON HIM AS I PASS
DAVRIGNY SAID VILLEFORT BE SO KIND I BESEECH YOU AS TO ACCOMPANY THIS GENTLEMAN HERE IS THE KEY OF THE DOOR SO THAT YOU CAN GO IN AND OUT AS YOU PLEASE YOU WILL BRING THE PRIEST WITH YOU AND WILL OBLIGE ME BY INTRODUCING HIM INTO MY CHILDS ROOM DO YOU WISH TO SEE HIM
I ONLY WISH TO BE ALONE YOU WILL EXCUSE ME WILL YOU NOT
I AM GOING SIR AND I DO NOT HESITATE TO SAY THAT NO PRAYERS WILL BE MORE FERVENT THAN MINE
</code></pre>
    </div>
    <div class="tab-pane " id="341531-solution-py" aria-labelledby="tab-341531-solution-py" role="tabpanel">
      <pre><code></code>import ngram_quiz_3.utils as utils

test_sentences &#x3D; [
    &#x27;the old man spoke to me&#x27;,
    &#x27;me to spoke man old the&#x27;,
    &#x27;old man me old man me&#x27;,
]


def log_prob_of_sentence(sentence, bigram_log_dict):
    # get the sentence bigrams
    s_tokens, s_bigrams &#x3D; utils.sentence_to_bigrams(sentence)

    # add the log probabilites of the bigrams in the sentence
    total_log_prob &#x3D; 0.
    for bg in s_bigrams:
        if bg in bigram_log_dict:
            total_log_prob &#x3D; total_log_prob + bigram_log_dict[bg]
        else:
            total_log_prob &#x3D; total_log_prob + bigram_log_dict[&#x27;&lt;unk&gt;&#x27;]
    return total_log_prob
</code></pre>
    </div>
  </div>
</div>



</div>


</div>
<div class="divider"></div>
          </div>

          <div class="col-12">
            <p class="text-right">
              <a href="19. References Traditional ASR.html" class="btn btn-outline-primary mt-4" role="button">Next Concept</a>
            </p>
          </div>
        </div>
      </main>

      <footer class="footer">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <p class="text-center">
                <a href="https://github.com/udacimak/udacimak#readme" target="_blank">udacimak v1.3.0</a>
              </p>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </div>


  <script src="../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../assets/js/bootstrap.min.js"></script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('18. Quiz: N-Grams')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
</body>

</html>
