WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.679
If HMM's work why do we need a new model.

00:00:04.679 --> 00:00:06.809
It comes down to potential.

00:00:06.809 --> 00:00:11.945
Suppose we have all the data we need and all the processing power we want.

00:00:11.945 --> 00:00:14.804
How far can an HMM model take us,

00:00:14.804 --> 00:00:17.879
and how far could some other model take us?

00:00:17.879 --> 00:00:22.535
According to by Baidu's Adam Coates in a recent presentation,

00:00:22.535 --> 00:00:27.399
additional training of a traditional ASR levels off inaccuracy.

00:00:27.399 --> 00:00:31.559
Meanwhile, Deep Neural Network Solutions are unimpressive with

00:00:31.559 --> 00:00:37.000
small data sets but they shine as we increase data and model sizes.

00:00:37.000 --> 00:00:39.554
Here's the process we've looked at so far.

00:00:39.554 --> 00:00:44.465
We extract features from the audio speech signal with MFCC.

00:00:44.465 --> 00:00:50.280
Use an HMM acoustic model to convert to sound units, phonemes, or words.

00:00:50.280 --> 00:00:54.000
Then, it uses statistical language models such as N-grams to

00:00:54.000 --> 00:00:58.500
straighten out language ambiguities and create the final text sequence.

00:00:58.500 --> 00:01:03.929
It's possible to replace the many tune parts with a multiple layer deep neural network.

00:01:03.929 --> 00:01:07.484
Let's get a little intuition as to why they can be replaced.

00:01:07.484 --> 00:01:10.920
In feature extraction, we've used models based on

00:01:10.920 --> 00:01:15.685
human sound production and perception to convert a spectrogram into features.

00:01:15.685 --> 00:01:17.850
This is similar, intuitively,

00:01:17.849 --> 00:01:23.625
to the idea of using Convolutional Neural Networks to extract features from image data.

00:01:23.625 --> 00:01:27.694
Spectrograms are visual representations of speech.

00:01:27.694 --> 00:01:30.449
So, we ought to be able to let a CNN

00:01:30.450 --> 00:01:34.094
find the relevant features for speech in the same way.

00:01:34.094 --> 00:01:37.814
An acoustic model implemented with HMMs,

00:01:37.814 --> 00:01:42.194
includes transition probabilities to organize time series data.

00:01:42.194 --> 00:01:46.989
Recurrent Neural Networks can also track time series data through memory,

00:01:46.989 --> 00:01:50.429
as we've seen in earlier lessons on RNNs.

00:01:50.430 --> 00:01:55.900
The traditional model also uses HMMs to sequence sound units into words.

00:01:55.900 --> 00:02:00.859
The RNNs produce probability densities over each time slice.

00:02:00.859 --> 00:02:04.224
So we need another way to solve the sequencing issue.

00:02:04.224 --> 00:02:07.559
A Connectionist Temporal Classification layer

00:02:07.560 --> 00:02:11.400
is used to convert the RNN outputs into words.

00:02:11.400 --> 00:02:14.890
So, we can replace the acoustic portion of

00:02:14.889 --> 00:02:19.569
the network with a combination of RNN and CTC layers.

00:02:19.569 --> 00:02:23.775
The end-to-end DNN still makes linguistic errors,

00:02:23.775 --> 00:02:28.385
especially on words that it hasn't seen in enough examples.

00:02:28.384 --> 00:02:32.234
It should be possible for the system to learn language probabilities

00:02:32.235 --> 00:02:36.570
from audio data but presently there just isn't enough.

00:02:36.569 --> 00:02:40.479
The existing technology of N-grams can still be used.

00:02:40.479 --> 00:02:47.114
Alternately, a Neural Language Model can be trained on massive amounts of available text.

00:02:47.115 --> 00:02:49.628
Using an NLM layer,

00:02:49.627 --> 00:02:54.229
the probabilities of spelling and context can be re scored for the system.

