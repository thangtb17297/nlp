WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.750
连续语音识别的发展历程很是坎坷

00:00:03.750 --> 00:00:06.019
在 20 世纪 70 年代初期

00:00:06.019 --> 00:00:10.449
美国借助 DARPA 挑战赛资助了 ASR 的研究 (该挑战赛由美国国防部高级研究计划局 DARPA
资助)

00:00:10.449 --> 00:00:15.980
当时的资助目标是开发一个能识别 1000 个单词的识别器

00:00:15.980 --> 00:00:20.260
几年后 卡内基梅隆大学的 Harpy 系统实现了这个目标

00:00:20.260 --> 00:00:24.445
但前景不佳 而且资金也耗尽了

00:00:24.445 --> 00:00:28.635
由此 AI 第一个真正的冬天到来了

00:00:28.635 --> 00:00:34.590
80 和 90 年代 概率模型的改进提高了 AI 的性能

00:00:34.590 --> 00:00:36.990
再近些年 由于计算能力的提高

00:00:36.990 --> 00:00:40.410
神经网络模型得以建立更多维数

00:00:40.409 --> 00:00:43.354
那语音识别的难点在哪呢？

00:00:43.354 --> 00:00:45.569
第一批难点在

00:00:45.570 --> 00:00:49.289
音频信号本身 比如噪声、

00:00:49.289 --> 00:00:53.429
经过的汽车声、时钟的滴答声、他人的谈话声、

00:00:53.429 --> 00:00:57.060
麦克风静电干扰等等 ASR 需要知道

00:00:57.060 --> 00:01:01.920
哪部分音频信号是它需要的 哪部分又是该舍弃的

00:01:01.920 --> 00:01:03.570
音高的不同

00:01:03.570 --> 00:01:05.265
音量的不同

00:01:05.265 --> 00:01:10.030
虽然在说同一个单词 讲话者的声音也会不同

00:01:10.030 --> 00:01:12.750
最起码在英语里 音高和音量

00:01:12.750 --> 00:01:16.165
不会影响单词的意思

00:01:16.165 --> 00:01:18.510
就算我用不同的方式说

00:01:18.510 --> 00:01:22.693
 “你好”

00:01:22.692 --> 00:01:25.590
我说的都是同一个单词 同一种拼写

00:01:25.590 --> 00:01:28.400
因此 我们甚至可以把这种差异当作

00:01:28.400 --> 00:01:31.870
另一种需要过滤的噪声

00:01:31.870 --> 00:01:34.115
语速的不同

00:01:34.114 --> 00:01:38.629
我们需要对齐并匹配以不同语速说出的单词

00:01:38.629 --> 00:01:42.079
用不同的语速说 “演讲”

00:01:42.079 --> 00:01:45.424
指的都是同一个单词 字母数也是一样的

00:01:45.424 --> 00:01:50.599
因此 ASR 需要把声音序列正确地对齐起来

00:01:50.599 --> 00:01:52.459
单词边界

00:01:52.459 --> 00:01:56.509
当我们说“不停顿地说出一连串单词” 时

00:01:56.510 --> 00:02:01.948
我们不会故意一字一顿地说话

00:02:01.947 --> 00:02:04.309
人类能听懂这个句子

00:02:04.310 --> 00:02:08.235
因为我们已经知道什么地方该是单词的边界

00:02:08.235 --> 00:02:13.500
这就又给我们带来了一类难题 和语言或知识有关

00:02:13.500 --> 00:02:18.324
事实上 人类获取语音的途径并不只是耳朵

00:02:18.324 --> 00:02:21.589
我们是有语言知识域的

00:02:21.590 --> 00:02:25.594
这个知识域让我们得以自动判断那些模棱两可的话是什么意思

00:02:25.594 --> 00:02:29.395
听起来一样但拼写不一样的单词

00:02:29.395 --> 00:02:33.980
在一种语境里有意义但在其它语境无意义的词群

00:02:33.979 --> 00:02:36.144
举一个典型的例子

00:02:36.145 --> 00:02:38.765
当我用英语说 "recognize speech"（“识别语音”）的时候

00:02:38.764 --> 00:02:41.764
听起来很像 "wreck a nice beach"（“破坏美丽的沙滩”）

00:02:41.764 --> 00:02:46.419
但你知道我的意思 因为你知道我在讲语音识别

00:02:46.419 --> 00:02:48.094
所以语境很重要

00:02:48.094 --> 00:02:52.490
要让计算机模型实现这种推断是个难点

00:02:52.490 --> 00:02:54.314
还有一个方面要考虑

00:02:54.314 --> 00:02:57.710
口语和书面语不同

00:02:57.710 --> 00:03:01.610
说口语时 我们会犹豫、重复、只说只词片语、

00:03:01.610 --> 00:03:02.930
口误

00:03:02.930 --> 00:03:06.560
人类听者可以将这些干扰过滤掉

00:03:06.560 --> 00:03:12.824
但想象计算机只从有声书和报纸音频里学习语言

00:03:12.824 --> 00:03:18.239
这种系统估计很难对意料之外的句子结构进行解码

00:03:18.240 --> 00:03:21.770
好了 我们已经提出了很多要解决的问题了

00:03:21.770 --> 00:03:25.175
音高、音量和语速的不同

00:03:25.175 --> 00:03:29.915
单词界限、拼写和语境造成的语义模糊

00:03:29.914 --> 00:03:32.150
接下来我们要介绍几种办法

00:03:32.150 --> 00:03:35.575
来解决这些问题 我们会用到若干模型和技术

00:03:35.574 --> 00:03:39.000
先从语音本身开始学习吧

