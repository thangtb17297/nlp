WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.910
The idea for Hidden Markov Models is the following: Let's

00:00:02.910 --> 00:00:05.620
say that a way of tagging the sentence "Jane will spot

00:00:05.620 --> 00:00:09.609
Will" is noun-modal-verb-noun and

00:00:09.609 --> 00:00:12.044
we'll calculate a probability associated with this tagging.

00:00:12.044 --> 00:00:13.529
So we need two things.

00:00:13.529 --> 00:00:16.320
First of all, how likely is it that a noun is followed by

00:00:16.320 --> 00:00:20.204
a modal and a modal by a verb and a verb by a noun.

00:00:20.204 --> 00:00:23.849
These need to be high in order for this tagging to be likely.

00:00:23.850 --> 00:00:26.530
These are called the transition probabilities.

00:00:26.530 --> 00:00:28.560
Now, the second set of probabilities we need to

00:00:28.559 --> 00:00:31.140
calculate are these: What is the probability that

00:00:31.140 --> 00:00:35.820
a noun will be the word Jane and that a modal will be the word will, etc.

00:00:35.820 --> 00:00:39.195
This also need to be relatively high for our tagging to be likely.

00:00:39.195 --> 00:00:41.765
These are called the Emission Probabilities.

00:00:41.765 --> 00:00:43.740
So, here are sentences with

00:00:43.740 --> 00:00:47.849
their corresponding tags and we're going to calculate the Emission Probabilities,

00:00:47.848 --> 00:00:50.459
that is the probability that if a word is, say,

00:00:50.460 --> 00:00:54.240
a noun that that word will be Mary or Jane, etc.

00:00:54.240 --> 00:00:58.200
So, to do this we again do a counting table like this one where, for example,

00:00:58.200 --> 00:01:01.140
the entry on the Mary row and the noun column is

00:01:01.140 --> 00:01:04.950
four because Mary appears four times as a noun.

00:01:04.950 --> 00:01:07.155
Now, in order to find the probabilities,

00:01:07.155 --> 00:01:11.835
we divide each column by the sum of the entries and we obtain the following numbers.

00:01:11.834 --> 00:01:14.774
And here's a graphical representation of this table.

00:01:14.775 --> 00:01:17.400
Say, if we know that a word is a noun,

00:01:17.400 --> 00:01:20.905
the probabilities of it being Mary is four over nine,

00:01:20.905 --> 00:01:23.129
Jane is two over nine,

00:01:23.129 --> 00:01:25.530
Will is one over nine,

00:01:25.530 --> 00:01:27.500
and Spot is two over nine.

00:01:27.500 --> 00:01:29.295
The other words are zero.

00:01:29.295 --> 00:01:32.129
Same thing for modal, and for verb.

00:01:32.129 --> 00:01:35.219
And notice that words can appear repeatedly here like

00:01:35.219 --> 00:01:38.010
Will which appears as a noun and also as a modal.

00:01:38.010 --> 00:01:39.030
That is no problem.

00:01:39.030 --> 00:01:41.724
So, now let's calculate the transition probabilities.

00:01:41.724 --> 00:01:45.629
These are the probabilities that are part of speech follows another part of speech.

00:01:45.629 --> 00:01:48.584
First, in order to actually get the whole picture,

00:01:48.584 --> 00:01:50.849
we'll add starting and ending tags on

00:01:50.849 --> 00:01:55.064
each sentence and we'll treat these tags as parts of speech as well.

00:01:55.064 --> 00:01:57.974
And now we make a table of counts in this table.

00:01:57.974 --> 00:02:01.544
We count the number of appearances of each pair of parts of speech.

00:02:01.545 --> 00:02:05.475
For example, this three here in the noun row and the modal column

00:02:05.474 --> 00:02:09.419
corresponds to the three occurrences of a noun followed by a modal.

00:02:09.419 --> 00:02:11.173
Now, to find the probabilities,

00:02:11.174 --> 00:02:14.040
we divide each row by the sum of the entries in the row.

00:02:14.039 --> 00:02:15.780
In this way, if we look at,

00:02:15.780 --> 00:02:17.625
say, the modal row,

00:02:17.625 --> 00:02:21.574
the probability that the next part of speech is a noun is one quarter,

00:02:21.574 --> 00:02:24.000
but it's a verb it's three quarters and that

00:02:24.000 --> 00:02:26.835
it's a modal or the end of the sentence is zero.

00:02:26.835 --> 00:02:30.360
And here's a nice graph of our transition probabilities.

00:02:30.360 --> 00:02:33.330
We draw our parts of speech and arrows between

00:02:33.330 --> 00:02:36.920
them with the transition probabilities attached to them,

00:02:36.919 --> 00:02:41.834
and as a final step we combine the two previous graphs to form a Hidden Markov Model.

00:02:41.835 --> 00:02:44.745
We have our words, which are observations.

00:02:44.745 --> 00:02:47.025
These are called the observations because they are

00:02:47.025 --> 00:02:49.680
the things we observe when we read the sentences.

00:02:49.680 --> 00:02:52.590
And the parts of speech are called the Hidden States,

00:02:52.590 --> 00:02:57.254
since they are the ones we don't know and we have to infer based on the words.

00:02:57.254 --> 00:03:00.329
And among the Hidden States we have the transmission probabilities,

00:03:00.330 --> 00:03:04.250
and between the Hidden States and the observation we have the Emission Probabilities.

00:03:04.250 --> 00:03:07.000
And that's it. That's a Hidden Markov Model.

